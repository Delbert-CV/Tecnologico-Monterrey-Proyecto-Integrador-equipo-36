{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c403c50",
   "metadata": {},
   "source": [
    "# EQUIPO 36 | Avance 5: Modelo Final\n",
    "## Proyecto: Predicción de infestaciones de gorgojo del agave\n",
    "## Integrantes equipo 36:\n",
    "\n",
    "| Nombre | Matrícula |\n",
    "| ------ | --------- |\n",
    "| André Martins Cordebello | A00572928 |\n",
    "| Enrique Eduardo Solís Da Costa | A00572678 |\n",
    "| Delbert Francisco Custodio Vargas | A01795613 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130be08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from imblearn.ensemble  import BalancedRandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from bayes_opt import BayesianOptimization\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import BallTree\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import optuna\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ec926e",
   "metadata": {},
   "source": [
    "## Cargamos el dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e928974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data_with_weather_information.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad8b6e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tramp_id                               object\n",
       "sampling_date                  datetime64[ns]\n",
       "lat                                   float64\n",
       "lon                                   float64\n",
       "municipality                           object\n",
       "plantation_age                          int64\n",
       "capture_count                         float64\n",
       "state                                  object\n",
       "square_area_imputed                   float64\n",
       "month                                   int64\n",
       "year                                    int64\n",
       "year-month                             object\n",
       "day_of_year_sin                       float64\n",
       "day_of_year_cos                       float64\n",
       "day_of_week_sin                       float64\n",
       "day_of_week_cos                       float64\n",
       "week_of_year_sin                      float64\n",
       "week_of_year_cos                      float64\n",
       "month_sin                             float64\n",
       "month_cos                             float64\n",
       "critical_season                         int64\n",
       "severity_encoded                        int64\n",
       "distance_to_nearest_hotspot           float64\n",
       "hotspots_within_5km                     int64\n",
       "precipitation                         float64\n",
       "avg_temp                              float64\n",
       "max_temp                              float64\n",
       "min_temp                              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d6ed3",
   "metadata": {},
   "source": [
    "# Modelos individuales: `LightGBM` y `CatBoost`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f22b7",
   "metadata": {},
   "source": [
    "### Carga de nuestro Train y Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d653b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiamos el dataframe con la información\n",
    "train_test_df = df.copy()\n",
    "train_test_df = train_test_df.sort_values(by='sampling_date').reset_index(drop=True)\n",
    "\n",
    "# Hacemos un encoding basico para State y Municipalidad\n",
    "for col in ['state', 'municipality']:\n",
    "    le = LabelEncoder()\n",
    "    train_test_df[col] = le.fit_transform(train_test_df[col])\n",
    "\n",
    "# Generamos la mascara para obtener los datos de antes del 2025 y del 2025 por separado\n",
    "train_mask = train_test_df['sampling_date'].dt.year < 2025\n",
    "test_mask  = train_test_df['sampling_date'].dt.year == 2025\n",
    "\n",
    "# Excluimos la variable objetivo (severity_encoded) y algunos variables o features que ya tenemos contenidos en nuestros\n",
    "# features creados. `capture_count` no podemos tomarlo en cuenta porque se relaciona directamente con la severidad.\n",
    "exclude_cols = [\n",
    "    'severity_encoded','tramp_id', 'capture_count', \n",
    "    'month', 'year-month', 'sampling_date', 'municipality', \n",
    "    'state'\n",
    "]\n",
    "\n",
    "# Cargamos los features a tomar en cuenta (obviamos los features en exclude_cols)\n",
    "features = [col for col in train_test_df.columns if col not in exclude_cols]\n",
    "\n",
    "# Generamos nuestro split de entrenamiento y test por medio de las mascaras train_mask y test_mask\n",
    "X_train, y_train = train_test_df.loc[train_mask, features], train_test_df.loc[train_mask, 'severity_encoded'] # El train dataset es la data historica de 2014 a 2024\n",
    "X_test,  y_test  = train_test_df.loc[test_mask,  features], train_test_df.loc[test_mask,  'severity_encoded'] # El test dataset es la data a partir de 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea68086",
   "metadata": {},
   "source": [
    "### Modelo `LightGBM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "388366bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Resultados para LightGBM individual:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.265     0.327     0.293     24445\n",
      "           1      0.771     0.712     0.740     82928\n",
      "           2      0.139     0.167     0.152      2544\n",
      "           3      0.383     1.000     0.554       110\n",
      "\n",
      "    accuracy                          0.614    110027\n",
      "   macro avg      0.390     0.551     0.435    110027\n",
      "weighted avg      0.643     0.614     0.627    110027\n",
      "\n",
      "\n",
      "\n",
      "Matriz de confusión para LightGBM individual:\n",
      "\n",
      "[[ 7988 16155   288    14]\n",
      " [21473 59007  2342   106]\n",
      " [  674  1387   426    57]\n",
      " [    0     0     0   110]]\n"
     ]
    }
   ],
   "source": [
    "LightGBM_X_train = X_train.copy()\n",
    "LightGBM_X_test  = X_test.copy()\n",
    "LightGBM_Y_train = y_train.copy()\n",
    "LightGBM_Y_test  = y_test.copy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "LightGBM_X_train[['distance_to_nearest_hotspot']] = scaler.fit_transform(LightGBM_X_train[['distance_to_nearest_hotspot']])\n",
    "LightGBM_X_test[['distance_to_nearest_hotspot']] = scaler.transform(LightGBM_X_test[['distance_to_nearest_hotspot']])\n",
    "\n",
    "lgbm_best_params = LGBMClassifier(\n",
    "    boosting_type = \"gbdt\",\n",
    "    objective = \"multiclass\",\n",
    "    num_class = 4,\n",
    "    class_weight = \"balanced\",\n",
    "    is_unbalance = False,\n",
    "    device_type = \"gpu\",\n",
    "    min_gain_to_split = 0.001,  \n",
    "    random_state = 42,\n",
    "    verbose = -1,\n",
    "    learning_rate= 0.031205207400998834,\n",
    "    num_leaves= 110,\n",
    "    max_depth= 11,\n",
    "    feature_fraction= 0.91959030797181,\n",
    "    bagging_fraction= 0.7694621015318531,\n",
    "    lambda_l1= 1.8616621273598788,\n",
    "    lambda_l2= 2.6453430076619573,\n",
    "    min_child_samples= 70,\n",
    "    n_estimators= 299\n",
    ")\n",
    "\n",
    "lgbm_best_params.fit(LightGBM_X_train, LightGBM_Y_train)\n",
    "\n",
    "y_pred_lgbm_best = lgbm_best_params.predict(LightGBM_X_test, categorical_features=['critical_season'])\n",
    "\n",
    "\n",
    "print(\"\\n\\nResultados para LightGBM individual:\\n\\n\")\n",
    "print(classification_report(LightGBM_Y_test, y_pred_lgbm_best, digits=3))\n",
    "\n",
    "print(\"\\n\\nMatriz de confusión para LightGBM individual:\\n\")\n",
    "print(confusion_matrix(y_test, y_pred_lgbm_best))\n",
    "\n",
    "\n",
    "# Probabilidades\n",
    "y_pred_proba_lgbm_best_params = lgbm_best_params.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e90234",
   "metadata": {},
   "source": [
    "### Modelo `CatBoost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36cce033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para CatBoost según Optuna:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.20      0.24     24445\n",
      "           1       0.76      0.70      0.73     82928\n",
      "           2       0.07      0.48      0.12      2544\n",
      "           3       0.33      1.00      0.49       110\n",
      "\n",
      "    accuracy                           0.58    110027\n",
      "   macro avg       0.37      0.59      0.40    110027\n",
      "weighted avg       0.64      0.58      0.61    110027\n",
      "\n",
      "\n",
      "Matriz de confusión para CatBoost:\n",
      "[[ 4965 17050  2411    19]\n",
      " [11127 57787 13869   145]\n",
      " [  161  1105  1214    64]\n",
      " [    0     0     0   110]]\n"
     ]
    }
   ],
   "source": [
    "CatBoost_X_train = X_train.copy()\n",
    "CatBoost_X_test  = X_test.copy()\n",
    "CatBoost_Y_train = y_train.copy()\n",
    "CatBoost_Y_test  = y_test.copy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "CatBoost_X_train[['distance_to_nearest_hotspot']] = scaler.fit_transform(CatBoost_X_train[['distance_to_nearest_hotspot']])\n",
    "CatBoost_X_test[['distance_to_nearest_hotspot']] = scaler.transform(CatBoost_X_test[['distance_to_nearest_hotspot']])\n",
    "\n",
    "\n",
    "class_counts = CatBoost_Y_train.value_counts().sort_index()\n",
    "num_classes = len(class_counts)\n",
    "total = len(CatBoost_Y_train)\n",
    "class_weights = {i: total / (num_classes * count) for i, count in class_counts.items()}\n",
    "\n",
    "weights = CatBoost_Y_train.map(class_weights)\n",
    "\n",
    "cat_model_best = CatBoostClassifier(                 \n",
    "    loss_function='MultiClass',    \n",
    "    eval_metric='TotalF1',         \n",
    "    auto_class_weights='Balanced', \n",
    "    random_seed=42,\n",
    "    task_type='GPU',               \n",
    "    #verbose=100,\n",
    "    iterations= 900,\n",
    "    learning_rate= 0.03169683936081736,\n",
    "    depth= 8,\n",
    "    l2_leaf_reg= 1.0126729009882989,\n",
    "    bagging_temperature= 0.13742770730370382,\n",
    "    border_count= 238,\n",
    "    random_strength= 1.490982729702571,\n",
    "    grow_policy= 'SymmetricTree',\n",
    "    logging_level= 'Silent'\n",
    ")\n",
    "\n",
    "categorical_cols = ['critical_season']\n",
    "\n",
    "cat_model_best.fit(\n",
    "    CatBoost_X_train,\n",
    "    CatBoost_Y_train,\n",
    "    cat_features=categorical_cols if 'categorical_cols' in locals() else None,\n",
    "    eval_set=(CatBoost_X_test, CatBoost_Y_test),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "y_pred_cd_best = cat_model_best.predict(CatBoost_X_test)\n",
    "y_pred_cd_best = y_pred_cd_best.flatten()\n",
    "\n",
    "print(\"Resultados para CatBoost según Optuna:\\n\")\n",
    "print(classification_report(CatBoost_Y_test, y_pred_cd_best))\n",
    "\n",
    "\n",
    "print(\"\\nMatriz de confusión para CatBoost:\")\n",
    "print(confusion_matrix(CatBoost_Y_test, y_pred_cd_best))\n",
    "y_pred_proba_cb_best_params = cat_model_best.predict_proba(CatBoost_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ee10b",
   "metadata": {},
   "source": [
    "# Estrategias de ensamble homogeneas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b38b6",
   "metadata": {},
   "source": [
    "Estas estrategias son conocidas por combinar 2 o más modelos del mismo tipo para mejorar la estabilidad y resultados obtenidos en cuánto a las predicciones que un único modelo podría hacer. Estos ensambles se llaman homogéneos debido a que usan una combinación de un único modelo base, y por cada instancia del mismo modelo se utilizan distintos hiperparámetros (Breiman, 1996). Esto, en algunos casos, logra reducir la varianza que pueda tener un modelo y aumenta la generalización que el mismo pueda hacer. Muchas de estas estrategias buscan promediar los resultados obtenidos por cada modelo base del mismo tipo.\n",
    "\n",
    "Algunas técnicas comunes de ensamble homogéneas son las siguientes:\n",
    "\n",
    "- Baggin (Bootstrap Aggregating), donde se entrenan varios modelos con subconjuntos de datos generados por muestreos aleatorios.\n",
    "- Boosting, donde los modelos se entrenan de manera secuencial para que cada nuevo modelo corrija errores que los modelos pasados pudieron tener.\n",
    "- Stacking homogéneo, la cual mezcla distintas instancias del  mismo modelo con hiperparámetros distintos, para luego utilizar un `meta-modelo` sencillo que combine las salidas de éstas instancias.\n",
    "\n",
    "\n",
    "Por ejemplo, varios `DecisionTrees`, `RandomForest` o `CatBoost` con distintos parámetros podrían formar parte de estrategias de ensamble homogéneas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fbbfd1",
   "metadata": {},
   "source": [
    "## `LightGBM Bagging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3640b4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro para Bagging con LightGBM: 0.4327\n",
      "\n",
      "\n",
      "Resultados para LightGBM después de usar Bagging:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.254     0.275     0.264     24445\n",
      "           1      0.765     0.745     0.755     82928\n",
      "           2      0.163     0.159     0.161      2544\n",
      "           3      0.381     1.000     0.551       110\n",
      "\n",
      "    accuracy                          0.627    110027\n",
      "   macro avg      0.391     0.545     0.433    110027\n",
      "weighted avg      0.637     0.627     0.632    110027\n",
      "\n",
      "\n",
      "\n",
      "Matriz de confusión para LightGBM con bagging:\n",
      "\n",
      "[[ 6723 17538   169    15]\n",
      " [19165 61749  1907   107]\n",
      " [  623  1459   405    57]\n",
      " [    0     0     0   110]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Cargamos nuestros train y test sets\n",
    "Bagging_X_train = X_train.copy()\n",
    "Bagging_Y_train = y_train.copy()\n",
    "Bagging_X_test  = X_test.copy()\n",
    "Bagging_Y_test  = y_test.copy()\n",
    "\n",
    "#  Cantidad de modelos a generar para el Bagging\n",
    "n_models = 15\n",
    "models = []\n",
    "oof_preds = []\n",
    "test_preds = []\n",
    "\n",
    "\n",
    "for i in range(n_models):\n",
    "    \n",
    "    # Generamos nuevos sets de entrenamiento y test por cada ciclo\n",
    "    X_train_sub, y_train_sub = resample(Bagging_X_train, Bagging_Y_train, replace=True, random_state=42 + i)\n",
    "    \n",
    "    # Instanciamos LightGBM Classifier\n",
    "    model = lgb.LGBMClassifier( \n",
    "                                boosting_type = \"gbdt\",\n",
    "                                objective = \"multiclass\",\n",
    "                                num_class = 4,\n",
    "                                class_weight = \"balanced\",\n",
    "                                is_unbalance = False,\n",
    "                                device_type = \"gpu\",\n",
    "                                min_gain_to_split = 0.001,  \n",
    "                                random_state = 42,\n",
    "                                verbose = -1,\n",
    "                                learning_rate= 0.031205207400998834,\n",
    "                                num_leaves= 110,\n",
    "                                max_depth= 11,\n",
    "                                feature_fraction= 0.91959030797181,\n",
    "                                bagging_fraction= 0.7694621015318531,\n",
    "                                lambda_l1= 1.8616621273598788,\n",
    "                                lambda_l2= 2.6453430076619573,\n",
    "                                min_child_samples= 70,\n",
    "                                n_estimators= 299\n",
    "                            )\n",
    "    \n",
    "    # Entrenamos sobre los sets de entrenamiento y test por ciclo\n",
    "    model.fit(X_train_sub, y_train_sub)\n",
    "    models.append(model)\n",
    "\n",
    "    # Guardamos las predicciones out-of-fold\n",
    "    y_pred = model.predict_proba(Bagging_X_test)\n",
    "    test_preds.append(y_pred)\n",
    "\n",
    "# Promediamos los resultados\n",
    "avg_proba = np.mean(test_preds, axis=0)\n",
    "y_pred_final = np.argmax(avg_proba, axis=1)\n",
    "\n",
    "# Calculamos el F1 Score\n",
    "f1_macro = f1_score(Bagging_Y_test, y_pred_final, average=\"macro\")\n",
    "print(f\"F1-macro para Bagging con LightGBM: {f1_macro:.4f}\")\n",
    "\n",
    "# Imprimimos resultados\n",
    "print(\"\\n\\nResultados para LightGBM después de usar Bagging:\\n\\n\")\n",
    "print(classification_report(Bagging_Y_test, y_pred_final, digits=3))\n",
    "\n",
    "print(\"\\n\\nMatriz de confusión para LightGBM con bagging:\\n\")\n",
    "print(confusion_matrix(Bagging_Y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef39f8",
   "metadata": {},
   "source": [
    "Para la estrategia de Bagging, notamos que `LightGBM` no tuvo una mejora, por lo que descartamos que esta sea una estrategia viable por medio de este modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd2a96",
   "metadata": {},
   "source": [
    "## `LightGBM Stacking`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f003205e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados luego de hacer LightGBM Stacking\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2688    0.2665    0.2676     24445\n",
      "           1     0.7684    0.7923    0.7802     82928\n",
      "           2     0.1034    0.0012    0.0023      2544\n",
      "           3     0.3960    0.9000    0.5500       110\n",
      "\n",
      "    accuracy                         0.6573    110027\n",
      "   macro avg     0.3842    0.4900    0.4000    110027\n",
      "weighted avg     0.6417    0.6573    0.6481    110027\n",
      "\n",
      "\n",
      "Matriz de confusion para LightGBM stacking:\n",
      "\n",
      "[[ 6514 17917     6     8]\n",
      " [17118 65707    12    91]\n",
      " [  605  1884     3    52]\n",
      " [    0     3     8    99]]\n"
     ]
    }
   ],
   "source": [
    "X_train_stack = X_train.copy()\n",
    "y_train_stack = y_train.copy()\n",
    "X_test_stack  = X_test.copy()\n",
    "y_test_stack  = y_test.copy()\n",
    "\n",
    "lgbm_1 = lgb.LGBMClassifier(\n",
    "    boosting_type='gbdt', \n",
    "    objective='multiclass', \n",
    "    num_class=4,            \n",
    "    class_weight='balanced',\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    max_depth=-1,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    device='gpu',\n",
    "    is_unbalance=False\n",
    ")\n",
    "\n",
    "lgbm_2 = lgb.LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=4,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=10,\n",
    "    feature_fraction=0.9,\n",
    "    bagging_fraction=0.8,\n",
    "    random_state=42,\n",
    "    device_type='gpu',\n",
    "    is_unbalance=False,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "lgbm_3 = lgb.LGBMClassifier(\n",
    "    boosting_type = \"gbdt\",\n",
    "    objective = \"multiclass\",\n",
    "    num_class = 4,\n",
    "    class_weight = \"balanced\",\n",
    "    is_unbalance = False,\n",
    "    device_type = \"gpu\",\n",
    "    min_gain_to_split = 0.001,  \n",
    "    random_state = 42,\n",
    "    verbose = -1,\n",
    "    learning_rate= 0.031205207400998834,\n",
    "    num_leaves= 110,\n",
    "    max_depth= 11,\n",
    "    feature_fraction= 0.91959030797181,\n",
    "    bagging_fraction= 0.7694621015318531,\n",
    "    lambda_l1= 1.8616621273598788,\n",
    "    lambda_l2= 2.6453430076619573,\n",
    "    min_child_samples= 70,\n",
    "    n_estimators= 299\n",
    ")\n",
    "\n",
    "meta_model = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "\n",
    "\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lgbm1', lgbm_1),\n",
    "        ('lgbm2', lgbm_2),\n",
    "        ('lgbm3', lgbm_3)\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stack_model.fit(X_train_stack, y_train_stack)\n",
    "\n",
    "y_pred = stack_model.predict(X_test_stack)\n",
    "\n",
    "print(\"Resultados luego de hacer LightGBM Stacking\")\n",
    "print(classification_report(y_test_stack, y_pred, digits=4))\n",
    "\n",
    "print(\"\\nMatriz de confusion para LightGBM stacking:\\n\")\n",
    "print(confusion_matrix(y_test_stack, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3834f",
   "metadata": {},
   "source": [
    "Por medio del uso de Stacking, vemos un ligero mejoramiento en la `precision` de la clase 3, a costa de un peor desempeño en el recall de dicha clase. En nuestro caso, tener un recall de más del 90% es aceptable, pero notamos que perdimos un aproximado de 9% en promedio de esta métrica por una mejora del 1% aproximadamente en nuestra `precision`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0810ca",
   "metadata": {},
   "source": [
    "## `CatBoost Bagging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9c96649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro para Bagging con CatBoost: 0.3919\n",
      "\n",
      "\n",
      "Resultados para CatBoost después de usar Bagging:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.286     0.156     0.202     24445\n",
      "           1      0.752     0.737     0.744     82928\n",
      "           2      0.076     0.451     0.131      2544\n",
      "           3      0.325     1.000     0.491       110\n",
      "\n",
      "    accuracy                          0.601    110027\n",
      "   macro avg      0.360     0.586     0.392    110027\n",
      "weighted avg      0.632     0.601     0.609    110027\n",
      "\n",
      "\n",
      "\n",
      "Matriz de confusión para CatBoost Bagging:\n",
      "\n",
      "[[ 3821 18976  1629    19]\n",
      " [ 9445 61088 12250   145]\n",
      " [  113  1220  1147    64]\n",
      " [    0     0     0   110]]\n"
     ]
    }
   ],
   "source": [
    "# Generamos nuestros Train y Test datasets\n",
    "Bagging_X_train = X_train.copy()\n",
    "Bagging_Y_train = y_train.copy()\n",
    "Bagging_X_test  = X_test.copy()\n",
    "Bagging_Y_test  = y_test.copy()\n",
    "\n",
    "#  Cantidad de modelos a generar para el Bagging\n",
    "n_models = 5\n",
    "models = []\n",
    "oof_preds = []\n",
    "test_preds = []\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "Bagging_X_train[['distance_to_nearest_hotspot']] = scaler.fit_transform(Bagging_X_train[['distance_to_nearest_hotspot']])\n",
    "Bagging_X_test[['distance_to_nearest_hotspot']] = scaler.transform(Bagging_X_test[['distance_to_nearest_hotspot']])\n",
    "\n",
    "\n",
    "class_counts = Bagging_Y_train.value_counts().sort_index()\n",
    "num_classes = len(class_counts)\n",
    "total = len(Bagging_Y_train)\n",
    "class_weights = {i: total / (num_classes * count) for i, count in class_counts.items()}\n",
    "\n",
    "weights = Bagging_Y_train.map(class_weights)\n",
    "\n",
    "categorical_cols = ['critical_season']\n",
    "\n",
    "for i in range(n_models):\n",
    "    \n",
    "    X_train_sub, y_train_sub = resample(Bagging_X_train, Bagging_Y_train, replace=True, random_state=42 + i)\n",
    "\n",
    "    model = CatBoostClassifier(                 \n",
    "        loss_function='MultiClass',    \n",
    "        eval_metric='TotalF1',         \n",
    "        auto_class_weights='Balanced', \n",
    "        random_seed=42,\n",
    "        task_type='GPU',               \n",
    "        #verbose=100,\n",
    "        iterations= 900,\n",
    "        learning_rate= 0.03169683936081736,\n",
    "        depth= 8,\n",
    "        l2_leaf_reg= 1.0126729009882989,\n",
    "        bagging_temperature= 0.13742770730370382,\n",
    "        border_count= 238,\n",
    "        random_strength= 1.490982729702571,\n",
    "        grow_policy= 'SymmetricTree',\n",
    "        logging_level= 'Silent'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    model.fit(\n",
    "        X_train_sub,\n",
    "        y_train_sub,\n",
    "        cat_features=categorical_cols if 'categorical_cols' in locals() else None,\n",
    "        eval_set=(Bagging_X_test, Bagging_Y_test),\n",
    "        use_best_model=True,\n",
    "        verbose=False)\n",
    "    \n",
    "    models.append(model)\n",
    "\n",
    "    # Guardamos las predicciones out-of-fold\n",
    "    y_pred = model.predict_proba(Bagging_X_test)\n",
    "    test_preds.append(y_pred)\n",
    "    \n",
    "    \n",
    "avg_proba = np.mean(test_preds, axis=0)\n",
    "y_pred_final = np.argmax(avg_proba, axis=1)\n",
    "\n",
    "f1_macro = f1_score(Bagging_Y_test, y_pred_final, average=\"macro\")\n",
    "print(f\"F1-macro para Bagging con CatBoost: {f1_macro:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nResultados para CatBoost después de usar Bagging:\\n\\n\")\n",
    "print(classification_report(Bagging_Y_test, y_pred_final, digits=3))\n",
    "\n",
    "print(\"\\n\\nMatriz de confusión para CatBoost Bagging:\\n\")\n",
    "print(confusion_matrix(Bagging_Y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091641a",
   "metadata": {},
   "source": [
    "## `Stacking CatBoost` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59356e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CatBoost #1 ...\n",
      "Modelo #1 Listo — F1-macro: 0.4071 | Tiempo: 26.66s\n",
      "\n",
      "CatBoost #2 ...\n",
      "Modelo #2 Listo — F1-macro: 0.3695 | Tiempo: 17.27s\n",
      "\n",
      "CatBoost #3 ...\n",
      "Modelo #3 Listo — F1-macro: 0.3757 | Tiempo: 13.92s\n",
      "              Model  F1-macro  Training Time (s)\n",
      "0  CatBoost_Model_1  0.407078          26.656607\n",
      "1  CatBoost_Model_2  0.369531          17.271919\n",
      "2  CatBoost_Model_3  0.375697          13.924674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados para CatBoost Stacking :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2302    0.1933    0.2102     24445\n",
      "           1     0.7579    0.8163    0.7860     82928\n",
      "           2     0.1515    0.0039    0.0077      2544\n",
      "           3     0.3717    0.3818    0.3767       110\n",
      "\n",
      "    accuracy                         0.6587    110027\n",
      "   macro avg     0.3778    0.3489    0.3451    110027\n",
      "weighted avg     0.6263    0.6587    0.6397    110027\n",
      "\n",
      "\n",
      "Matriz de confusion para CatBoost Stacking:\n",
      "[[ 4726 19711     3     5]\n",
      " [15156 67697    27    48]\n",
      " [  647  1869    10    18]\n",
      " [    0    42    26    42]]\n"
     ]
    }
   ],
   "source": [
    "X_train_stack = X_train.copy()\n",
    "y_train_stack = y_train.copy()\n",
    "X_test_stack  = X_test.copy()\n",
    "y_test_stack  = y_test.copy()\n",
    "\n",
    "\n",
    "cb_1 = CatBoostClassifier(                 \n",
    "    loss_function='MultiClass',    \n",
    "    eval_metric='TotalF1',         \n",
    "    auto_class_weights='Balanced', \n",
    "    random_seed=42,\n",
    "    task_type='GPU',               \n",
    "    iterations=900,\n",
    "    learning_rate=0.03169683936081736,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=1.0126729009882989,\n",
    "    bagging_temperature=0.13742770730370382,\n",
    "    border_count=238,\n",
    "    random_strength=1.490982729702571,\n",
    "    grow_policy='SymmetricTree',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "cb_2 = CatBoostClassifier(\n",
    "    iterations=300,\n",
    "    learning_rate=0.05,\n",
    "    depth=10,\n",
    "    l2_leaf_reg=3,\n",
    "    loss_function=\"MultiClass\",\n",
    "    eval_metric=\"TotalF1\",\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    task_type=\"GPU\",\n",
    "    random_seed=100,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "cb_3 = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.02,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=2,\n",
    "    loss_function=\"MultiClass\",\n",
    "    eval_metric=\"TotalF1\",\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    task_type=\"GPU\",\n",
    "    random_seed=2025,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "meta_model = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "\n",
    "models = [cb_1, cb_2, cb_3]\n",
    "\n",
    "train_probas = []\n",
    "test_probas  = []\n",
    "results = []\n",
    "\n",
    "for i, model in enumerate(models, 1):\n",
    "    print(f\"\\nCatBoost #{i} ...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    model.fit(X_train_stack, y_train_stack)\n",
    "    \n",
    "    y_train_pred = model.predict_proba(X_train_stack)\n",
    "    y_test_pred  = model.predict_proba(X_test_stack)\n",
    "    \n",
    "    train_probas.append(y_train_pred)\n",
    "    test_probas.append(y_test_pred)\n",
    "    \n",
    "    y_pred_class = np.argmax(y_test_pred, axis=1)\n",
    "    f1_macro = f1_score(y_test_stack, y_pred_class, average=\"macro\")\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    results.append([f\"CatBoost_Model_{i}\", f1_macro, elapsed])\n",
    "    print(f\"Modelo #{i} Listo — F1-macro: {f1_macro:.4f} | Tiempo: {elapsed:.2f}s\")\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"F1-macro\", \"Training Time (s)\"])\n",
    "print(results_df)\n",
    "\n",
    "X_meta_train = np.hstack(train_probas)\n",
    "X_meta_test  = np.hstack(test_probas)\n",
    "\n",
    "meta_model = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "meta_model.fit(X_meta_train, y_train_stack)\n",
    "\n",
    "y_pred_stack = meta_model.predict(X_meta_test)\n",
    "\n",
    "print(\"\\nResultados para CatBoost Stacking :\")\n",
    "print(classification_report(y_test_stack, y_pred_stack, digits=4))\n",
    "print(\"\\nMatriz de confusion para CatBoost Stacking:\")\n",
    "print(confusion_matrix(y_test_stack, y_pred_stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af65c4bb",
   "metadata": {},
   "source": [
    "# Estrategias de ensamble heterogéneas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade24f2a",
   "metadata": {},
   "source": [
    "A diferencia de las estrategias de ensamble homogéneas, para las heterogéneas buscamos la combinación de distintos tipos de modelos o algoritmos de aprendizaje automático. La idea de este enfoque es la de aprovechar las fortalezas, debilidades y complejidades de cada tipo de modelo para tener una diversidad o combinación estructural. Esta diversidad tiende a mejorar el rendimiento de las predicciones y también mejora la robustez que pueda tener una solución de ML (Kuncheva, 2004).\n",
    "\n",
    "Las estrategias más comunes llevan por nombre `Voting Classifier` y `Stacking heterogéneo`. El enfoque del `VotingClassifier` como método de ensamble es realizar predicciones independientes de modelos distintos, para luego combinar dichos resultados y predecir una nueva salida por medio de la combinación de las predicciones independientes; en el caso de clasificación se utiliza una votación mayoritaria, mientras que para el caso de regresiones se utiliza una media ponderada. Ahora bien, para el `Stacking heterogéneo` buscamos usar las predicciones independientes de cada modelo como una entrada para un meta-modelo que se entrena para combinar los resultados; es decir, el meta-modelo predice una nueva salida con base en las predicciones de los modelos independientes, lo que permite que algunas implementaciones encuentren patrones no lineales y mejoren los resultados (Rokach, 2010).\n",
    "\n",
    "Por último, estos ensambles son útiles cuando se tienen modelos \"especializados\", es decir, contamos con modelos base los cuales son buenos en alguna métrica en específico (por ejemplo, un modelo es bueno en `precision` y el otro modelo base es bueno respecto a su `recall`) (Wolpert, 1992). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb8e29",
   "metadata": {},
   "source": [
    "## `VotingClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28328fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 11:36:48,682] Using an existing study with name 'VotingClassifier_Optim' instead of creating a new one.\n",
      "[I 2025-10-25 11:36:58,335] Trial 80 finished with value: 0.5765146373708004 and parameters: {'lgb_weight': 1.0417712744545098, 'cb_weight': 0.056594033959469436}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:37:07,939] Trial 81 finished with value: 0.5759832999189951 and parameters: {'lgb_weight': 0.563104032076097, 'cb_weight': 1.4341824024384295}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:37:17,610] Trial 82 finished with value: 0.5765599055462376 and parameters: {'lgb_weight': 0.5026437698280065, 'cb_weight': 0.12372987139512043}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:37:27,581] Trial 83 finished with value: 0.5764704951220839 and parameters: {'lgb_weight': 0.4307010642830197, 'cb_weight': 0.04759900210449882}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:37:37,237] Trial 84 finished with value: 0.5752755826897941 and parameters: {'lgb_weight': 0.392473643189507, 'cb_weight': 1.1663059008113281}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:37:46,985] Trial 85 finished with value: 0.5763327181725425 and parameters: {'lgb_weight': 0.6270155053786755, 'cb_weight': 0.8206848274398317}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:37:56,659] Trial 86 finished with value: 0.5764203680408292 and parameters: {'lgb_weight': 0.28453163222878786, 'cb_weight': 0.22447491112280793}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:38:06,318] Trial 87 finished with value: 0.5765231760816969 and parameters: {'lgb_weight': 1.6305620012141029, 'cb_weight': 0.03213072884294006}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:38:16,084] Trial 88 finished with value: 0.5765307550435842 and parameters: {'lgb_weight': 0.5294686827972968, 'cb_weight': 0.10333601141773009}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:38:25,772] Trial 89 finished with value: 0.5765307550435842 and parameters: {'lgb_weight': 1.7885224037781113, 'cb_weight': 0.3468860367559405}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:38:35,456] Trial 90 finished with value: 0.5764792971412088 and parameters: {'lgb_weight': 1.8676781105256646, 'cb_weight': 0.272756404210291}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:38:45,325] Trial 91 finished with value: 0.5765959596191853 and parameters: {'lgb_weight': 0.4774936163910352, 'cb_weight': 0.17010472604486707}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:38:54,957] Trial 92 finished with value: 0.5764660295906362 and parameters: {'lgb_weight': 0.06049112615016916, 'cb_weight': 0.007779149004175574}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:39:04,537] Trial 93 finished with value: 0.5764421223693378 and parameters: {'lgb_weight': 0.12819215159686884, 'cb_weight': 0.08143355279882228}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:39:14,398] Trial 94 finished with value: 0.576521977495283 and parameters: {'lgb_weight': 1.9377696271949305, 'cb_weight': 0.03562785861205909}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:39:24,311] Trial 95 finished with value: 0.5765373846756493 and parameters: {'lgb_weight': 0.6443690237332096, 'cb_weight': 0.1357305668302167}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:39:33,828] Trial 96 finished with value: 0.5765618069230852 and parameters: {'lgb_weight': 0.7288561522086662, 'cb_weight': 0.20753283465351124}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:39:43,069] Trial 97 finished with value: 0.5766083874908431 and parameters: {'lgb_weight': 0.2522032684027118, 'cb_weight': 0.09212713795583634}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:39:52,698] Trial 98 finished with value: 0.5744502479766468 and parameters: {'lgb_weight': 0.18605588483634763, 'cb_weight': 0.7176995763089357}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:40:02,274] Trial 99 finished with value: 0.5766128941158674 and parameters: {'lgb_weight': 0.3494887548382901, 'cb_weight': 0.16613897245509288}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:40:11,556] Trial 100 finished with value: 0.57631834396201 and parameters: {'lgb_weight': 0.8094479788366603, 'cb_weight': 0.9565041971627037}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:40:21,340] Trial 101 finished with value: 0.5764818304386718 and parameters: {'lgb_weight': 1.7444655023362095, 'cb_weight': 0.04493520238628067}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:40:30,928] Trial 102 finished with value: 0.5765048254694474 and parameters: {'lgb_weight': 1.6115435301837377, 'cb_weight': 0.26131593880606174}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:40:40,190] Trial 103 finished with value: 0.5765407984155243 and parameters: {'lgb_weight': 1.4848566915782642, 'cb_weight': 0.09522304073883646}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:40:49,672] Trial 104 finished with value: 0.5765910484587121 and parameters: {'lgb_weight': 1.3700110731433544, 'cb_weight': 0.4329254906911444}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:40:59,009] Trial 105 finished with value: 0.5764955330882872 and parameters: {'lgb_weight': 0.5760554016158643, 'cb_weight': 0.007009659913219798}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:41:08,929] Trial 106 finished with value: 0.5765099785623098 and parameters: {'lgb_weight': 1.6512271853504275, 'cb_weight': 0.13370288187963564}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:41:19,005] Trial 107 finished with value: 0.5766141838144941 and parameters: {'lgb_weight': 1.5636575945196334, 'cb_weight': 0.5546217295875046}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:41:28,709] Trial 108 finished with value: 0.5764727523728755 and parameters: {'lgb_weight': 1.7101397290586575, 'cb_weight': 0.1943110069479663}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-25 11:41:38,577] Trial 109 finished with value: 0.5763411261967679 and parameters: {'lgb_weight': 1.234381701185317, 'cb_weight': 1.7164523828617644}. Best is trial 41 with value: 0.5799116275863289.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mejor F1-Macro: 0.5799\n",
      "Pesos óptimos: {'lgb_weight': 1.9516866156032004, 'cb_weight': 0.6156930840292797}\n"
     ]
    }
   ],
   "source": [
    "Voting_X_train = X_train.copy()\n",
    "Voting_Y_train = y_train.copy()\n",
    "Voting_X_test  = X_test.copy()\n",
    "Voting_Y_test  = y_test.copy()\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "def objective(trial):\n",
    "    w1 = trial.suggest_float(\"lgb_weight\", 0.0, 2.0)\n",
    "    w2 = trial.suggest_float(\"cb_weight\", 0.0, 2.0)\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_idx, val_idx in tscv.split(Voting_X_train):\n",
    "        X_tr, X_val = Voting_X_train.iloc[train_idx], Voting_X_train.iloc[val_idx]\n",
    "        y_tr, y_val = Voting_Y_train.iloc[train_idx], Voting_Y_train.iloc[val_idx]\n",
    "\n",
    "        prob_lgb = lgbm_best_params.predict_proba(X_val)\n",
    "        prob_cb  = cat_model_best.predict_proba(X_val)\n",
    "\n",
    "        combined_proba = (w1 * prob_lgb + w2 * prob_cb) / (w1 + w2 + 1e-8)\n",
    "        y_pred = np.argmax(combined_proba, axis=1)\n",
    "\n",
    "        f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "study_name = \"VotingClassifier_Optim\"\n",
    "storage_name = f\"sqlite:///{study_name}.db\"\n",
    "\n",
    "study_VotingClassifier = optuna.create_study(study_name=study_name, storage=storage_name, direction='maximize', load_if_exists=True)\n",
    "study_VotingClassifier.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "\n",
    "best_w1 = study_VotingClassifier.best_params[\"lgb_weight\"]\n",
    "best_w2 = study_VotingClassifier.best_params[\"cb_weight\"]\n",
    "\n",
    "prob_lgb_test = lgbm_best_params.predict_proba(Voting_X_test)\n",
    "prob_cb_test  = cat_model_best.predict_proba(Voting_X_test)\n",
    "combined_proba_test = (best_w1 * prob_lgb_test + best_w2 * prob_cb_test) / (best_w1 + best_w2 + 1e-8)\n",
    "\n",
    "y_pred_voting = np.argmax(combined_proba_test, axis=1)\n",
    "\n",
    "print(f\"\\n\\nMejor F1-Macro: {study_VotingClassifier.best_value:.4f}\")\n",
    "print(f\"Pesos óptimos: {study_VotingClassifier.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62a21099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de entrenamiento: 74.56 segundos\n",
      "\n",
      "\n",
      "Reporte de clasificación (VotingClassifier)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.265     0.324     0.292     24445\n",
      "           1      0.771     0.714     0.741     82928\n",
      "           2      0.139     0.164     0.150      2544\n",
      "           3      0.383     1.000     0.554       110\n",
      "\n",
      "    accuracy                          0.615    110027\n",
      "   macro avg      0.389     0.551     0.434    110027\n",
      "weighted avg      0.643     0.615     0.628    110027\n",
      "\n",
      "\n",
      "Matriz de confusión para VotingClassifier:\n",
      "[[ 7929 16216   286    14]\n",
      " [21288 59227  2307   106]\n",
      " [  675  1395   417    57]\n",
      " [    0     0     0   110]]\n"
     ]
    }
   ],
   "source": [
    "# Crear VotingClassifier \n",
    "voting_model = VotingClassifier( \n",
    "                                estimators=[ ('lgb', lgbm_best_params), ('cb', cat_model_best) ], \n",
    "                                voting='soft', \n",
    "                                weights=[0.5685868882216454, 0.010730550423117324], \n",
    "                                n_jobs=-1 \n",
    "                            ) \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "voting_model.fit(Voting_X_train, Voting_Y_train) \n",
    "\n",
    "train_time = time.time() - start \n",
    "\n",
    "y_pred_voting = voting_model.predict(Voting_X_test) \n",
    "y_proba_voting = voting_model.predict_proba(Voting_X_test) \n",
    "\n",
    "print(f\"Tiempo de entrenamiento: {train_time:.2f} segundos\\n\") \n",
    "print(\"\\nReporte de clasificación (VotingClassifier)\") \n",
    "print(classification_report(Voting_Y_test, y_pred_voting, digits=3)) \n",
    "print(\"\\nMatriz de confusión para VotingClassifier:\") \n",
    "print(confusion_matrix(Voting_Y_test, y_pred_voting))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f14e422",
   "metadata": {},
   "source": [
    "## `Stacking heterogéneo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9e36e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape del meta-train set: (717829, 8)\n",
      "Shape del meta-test set: (110027, 8)\n"
     ]
    }
   ],
   "source": [
    "# Creamos copias de seguridad de nuestro train y test set\n",
    "Stacking_X_train = X_train.copy()\n",
    "Stacking_Y_train = y_train.copy()\n",
    "Stacking_X_test  = X_test.copy()\n",
    "Stacking_Y_test  = y_test.copy()\n",
    "\n",
    "# Generamos las probabilidades de las predicciones por cada clase y por cada modelo\n",
    "y_pred_lgb_train = lgbm_best_params.predict_proba(Stacking_X_train)\n",
    "y_pred_lgb_test  = lgbm_best_params.predict_proba(Stacking_X_test)\n",
    "y_pred_cb_train = cat_model_best.predict_proba(Stacking_X_train)\n",
    "y_pred_cb_test  = cat_model_best.predict_proba(Stacking_X_test)\n",
    "\n",
    "# Unimos (stack) las probabilidades\n",
    "X_meta_train = np.hstack([y_pred_lgb_train, y_pred_cb_train])\n",
    "X_meta_test  = np.hstack([y_pred_lgb_test,  y_pred_cb_test])\n",
    "\n",
    "# Copiamos las clases correctas\n",
    "y_meta_train = Stacking_Y_train.copy()\n",
    "y_meta_test  = Stacking_Y_test.copy()\n",
    "\n",
    "# Vemos la forma de cada dataset\n",
    "print(\"Shape del meta-train set:\", X_meta_train.shape)\n",
    "print(\"Shape del meta-test set:\", X_meta_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0860b22",
   "metadata": {},
   "source": [
    "#### Usando `LightGBM` como meta-modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12da650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Resultados para Stacking usando LightGBM como meta-modelo:\n",
      "F1-macro: 0.41621822186423346\n",
      "\n",
      "Métricas para LightGBM como meta-modelo:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.34      0.28     24445\n",
      "           1       0.76      0.64      0.70     82928\n",
      "           2       0.08      0.15      0.11      2544\n",
      "           3       0.42      0.95      0.58       110\n",
      "\n",
      "    accuracy                           0.56    110027\n",
      "   macro avg       0.37      0.52      0.42    110027\n",
      "weighted avg       0.63      0.56      0.59    110027\n",
      "\n",
      "\n",
      "\n",
      "Matriz de confusión con LightGBM como meta-modelo:\n",
      "\n",
      "[[ 8415 15500   523     7]\n",
      " [25960 53028  3853    87]\n",
      " [ 1025  1077   390    52]\n",
      " [    0     0     5   105]]\n"
     ]
    }
   ],
   "source": [
    "# Definimos un LightGBM pequeño como meta-learner\n",
    "meta_model = lgb.LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='multiclass',\n",
    "    num_class=len(np.unique(y_meta_train)),\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=250,\n",
    "    max_depth=10,\n",
    "    num_leaves=20,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    device_type='gpu',\n",
    "    min_split_gain=1\n",
    ")\n",
    "\n",
    "# Entrenamos el meta-modelo\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Predicciones finales\n",
    "y_meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Evaluamos el rendimiento\n",
    "print(\"\\n\\nResultados para Stacking usando LightGBM como meta-modelo:\")\n",
    "print(\"F1-macro:\", f1_score(y_meta_test, y_meta_pred, average='macro'))\n",
    "print(\"\\nMétricas para LightGBM como meta-modelo:\")\n",
    "print(classification_report(y_meta_test, y_meta_pred))\n",
    "print(\"\\n\\nMatriz de confusión con LightGBM como meta-modelo:\\n\")\n",
    "print(confusion_matrix(y_meta_test,y_meta_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baae174a",
   "metadata": {},
   "source": [
    "#### Usando `CatBoost` como meta-modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08ffd827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro (CatBoost meta-modelo): 0.41441359662098287\n",
      "\n",
      "Reporte de clasificación (CatBoost meta-modelo):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.33      0.28     24445\n",
      "           1       0.76      0.66      0.71     82928\n",
      "           2       0.08      0.15      0.11      2544\n",
      "           3       0.40      0.96      0.56       110\n",
      "\n",
      "    accuracy                           0.58    110027\n",
      "   macro avg       0.37      0.53      0.41    110027\n",
      "weighted avg       0.63      0.58      0.60    110027\n",
      "\n",
      "\n",
      "\n",
      "Matriz de confusión con CatBoost como meta-modelo:\n",
      "\n",
      "[[ 7965 15934   534    12]\n",
      " [24185 55012  3637    94]\n",
      " [  982  1127   381    54]\n",
      " [    0     0     4   106]]\n"
     ]
    }
   ],
   "source": [
    "meta_model_cb = CatBoostClassifier(\n",
    "    loss_function='MultiClass',\n",
    "    eval_metric='TotalF1',\n",
    "    auto_class_weights='Balanced',\n",
    "    task_type='GPU',\n",
    "    iterations=250,        \n",
    "    learning_rate=0.02,  \n",
    "    depth=15,         \n",
    "    l2_leaf_reg=3,     \n",
    "    random_strength=1.0,\n",
    "    verbose=False,  \n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Entrenamos el meta-modelo\n",
    "meta_model_cb.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Predicciones finales\n",
    "y_meta_cb_pred = meta_model_cb.predict(X_meta_test)\n",
    "\n",
    "# Evaluación\n",
    "print(\"F1-macro (CatBoost meta-modelo):\", f1_score(y_meta_test, y_meta_cb_pred, average='macro'))\n",
    "print(\"\\nReporte de clasificación (CatBoost meta-modelo):\")\n",
    "print(classification_report(y_meta_test, y_meta_cb_pred))\n",
    "print(\"\\n\\nMatriz de confusión con CatBoost como meta-modelo:\\n\")\n",
    "print(confusion_matrix(y_meta_test,y_meta_cb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c43fe8",
   "metadata": {},
   "source": [
    "#### Usando `RandomForestClassifier` como meta-modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88117574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro (RandomForest meta-modelo): 0.38437823721925524\n",
      "\n",
      "Reporte de clasificación (RandomForest meta-modelo):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.08      0.11     24445\n",
      "           1       0.75      0.81      0.78     82928\n",
      "           2       0.08      0.29      0.12      2544\n",
      "           3       0.38      0.82      0.52       110\n",
      "\n",
      "    accuracy                           0.64    110027\n",
      "   macro avg       0.35      0.50      0.38    110027\n",
      "weighted avg       0.61      0.64      0.62    110027\n",
      "\n",
      "\n",
      "\n",
      "Matriz de confusión con RandomForest como meta-modelo:\n",
      "\n",
      "[[ 1942 21592   901    10]\n",
      " [ 7429 67560  7854    85]\n",
      " [  504  1248   742    50]\n",
      " [    0     0    20    90]]\n"
     ]
    }
   ],
   "source": [
    "meta_model_rf = RandomForestClassifier(\n",
    "    n_estimators=150,          # Número reducido de árboles\n",
    "    max_depth=6,               # Árboles poco profundos\n",
    "    class_weight='balanced',   # Para manejar desbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Entrenamiento del meta-modelo\n",
    "meta_model_rf.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Predicciones finales\n",
    "y_meta_rf_pred = meta_model_rf.predict(X_meta_test)\n",
    "\n",
    "# Evaluación\n",
    "print(\"F1-macro (RandomForest meta-modelo):\", f1_score(y_meta_test, y_meta_rf_pred, average='macro'))\n",
    "print(\"\\nReporte de clasificación (RandomForest meta-modelo):\")\n",
    "print(classification_report(y_meta_test, y_meta_rf_pred))\n",
    "print(\"\\n\\nMatriz de confusión con RandomForest como meta-modelo:\\n\")\n",
    "print(confusion_matrix(y_meta_test,y_meta_rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25749d1b",
   "metadata": {},
   "source": [
    "#### Usando `LogistciRegression` como meta-modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e44785f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro (LogisticRegression meta-modelo): 0.4100268724838725\n",
      "\n",
      "Reporte de clasificación (LogisticRegression meta-modelo):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.23      0.23     24445\n",
      "           1       0.76      0.74      0.75     82928\n",
      "           2       0.06      0.11      0.08      2544\n",
      "           3       0.41      0.98      0.58       110\n",
      "\n",
      "    accuracy                           0.61    110027\n",
      "   macro avg       0.37      0.52      0.41    110027\n",
      "weighted avg       0.63      0.61      0.62    110027\n",
      "\n",
      "\n",
      "\n",
      "Matriz de confusión con LogisticRegression como meta-modelo:\n",
      "\n",
      "[[ 5720 18203   514     8]\n",
      " [17931 61269  3635    93]\n",
      " [  893  1315   281    55]\n",
      " [    0     0     2   108]]\n"
     ]
    }
   ],
   "source": [
    "meta_model_lr = LogisticRegression(\n",
    "    max_iter=2000,             # Iteraciones suficientes para convergencia\n",
    "    class_weight='balanced',\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenamiento del meta-modelo\n",
    "meta_model_lr.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Predicciones finales\n",
    "y_meta_lr_pred = meta_model_lr.predict(X_meta_test)\n",
    "\n",
    "# Evaluación\n",
    "print(\"F1-macro (LogisticRegression meta-modelo):\", f1_score(y_meta_test, y_meta_lr_pred, average='macro'))\n",
    "print(\"\\nReporte de clasificación (LogisticRegression meta-modelo):\")\n",
    "print(classification_report(y_meta_test, y_meta_lr_pred))\n",
    "print(\"\\n\\nMatriz de confusión con LogisticRegression como meta-modelo:\\n\")\n",
    "print(confusion_matrix(y_meta_test,y_meta_lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccea91fc",
   "metadata": {},
   "source": [
    "#### Usando `XGBoost` como meta-modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0d66a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro (XGBoost meta-modelo): 0.3630376736135337\n",
      "\n",
      "Reporte de clasificación (XGBoost meta-modelo):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.14      0.16     24445\n",
      "           1       0.75      0.84      0.80     82928\n",
      "           2       0.12      0.01      0.01      2544\n",
      "           3       0.38      0.68      0.49       110\n",
      "\n",
      "    accuracy                           0.67    110027\n",
      "   macro avg       0.36      0.42      0.36    110027\n",
      "weighted avg       0.62      0.67      0.64    110027\n",
      "\n",
      "\n",
      "\n",
      "Matriz de confusión con XGBoost como meta-modelo:\n",
      "\n",
      "[[ 3338 21092    11     4]\n",
      " [12914 69887    53    74]\n",
      " [  845  1640    13    46]\n",
      " [    0     0    35    75]]\n"
     ]
    }
   ],
   "source": [
    "# Meta-modelo basado en XGBoost\n",
    "meta_model_xgb = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=len(np.unique(y_meta_train)),\n",
    "    tree_method='hist',\n",
    "    device='cuda',              # Usa GPU\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=250,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=1,\n",
    "    random_state=42,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "# Entrenamiento del meta-modelo\n",
    "meta_model_xgb.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Predicciones finales\n",
    "y_meta_xgb_pred = meta_model_xgb.predict(X_meta_test)\n",
    "\n",
    "# Evaluación\n",
    "print(\"F1-macro (XGBoost meta-modelo):\", f1_score(y_meta_test, y_meta_xgb_pred, average='macro'))\n",
    "print(\"\\nReporte de clasificación (XGBoost meta-modelo):\")\n",
    "print(classification_report(y_meta_test, y_meta_xgb_pred))\n",
    "print(\"\\n\\nMatriz de confusión con XGBoost como meta-modelo:\\n\")\n",
    "print(confusion_matrix(y_meta_test,y_meta_xgb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99985f4b",
   "metadata": {},
   "source": [
    "### Usando `MLP` como meta-modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76bf2e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para MLP:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.16      0.20     24445\n",
      "           1       0.75      0.65      0.70     82928\n",
      "           2       0.04      0.39      0.08      2544\n",
      "           3       0.41      0.98      0.58       110\n",
      "\n",
      "    accuracy                           0.54    110027\n",
      "   macro avg       0.37      0.55      0.39    110027\n",
      "weighted avg       0.63      0.54      0.57    110027\n",
      "\n",
      "\n",
      "\n",
      "Matriz de confusión con MLP como meta-modelo:\n",
      "\n",
      "[[ 3951 17067  3419     8]\n",
      " [10193 54286 18356    93]\n",
      " [  494  1013   982    55]\n",
      " [    0     0     2   108]]\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled = X_meta_train.astype('float32')\n",
    "X_valid_scaled = X_meta_test.astype('float32')\n",
    "\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_meta_train)\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 64, 4),\n",
    "    activation='tanh',\n",
    "    solver='adam',\n",
    "    learning_rate_init=0.0001,\n",
    "    alpha=0.001,\n",
    "    batch_size=256,\n",
    "    max_iter=50,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=20,\n",
    "    random_state=42,\n",
    "    warm_start=True,\n",
    "    validation_fraction=0.05\n",
    ")\n",
    "\n",
    "# Armamos un ciclo para asegurar que nuestro modelo MLP pueda\n",
    "# ver la mayoria de casos del test de entrenamiento.\n",
    "for i in range(6):\n",
    "    mlp.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
    "\n",
    "y_pred = mlp.predict(X_valid_scaled)\n",
    "\n",
    "y_pred_proba_mlp = mlp.predict_proba(X_valid_scaled)\n",
    "\n",
    "print(\"Resultados para MLP:\\n\")\n",
    "print(classification_report(y_meta_test, y_pred))\n",
    "print(\"\\n\\nMatriz de confusión con MLP como meta-modelo:\\n\")\n",
    "print(confusion_matrix(y_meta_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3960450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados por modelo:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Tipo de Ensamble</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>Precision-macro</th>\n",
       "      <th>Recall-macro</th>\n",
       "      <th>Tiempo Entrenamiento (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM Individual</td>\n",
       "      <td>Individual</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.551</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Voting (LGBM + CatBoost)</td>\n",
       "      <td>Ensamble heterogéneo</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.551</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bagging LightGBM</td>\n",
       "      <td>Ensamble homogéneo</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.545</td>\n",
       "      <td>645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stacking Heterogéneo (Meta: LightGBM)</td>\n",
       "      <td>Ensamble heterogéneo</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.520</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stacking Heterogéneo (Meta: CatBoost)</td>\n",
       "      <td>Ensamble heterogéneo</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.530</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stacking Heterogéneo (Meta: LogisticRegression)</td>\n",
       "      <td>Ensamble heterogéneo</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.520</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CatBoost Individual</td>\n",
       "      <td>Individual</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.590</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Stacking LightGBM</td>\n",
       "      <td>Ensamble homogéneo</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.490</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bagging CatBoost</td>\n",
       "      <td>Ensamble homogéneo</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.586</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Stacking Heterogéneo (Meta: MLP)</td>\n",
       "      <td>Ensamble heterogéneo</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.550</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Stacking Heterogéneo (Meta: RandomForest)</td>\n",
       "      <td>Ensamble heterogéneo</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.500</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Stacking Heterogéneo (Meta: XGBoost)</td>\n",
       "      <td>Ensamble heterogéneo</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.420</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Stacking CatBoost</td>\n",
       "      <td>Ensamble homogéneo</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.349</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Modelo      Tipo de Ensamble  \\\n",
       "0                               LightGBM Individual            Individual   \n",
       "1                          Voting (LGBM + CatBoost)  Ensamble heterogéneo   \n",
       "2                                  Bagging LightGBM    Ensamble homogéneo   \n",
       "3             Stacking Heterogéneo (Meta: LightGBM)  Ensamble heterogéneo   \n",
       "4             Stacking Heterogéneo (Meta: CatBoost)  Ensamble heterogéneo   \n",
       "5   Stacking Heterogéneo (Meta: LogisticRegression)  Ensamble heterogéneo   \n",
       "6                               CatBoost Individual            Individual   \n",
       "7                                 Stacking LightGBM    Ensamble homogéneo   \n",
       "8                                  Bagging CatBoost    Ensamble homogéneo   \n",
       "9                  Stacking Heterogéneo (Meta: MLP)  Ensamble heterogéneo   \n",
       "10        Stacking Heterogéneo (Meta: RandomForest)  Ensamble heterogéneo   \n",
       "11             Stacking Heterogéneo (Meta: XGBoost)  Ensamble heterogéneo   \n",
       "12                                Stacking CatBoost    Ensamble homogéneo   \n",
       "\n",
       "    F1-macro  Precision-macro  Recall-macro  Tiempo Entrenamiento (s)  \n",
       "0      0.435            0.390         0.551                        47  \n",
       "1      0.434            0.389         0.551                        75  \n",
       "2      0.433            0.391         0.545                       645  \n",
       "3      0.420            0.370         0.520                        13  \n",
       "4      0.410            0.370         0.530                       174  \n",
       "5      0.410            0.370         0.520                         8  \n",
       "6      0.400            0.370         0.590                        27  \n",
       "7      0.400            0.384         0.490                       325  \n",
       "8      0.392            0.360         0.586                       138  \n",
       "9      0.390            0.370         0.550                       167  \n",
       "10     0.380            0.350         0.500                        17  \n",
       "11     0.360            0.360         0.420                         7  \n",
       "12     0.345            0.378         0.349                        64  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model_results = [\n",
    "    # --- Modelos individuales ---\n",
    "    [\"LightGBM Individual\", \"Individual\", 0.435, 0.390, 0.551, 47],\n",
    "    [\"CatBoost Individual\", \"Individual\", 0.400, 0.370, 0.590, 27],\n",
    "\n",
    "    # --- Modelos con Bagging ---\n",
    "    [\"Bagging LightGBM\", \"Ensamble homogéneo\", 0.433, 0.391, 0.545, 645],\n",
    "    [\"Bagging CatBoost\", \"Ensamble homogéneo\", 0.392, 0.360, 0.586, 138],\n",
    "\n",
    "    # --- Modelos con Stacking homogéneo ---\n",
    "    [\"Stacking LightGBM\", \"Ensamble homogéneo\", 0.400, 0.384, 0.490, 325],\n",
    "    [\"Stacking CatBoost\", \"Ensamble homogéneo\", 0.345, 0.378, 0.349, 64],\n",
    "\n",
    "    # --- VotingClassifier heterogéneo ---\n",
    "    [\"Voting (LGBM + CatBoost)\", \"Ensamble heterogéneo\", 0.434, 0.389, 0.551, 75],\n",
    "\n",
    "    # --- Stacking heterogéneo (meta-modelos) ---\n",
    "    [\"Stacking Heterogéneo (Meta: LightGBM)\", \"Ensamble heterogéneo\", 0.420, 0.370, 0.520, 13],\n",
    "    [\"Stacking Heterogéneo (Meta: CatBoost)\", \"Ensamble heterogéneo\", 0.410, 0.370, 0.530, 174],\n",
    "    [\"Stacking Heterogéneo (Meta: RandomForest)\", \"Ensamble heterogéneo\", 0.380, 0.350, 0.500, 17],\n",
    "    [\"Stacking Heterogéneo (Meta: LogisticRegression)\", \"Ensamble heterogéneo\", 0.410, 0.370, 0.520, 8],\n",
    "    [\"Stacking Heterogéneo (Meta: XGBoost)\", \"Ensamble heterogéneo\", 0.360, 0.360, 0.420, 7],\n",
    "    [\"Stacking Heterogéneo (Meta: MLP)\", \"Ensamble heterogéneo\", 0.390, 0.370, 0.550, 167]\n",
    "]\n",
    "\n",
    "\n",
    "df_comparativa = pd.DataFrame(\n",
    "    model_results,\n",
    "    columns=[\"Modelo\", \"Tipo de Ensamble\", \"F1-macro\", \"Precision-macro\", \"Recall-macro\", \"Tiempo Entrenamiento (s)\"]\n",
    ")\n",
    "\n",
    "# Ordenar por F1-macro (mayor a menor)\n",
    "df_comparativa = df_comparativa.sort_values(by=\"F1-macro\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Redondear métricas a 4 decimales\n",
    "df_comparativa[[\"F1-macro\", \"Precision-macro\", \"Recall-macro\"]] = df_comparativa[[\"F1-macro\", \"Precision-macro\", \"Recall-macro\"]].round(4)\n",
    "\n",
    "# Mostrar tabla\n",
    "print(\"\\nResultados por modelo:\\n\")\n",
    "display(df_comparativa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3249c4",
   "metadata": {},
   "source": [
    "# Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7234989",
   "metadata": {},
   "source": [
    "- Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), 123–140.\n",
    "- Freund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1), 119–139.\n",
    "- Kuncheva, L. I. (2004). Combining pattern classifiers: Methods and algorithms. Wiley-Interscience.\n",
    "- Rokach, L. (2010). Ensemble-based classifiers. Artificial Intelligence Review, 33(1), 1–39.\n",
    "- Wolpert, D. H. (1992). Stacked generalization. Neural Networks, 5(2), 241–259."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch311 (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
