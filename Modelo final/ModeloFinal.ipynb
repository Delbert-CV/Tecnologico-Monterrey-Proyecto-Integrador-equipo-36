{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c403c50",
   "metadata": {},
   "source": [
    "# EQUIPO 36 | Avance 5: Modelo Final\n",
    "## Proyecto: Predicción de infestaciones de gorgojo del agave\n",
    "## Integrantes equipo 36:\n",
    "\n",
    "| Nombre | Matrícula |\n",
    "| ------ | --------- |\n",
    "| André Martins Cordebello | A00572928 |\n",
    "| Enrique Eduardo Solís Da Costa | A00572678 |\n",
    "| Delbert Francisco Custodio Vargas | A01795613 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "130be08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from imblearn.ensemble  import BalancedRandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from bayes_opt import BayesianOptimization\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import BallTree\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import optuna\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ec926e",
   "metadata": {},
   "source": [
    "## Cargamos el dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e928974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data_with_weather_information.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad8b6e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tramp_id                               object\n",
       "sampling_date                  datetime64[ns]\n",
       "lat                                   float64\n",
       "lon                                   float64\n",
       "municipality                           object\n",
       "plantation_age                          int64\n",
       "capture_count                         float64\n",
       "state                                  object\n",
       "square_area_imputed                   float64\n",
       "month                                   int64\n",
       "year                                    int64\n",
       "year-month                             object\n",
       "day_of_year_sin                       float64\n",
       "day_of_year_cos                       float64\n",
       "day_of_week_sin                       float64\n",
       "day_of_week_cos                       float64\n",
       "week_of_year_sin                      float64\n",
       "week_of_year_cos                      float64\n",
       "month_sin                             float64\n",
       "month_cos                             float64\n",
       "critical_season                         int64\n",
       "severity_encoded                        int64\n",
       "distance_to_nearest_hotspot           float64\n",
       "hotspots_within_5km                     int64\n",
       "precipitation                         float64\n",
       "avg_temp                              float64\n",
       "max_temp                              float64\n",
       "min_temp                              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d6ed3",
   "metadata": {},
   "source": [
    "# Modelos individuales: `LightGBM` y `CatBoost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "76d653b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiamos el dataframe con la información\n",
    "train_test_df = df.copy()\n",
    "train_test_df = train_test_df.sort_values(by='sampling_date').reset_index(drop=True)\n",
    "\n",
    "# Hacemos un encoding basico para State y Municipalidad\n",
    "for col in ['state', 'municipality']:\n",
    "    le = LabelEncoder()\n",
    "    train_test_df[col] = le.fit_transform(train_test_df[col])\n",
    "\n",
    "# Generamos la mascara para obtener los datos de antes del 2025 y del 2025 por separado\n",
    "train_mask = train_test_df['sampling_date'].dt.year < 2025\n",
    "test_mask  = train_test_df['sampling_date'].dt.year == 2025\n",
    "\n",
    "# Excluimos la variable objetivo (severity_encoded) y algunos variables o features que ya tenemos contenidos en nuestros\n",
    "# features creados. `capture_count` no podemos tomarlo en cuenta porque se relaciona directamente con la severidad.\n",
    "exclude_cols = [\n",
    "    'severity_encoded','tramp_id', 'capture_count', \n",
    "    'month', 'year-month', 'sampling_date', 'municipality', \n",
    "    'state'\n",
    "]\n",
    "\n",
    "# Cargamos los features a tomar en cuenta (obviamos los features en exclude_cols)\n",
    "features = [col for col in train_test_df.columns if col not in exclude_cols]\n",
    "\n",
    "# Generamos nuestro split de entrenamiento y test por medio de las mascaras train_mask y test_mask\n",
    "X_train, y_train = train_test_df.loc[train_mask, features], train_test_df.loc[train_mask, 'severity_encoded'] # El train dataset es la data historica de 2014 a 2024\n",
    "X_test,  y_test  = train_test_df.loc[test_mask,  features], train_test_df.loc[test_mask,  'severity_encoded'] # El test dataset es la data a partir de 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea68086",
   "metadata": {},
   "source": [
    "### Modelo `LightGBM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "388366bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Resultados para LightGBM después de usar Optuna:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.263     0.330     0.293     24445\n",
      "           1      0.771     0.707     0.737     82928\n",
      "           2      0.131     0.153     0.141      2544\n",
      "           3      0.383     1.000     0.554       110\n",
      "\n",
      "    accuracy                          0.611    110027\n",
      "   macro avg      0.387     0.548     0.431    110027\n",
      "weighted avg      0.643     0.611     0.625    110027\n",
      "\n",
      "\n",
      "\n",
      "Matriz de confusión para LightGBM:\n",
      "\n",
      "[[ 8079 16056   296    14]\n",
      " [21921 58619  2282   106]\n",
      " [  725  1373   389    57]\n",
      " [    0     0     0   110]]\n"
     ]
    }
   ],
   "source": [
    "LightGBM_X_train = X_train.copy()\n",
    "LightGBM_X_test  = X_test.copy()\n",
    "LightGBM_Y_train = y_train.copy()\n",
    "LightGBM_Y_test  = y_test.copy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "LightGBM_X_train[['distance_to_nearest_hotspot']] = scaler.fit_transform(LightGBM_X_train[['distance_to_nearest_hotspot']])\n",
    "LightGBM_X_test[['distance_to_nearest_hotspot']] = scaler.transform(LightGBM_X_test[['distance_to_nearest_hotspot']])\n",
    "\n",
    "lgbm_best_params = LGBMClassifier(\n",
    "    boosting_type = \"gbdt\",\n",
    "    objective = \"multiclass\",\n",
    "    num_class = 4,\n",
    "    class_weight = \"balanced\",\n",
    "    is_unbalance = False,\n",
    "    device_type = \"gpu\",\n",
    "    min_gain_to_split = 0.001,  \n",
    "    random_state = 42,\n",
    "    verbose = -1,\n",
    "    learning_rate= 0.031205207400998834,\n",
    "    num_leaves= 110,\n",
    "    max_depth= 11,\n",
    "    feature_fraction= 0.91959030797181,\n",
    "    bagging_fraction= 0.7694621015318531,\n",
    "    lambda_l1= 1.8616621273598788,\n",
    "    lambda_l2= 2.6453430076619573,\n",
    "    min_child_samples= 70,\n",
    "    n_estimators= 299\n",
    ")\n",
    "\n",
    "lgbm_best_params.fit(LightGBM_X_train, LightGBM_Y_train)\n",
    "\n",
    "y_pred_lgbm_best = lgbm_best_params.predict(LightGBM_X_test, categorical_features=['critical_season'])\n",
    "\n",
    "\n",
    "print(\"\\n\\nResultados para LightGBM después de usar Optuna:\\n\\n\")\n",
    "print(classification_report(LightGBM_Y_test, y_pred_lgbm_best, digits=3))\n",
    "\n",
    "print(\"\\n\\nMatriz de confusión para LightGBM:\\n\")\n",
    "print(confusion_matrix(y_test, y_pred_lgbm_best))\n",
    "y_pred_proba_lgbm_best_params = lgbm_best_params.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "15bc4f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo para su futuro uso\n",
    "with open('ligthgbm.pkl', 'wb') as file:\n",
    "    pickle.dump(lgbm_best_params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e90234",
   "metadata": {},
   "source": [
    "### Modelo `CatBoost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "36cce033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para CatBoost según Optuna:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.20      0.24     24445\n",
      "           1       0.76      0.70      0.73     82928\n",
      "           2       0.07      0.48      0.12      2544\n",
      "           3       0.33      1.00      0.49       110\n",
      "\n",
      "    accuracy                           0.58    110027\n",
      "   macro avg       0.37      0.59      0.40    110027\n",
      "weighted avg       0.64      0.58      0.61    110027\n",
      "\n",
      "\n",
      "Matriz de confusión para CatBoost:\n",
      "[[ 4965 17050  2411    19]\n",
      " [11127 57787 13869   145]\n",
      " [  161  1105  1214    64]\n",
      " [    0     0     0   110]]\n"
     ]
    }
   ],
   "source": [
    "CatBoost_X_train = X_train.copy()\n",
    "CatBoost_X_test  = X_test.copy()\n",
    "CatBoost_Y_train = y_train.copy()\n",
    "CatBoost_Y_test  = y_test.copy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "CatBoost_X_train[['distance_to_nearest_hotspot']] = scaler.fit_transform(CatBoost_X_train[['distance_to_nearest_hotspot']])\n",
    "CatBoost_X_test[['distance_to_nearest_hotspot']] = scaler.transform(CatBoost_X_test[['distance_to_nearest_hotspot']])\n",
    "\n",
    "\n",
    "class_counts = CatBoost_Y_train.value_counts().sort_index()\n",
    "num_classes = len(class_counts)\n",
    "total = len(CatBoost_Y_train)\n",
    "class_weights = {i: total / (num_classes * count) for i, count in class_counts.items()}\n",
    "\n",
    "weights = CatBoost_Y_train.map(class_weights)\n",
    "\n",
    "cat_model_best = CatBoostClassifier(                 \n",
    "    loss_function='MultiClass',    \n",
    "    eval_metric='TotalF1',         \n",
    "    auto_class_weights='Balanced', \n",
    "    random_seed=42,\n",
    "    task_type='GPU',               \n",
    "    #verbose=100,\n",
    "    iterations= 900,\n",
    "    learning_rate= 0.03169683936081736,\n",
    "    depth= 8,\n",
    "    l2_leaf_reg= 1.0126729009882989,\n",
    "    bagging_temperature= 0.13742770730370382,\n",
    "    border_count= 238,\n",
    "    random_strength= 1.490982729702571,\n",
    "    grow_policy= 'SymmetricTree',\n",
    "    logging_level= 'Silent'\n",
    ")\n",
    "\n",
    "categorical_cols = ['critical_season']\n",
    "\n",
    "cat_model_best.fit(\n",
    "    CatBoost_X_train,\n",
    "    CatBoost_Y_train,\n",
    "    cat_features=categorical_cols if 'categorical_cols' in locals() else None,\n",
    "    eval_set=(CatBoost_X_test, CatBoost_Y_test),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "y_pred_cd_best = cat_model_best.predict(CatBoost_X_test)\n",
    "y_pred_cd_best = y_pred_cd_best.flatten()\n",
    "\n",
    "print(\"Resultados para CatBoost según Optuna:\\n\")\n",
    "print(classification_report(CatBoost_Y_test, y_pred_cd_best))\n",
    "\n",
    "\n",
    "print(\"\\nMatriz de confusión para CatBoost:\")\n",
    "print(confusion_matrix(CatBoost_Y_test, y_pred_cd_best))\n",
    "y_pred_proba_cv_best_params = cat_model_best.predict_proba(CatBoost_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ab78f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo para su futuro uso\n",
    "with open('catboost.pkl', 'wb') as file:\n",
    "    pickle.dump(cat_model_best, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ee10b",
   "metadata": {},
   "source": [
    "# Estrategias de ensamble homogeneas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fbbfd1",
   "metadata": {},
   "source": [
    "## `LightGBM Bagging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3640b4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro para Bagging con LightGBM: 0.4316\n",
      "\n",
      "\n",
      "Resultados para LightGBM después de usar Optuna:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.257     0.283     0.269     24445\n",
      "           1      0.767     0.747     0.757     82928\n",
      "           2      0.171     0.132     0.149      2544\n",
      "           3      0.381     1.000     0.551       110\n",
      "\n",
      "    accuracy                          0.630    110027\n",
      "   macro avg      0.394     0.541     0.432    110027\n",
      "weighted avg      0.639     0.630     0.634    110027\n",
      "\n",
      "\n",
      "\n",
      "Matriz de confusión para LightGBM Bagging:\n",
      "\n",
      "[[ 6914 17361   155    15]\n",
      " [19367 61978  1476   107]\n",
      " [  650  1501   336    57]\n",
      " [    0     0     0   110]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Cargamos nuestros train y test sets\n",
    "Bagging_X_train = X_train.copy()\n",
    "Bagging_Y_train = y_train.copy()\n",
    "Bagging_X_test  = X_test.copy()\n",
    "Bagging_Y_test  = y_test.copy()\n",
    "\n",
    "#  Cantidad de modelos a generar para el Bagging\n",
    "n_models = 15\n",
    "models = []\n",
    "oof_preds = []\n",
    "test_preds = []\n",
    "\n",
    "\n",
    "for i in range(n_models):\n",
    "    \n",
    "    # Generamos nuevos sets de entrenamiento y test por cada ciclo\n",
    "    X_train_sub, y_train_sub = resample(Bagging_X_train, Bagging_Y_train, replace=True, random_state=42 + i)\n",
    "    \n",
    "    # Instanciamos LightGBM Classifier\n",
    "    model = lgb.LGBMClassifier( \n",
    "                                boosting_type = \"gbdt\",\n",
    "                                objective = \"multiclass\",\n",
    "                                num_class = 4,\n",
    "                                class_weight = \"balanced\",\n",
    "                                is_unbalance = False,\n",
    "                                device_type = \"gpu\",\n",
    "                                min_gain_to_split = 0.001,  \n",
    "                                random_state = 42,\n",
    "                                verbose = -1,\n",
    "                                learning_rate= 0.031205207400998834,\n",
    "                                num_leaves= 110,\n",
    "                                max_depth= 11,\n",
    "                                feature_fraction= 0.91959030797181,\n",
    "                                bagging_fraction= 0.7694621015318531,\n",
    "                                lambda_l1= 1.8616621273598788,\n",
    "                                lambda_l2= 2.6453430076619573,\n",
    "                                min_child_samples= 70,\n",
    "                                n_estimators= 299\n",
    "                            )\n",
    "    \n",
    "    # Entrenamos sobre los sets de entrenamiento y test por ciclo\n",
    "    model.fit(X_train_sub, y_train_sub)\n",
    "    models.append(model)\n",
    "\n",
    "    # Guardamos las predicciones out-of-fold\n",
    "    y_pred = model.predict_proba(Bagging_X_test)\n",
    "    test_preds.append(y_pred)\n",
    "\n",
    "# Promediamos los resultados\n",
    "avg_proba = np.mean(test_preds, axis=0)\n",
    "y_pred_final = np.argmax(avg_proba, axis=1)\n",
    "\n",
    "# Calculamos el F1 Score\n",
    "f1_macro = f1_score(Bagging_Y_test, y_pred_final, average=\"macro\")\n",
    "print(f\"F1-macro para Bagging con LightGBM: {f1_macro:.4f}\")\n",
    "\n",
    "# Imprimimos resultados\n",
    "print(\"\\n\\nResultados para LightGBM después de usar Bagging:\\n\\n\")\n",
    "print(classification_report(Bagging_Y_test, y_pred_final, digits=3))\n",
    "\n",
    "print(\"\\n\\nMatriz de confusión para LightGBM Bagging:\\n\")\n",
    "print(confusion_matrix(Bagging_Y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd2a96",
   "metadata": {},
   "source": [
    "## `LightGBM Stacking`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f003205e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados luego de hacer LightGBM Stacking\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2677    0.2617    0.2647     24445\n",
      "           1     0.7679    0.7950    0.7812     82928\n",
      "           2     0.0800    0.0008    0.0016      2544\n",
      "           3     0.3976    0.9182    0.5549       110\n",
      "\n",
      "    accuracy                         0.6583    110027\n",
      "   macro avg     0.3783    0.4939    0.4006    110027\n",
      "weighted avg     0.6405    0.6583    0.6482    110027\n",
      "\n",
      "\n",
      "Matriz de confusion para LightGBM stacking:\n",
      "\n",
      "[[ 6397 18034     6     8]\n",
      " [16898 65927    11    92]\n",
      " [  599  1890     2    53]\n",
      " [    0     3     6   101]]\n"
     ]
    }
   ],
   "source": [
    "X_train_stack = X_train.copy()\n",
    "y_train_stack = y_train.copy()\n",
    "X_test_stack  = X_test.copy()\n",
    "y_test_stack  = y_test.copy()\n",
    "\n",
    "lgbm_1 = lgb.LGBMClassifier(\n",
    "    boosting_type='gbdt', \n",
    "    objective='multiclass', \n",
    "    num_class=4,            \n",
    "    class_weight='balanced',\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    max_depth=-1,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    device='gpu',\n",
    "    is_unbalance=False\n",
    ")\n",
    "\n",
    "lgbm_2 = lgb.LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=4,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=10,\n",
    "    feature_fraction=0.9,\n",
    "    bagging_fraction=0.8,\n",
    "    random_state=42,\n",
    "    device_type='gpu',\n",
    "    is_unbalance=False,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "lgbm_3 = lgb.LGBMClassifier(\n",
    "    boosting_type = \"gbdt\",\n",
    "    objective = \"multiclass\",\n",
    "    num_class = 4,\n",
    "    class_weight = \"balanced\",\n",
    "    is_unbalance = False,\n",
    "    device_type = \"gpu\",\n",
    "    min_gain_to_split = 0.001,  \n",
    "    random_state = 42,\n",
    "    verbose = -1,\n",
    "    learning_rate= 0.031205207400998834,\n",
    "    num_leaves= 110,\n",
    "    max_depth= 11,\n",
    "    feature_fraction= 0.91959030797181,\n",
    "    bagging_fraction= 0.7694621015318531,\n",
    "    lambda_l1= 1.8616621273598788,\n",
    "    lambda_l2= 2.6453430076619573,\n",
    "    min_child_samples= 70,\n",
    "    n_estimators= 299\n",
    ")\n",
    "\n",
    "meta_model = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "\n",
    "\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lgbm1', lgbm_1),\n",
    "        ('lgbm2', lgbm_2),\n",
    "        ('lgbm3', lgbm_3)\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stack_model.fit(X_train_stack, y_train_stack)\n",
    "\n",
    "y_pred = stack_model.predict(X_test_stack)\n",
    "\n",
    "print(\"Resultados luego de hacer LightGBM Stacking\")\n",
    "print(classification_report(y_test_stack, y_pred, digits=4))\n",
    "\n",
    "print(\"\\nMatriz de confusion para LightGBM stacking:\\n\")\n",
    "print(confusion_matrix(y_test_stack, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0810ca",
   "metadata": {},
   "source": [
    "## `CatBoost Bagging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e9c96649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro para Bagging con CatBoost: 0.3919\n",
      "\n",
      "\n",
      "Resultados para CatBoost después de usar Bagging:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.286     0.156     0.202     24445\n",
      "           1      0.752     0.737     0.744     82928\n",
      "           2      0.076     0.451     0.131      2544\n",
      "           3      0.325     1.000     0.491       110\n",
      "\n",
      "    accuracy                          0.601    110027\n",
      "   macro avg      0.360     0.586     0.392    110027\n",
      "weighted avg      0.632     0.601     0.609    110027\n",
      "\n",
      "\n",
      "\n",
      "Matriz de confusión para CatBoost Bagging:\n",
      "\n",
      "[[ 3821 18976  1629    19]\n",
      " [ 9445 61088 12250   145]\n",
      " [  113  1220  1147    64]\n",
      " [    0     0     0   110]]\n"
     ]
    }
   ],
   "source": [
    "# Generamos nuestros Train y Test datasets\n",
    "Bagging_X_train = X_train.copy()\n",
    "Bagging_Y_train = y_train.copy()\n",
    "Bagging_X_test  = X_test.copy()\n",
    "Bagging_Y_test  = y_test.copy()\n",
    "\n",
    "#  Cantidad de modelos a generar para el Bagging\n",
    "n_models = 5\n",
    "models = []\n",
    "oof_preds = []\n",
    "test_preds = []\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "Bagging_X_train[['distance_to_nearest_hotspot']] = scaler.fit_transform(Bagging_X_train[['distance_to_nearest_hotspot']])\n",
    "Bagging_X_test[['distance_to_nearest_hotspot']] = scaler.transform(Bagging_X_test[['distance_to_nearest_hotspot']])\n",
    "\n",
    "\n",
    "class_counts = Bagging_Y_train.value_counts().sort_index()\n",
    "num_classes = len(class_counts)\n",
    "total = len(Bagging_Y_train)\n",
    "class_weights = {i: total / (num_classes * count) for i, count in class_counts.items()}\n",
    "\n",
    "weights = Bagging_Y_train.map(class_weights)\n",
    "\n",
    "categorical_cols = ['critical_season']\n",
    "\n",
    "for i in range(n_models):\n",
    "    \n",
    "    X_train_sub, y_train_sub = resample(Bagging_X_train, Bagging_Y_train, replace=True, random_state=42 + i)\n",
    "\n",
    "    model = CatBoostClassifier(                 \n",
    "        loss_function='MultiClass',    \n",
    "        eval_metric='TotalF1',         \n",
    "        auto_class_weights='Balanced', \n",
    "        random_seed=42,\n",
    "        task_type='GPU',               \n",
    "        #verbose=100,\n",
    "        iterations= 900,\n",
    "        learning_rate= 0.03169683936081736,\n",
    "        depth= 8,\n",
    "        l2_leaf_reg= 1.0126729009882989,\n",
    "        bagging_temperature= 0.13742770730370382,\n",
    "        border_count= 238,\n",
    "        random_strength= 1.490982729702571,\n",
    "        grow_policy= 'SymmetricTree',\n",
    "        logging_level= 'Silent'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    model.fit(\n",
    "        X_train_sub,\n",
    "        y_train_sub,\n",
    "        cat_features=categorical_cols if 'categorical_cols' in locals() else None,\n",
    "        eval_set=(Bagging_X_test, Bagging_Y_test),\n",
    "        use_best_model=True,\n",
    "        verbose=False)\n",
    "    \n",
    "    models.append(model)\n",
    "\n",
    "    # Guardamos las predicciones out-of-fold\n",
    "    y_pred = model.predict_proba(Bagging_X_test)\n",
    "    test_preds.append(y_pred)\n",
    "    \n",
    "    \n",
    "avg_proba = np.mean(test_preds, axis=0)\n",
    "y_pred_final = np.argmax(avg_proba, axis=1)\n",
    "\n",
    "f1_macro = f1_score(Bagging_Y_test, y_pred_final, average=\"macro\")\n",
    "print(f\"F1-macro para Bagging con CatBoost: {f1_macro:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nResultados para CatBoost después de usar Bagging:\\n\\n\")\n",
    "print(classification_report(Bagging_Y_test, y_pred_final, digits=3))\n",
    "\n",
    "print(\"\\n\\nMatriz de confusión para CatBoost Bagging:\\n\")\n",
    "print(confusion_matrix(Bagging_Y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091641a",
   "metadata": {},
   "source": [
    "## `Stacking CatBoost` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "59356e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CatBoost #1 ...\n",
      "Modelo #1 Listo — F1-macro: 0.4071 | Tiempo: 25.74s\n",
      "\n",
      "CatBoost #2 ...\n",
      "Modelo #2 Listo — F1-macro: 0.3695 | Tiempo: 16.61s\n",
      "\n",
      "CatBoost #3 ...\n",
      "Modelo #3 Listo — F1-macro: 0.3757 | Tiempo: 13.60s\n",
      "              Model  F1-macro  Training Time (s)\n",
      "0  CatBoost_Model_1  0.407078          25.741772\n",
      "1  CatBoost_Model_2  0.369531          16.608711\n",
      "2  CatBoost_Model_3  0.375697          13.601240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CatBoost Stacking resultados:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2302    0.1933    0.2102     24445\n",
      "           1     0.7579    0.8163    0.7860     82928\n",
      "           2     0.1515    0.0039    0.0077      2544\n",
      "           3     0.3717    0.3818    0.3767       110\n",
      "\n",
      "    accuracy                         0.6587    110027\n",
      "   macro avg     0.3778    0.3489    0.3451    110027\n",
      "weighted avg     0.6263    0.6587    0.6397    110027\n",
      "\n",
      "\n",
      "Matriz de confusion:\n",
      "[[ 4726 19711     3     5]\n",
      " [15156 67697    27    48]\n",
      " [  647  1869    10    18]\n",
      " [    0    42    26    42]]\n"
     ]
    }
   ],
   "source": [
    "X_train_stack = X_train.copy()\n",
    "y_train_stack = y_train.copy()\n",
    "X_test_stack  = X_test.copy()\n",
    "y_test_stack  = y_test.copy()\n",
    "\n",
    "\n",
    "cb_1 = CatBoostClassifier(                 \n",
    "    loss_function='MultiClass',    \n",
    "    eval_metric='TotalF1',         \n",
    "    auto_class_weights='Balanced', \n",
    "    random_seed=42,\n",
    "    task_type='GPU',               \n",
    "    iterations=900,\n",
    "    learning_rate=0.03169683936081736,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=1.0126729009882989,\n",
    "    bagging_temperature=0.13742770730370382,\n",
    "    border_count=238,\n",
    "    random_strength=1.490982729702571,\n",
    "    grow_policy='SymmetricTree',\n",
    "    verbose=0  # evita conflictos con verbose\n",
    ")\n",
    "\n",
    "cb_2 = CatBoostClassifier(\n",
    "    iterations=300,\n",
    "    learning_rate=0.05,\n",
    "    depth=10,\n",
    "    l2_leaf_reg=3,\n",
    "    loss_function=\"MultiClass\",\n",
    "    eval_metric=\"TotalF1\",\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    task_type=\"GPU\",\n",
    "    random_seed=100,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "cb_3 = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.02,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=2,\n",
    "    loss_function=\"MultiClass\",\n",
    "    eval_metric=\"TotalF1\",\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    task_type=\"GPU\",\n",
    "    random_seed=2025,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "meta_model = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "\n",
    "models = [cb_1, cb_2, cb_3]\n",
    "\n",
    "train_probas = []\n",
    "test_probas  = []\n",
    "results = []\n",
    "\n",
    "for i, model in enumerate(models, 1):\n",
    "    print(f\"\\nCatBoost #{i} ...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    model.fit(X_train_stack, y_train_stack)\n",
    "    \n",
    "    y_train_pred = model.predict_proba(X_train_stack)\n",
    "    y_test_pred  = model.predict_proba(X_test_stack)\n",
    "    \n",
    "    train_probas.append(y_train_pred)\n",
    "    test_probas.append(y_test_pred)\n",
    "    \n",
    "    y_pred_class = np.argmax(y_test_pred, axis=1)\n",
    "    f1_macro = f1_score(y_test_stack, y_pred_class, average=\"macro\")\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    results.append([f\"CatBoost_Model_{i}\", f1_macro, elapsed])\n",
    "    print(f\"Modelo #{i} Listo — F1-macro: {f1_macro:.4f} | Tiempo: {elapsed:.2f}s\")\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"F1-macro\", \"Training Time (s)\"])\n",
    "print(results_df)\n",
    "\n",
    "X_meta_train = np.hstack(train_probas)\n",
    "X_meta_test  = np.hstack(test_probas)\n",
    "\n",
    "meta_model = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "meta_model.fit(X_meta_train, y_train_stack)\n",
    "\n",
    "y_pred_stack = meta_model.predict(X_meta_test)\n",
    "\n",
    "print(\"\\nCatBoost Stacking resultados:\")\n",
    "print(classification_report(y_test_stack, y_pred_stack, digits=4))\n",
    "print(\"\\nMatriz de confusion:\")\n",
    "print(confusion_matrix(y_test_stack, y_pred_stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af65c4bb",
   "metadata": {},
   "source": [
    "# Estrategias de ensamble heterogéneas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb8e29",
   "metadata": {},
   "source": [
    "## `VotingClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "28328fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 22:48:32,944] Using an existing study with name 'VotingClassifier_Optim' instead of creating a new one.\n",
      "[I 2025-10-21 22:48:42,403] Trial 20 finished with value: 0.5792678789619482 and parameters: {'lgb_weight': 0.4403983508656484, 'cb_weight': 0.14649566927715146}. Best is trial 6 with value: 0.5799062965369872.\n",
      "[I 2025-10-21 22:48:51,674] Trial 21 finished with value: 0.5793159451961619 and parameters: {'lgb_weight': 1.7154732357023226, 'cb_weight': 1.2617068154368596}. Best is trial 6 with value: 0.5799062965369872.\n",
      "[I 2025-10-21 22:49:00,919] Trial 22 finished with value: 0.5799092167221863 and parameters: {'lgb_weight': 0.6693836902992534, 'cb_weight': 0.011682848753281995}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:49:10,517] Trial 23 finished with value: 0.5798696485244378 and parameters: {'lgb_weight': 0.8110971831590188, 'cb_weight': 0.17634646392270098}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:49:20,130] Trial 24 finished with value: 0.5799068448558485 and parameters: {'lgb_weight': 0.48346031283850854, 'cb_weight': 0.006176208108062346}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:49:29,710] Trial 25 finished with value: 0.579873416927171 and parameters: {'lgb_weight': 0.4182587574829516, 'cb_weight': 0.011395018829100795}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:49:39,376] Trial 26 finished with value: 0.5792954489551035 and parameters: {'lgb_weight': 0.6265398876512444, 'cb_weight': 0.3228349726672441}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:49:48,597] Trial 27 finished with value: 0.5793285395586646 and parameters: {'lgb_weight': 0.2398293270937557, 'cb_weight': 0.14958693337142936}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:49:57,917] Trial 28 finished with value: 0.579315834302684 and parameters: {'lgb_weight': 0.5259444277932964, 'cb_weight': 0.3820426610880059}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:50:07,220] Trial 29 finished with value: 0.57927104332443 and parameters: {'lgb_weight': 0.7770753709541685, 'cb_weight': 0.6395207912407929}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:50:16,757] Trial 30 finished with value: 0.579263426266523 and parameters: {'lgb_weight': 0.3288807890284752, 'cb_weight': 0.1097931230125834}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:50:26,343] Trial 31 finished with value: 0.5798912065239469 and parameters: {'lgb_weight': 0.117969584092943, 'cb_weight': 0.00542309997763013}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:50:35,990] Trial 32 finished with value: 0.5793247426220578 and parameters: {'lgb_weight': 0.9542388095266149, 'cb_weight': 0.5178404907051413}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:50:45,741] Trial 33 finished with value: 0.5798598696770054 and parameters: {'lgb_weight': 1.158615937056079, 'cb_weight': 0.26233201914138354}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:50:55,434] Trial 34 finished with value: 0.579267479704903 and parameters: {'lgb_weight': 0.5396906130857679, 'cb_weight': 0.44688136892387326}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:51:05,041] Trial 35 finished with value: 0.5793145423420928 and parameters: {'lgb_weight': 0.2756013223147058, 'cb_weight': 0.19921772526239276}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:51:14,513] Trial 36 finished with value: 0.5793101464865422 and parameters: {'lgb_weight': 1.3214731338955845, 'cb_weight': 0.9941160326480822}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:51:24,208] Trial 37 finished with value: 0.5798947987657304 and parameters: {'lgb_weight': 0.402747306786312, 'cb_weight': 0.11764013451607744}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:51:33,767] Trial 38 finished with value: 0.5792937427099961 and parameters: {'lgb_weight': 0.6792129049278932, 'cb_weight': 0.34320116946901014}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:51:43,202] Trial 39 finished with value: 0.5792760988262652 and parameters: {'lgb_weight': 1.057578314793149, 'cb_weight': 1.340551752952996}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:51:52,775] Trial 40 finished with value: 0.5798814883317263 and parameters: {'lgb_weight': 0.916103785294569, 'cb_weight': 0.24102105248328956}. Best is trial 22 with value: 0.5799092167221863.\n",
      "[I 2025-10-21 22:52:02,495] Trial 41 finished with value: 0.5799116275863289 and parameters: {'lgb_weight': 1.9516866156032004, 'cb_weight': 0.6156930840292797}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-21 22:52:12,468] Trial 42 finished with value: 0.5792778126288513 and parameters: {'lgb_weight': 1.9163634163030963, 'cb_weight': 0.7707633332727977}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-21 22:52:22,213] Trial 43 finished with value: 0.5793905314731532 and parameters: {'lgb_weight': 1.8109049048545278, 'cb_weight': 0.5805487533849809}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-21 22:52:31,899] Trial 44 finished with value: 0.5792830468044375 and parameters: {'lgb_weight': 1.9799193272224818, 'cb_weight': 0.8543534517006881}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-21 22:52:41,485] Trial 45 finished with value: 0.5798964530973516 and parameters: {'lgb_weight': 1.66858686788798, 'cb_weight': 0.0912068225533378}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-21 22:52:50,909] Trial 46 finished with value: 0.5798566952563308 and parameters: {'lgb_weight': 1.8794378833229508, 'cb_weight': 0.3695367415220296}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-21 22:53:00,180] Trial 47 finished with value: 0.5792757942437431 and parameters: {'lgb_weight': 1.40341538576814, 'cb_weight': 0.6939172445112263}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-21 22:53:09,431] Trial 48 finished with value: 0.5798905111954463 and parameters: {'lgb_weight': 1.6229383893719525, 'cb_weight': 0.25802191026518506}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-21 22:53:18,911] Trial 49 finished with value: 0.5792018748328843 and parameters: {'lgb_weight': 1.5265706875635725, 'cb_weight': 1.7826928287024033}. Best is trial 41 with value: 0.5799116275863289.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mejor F1-Macro: 0.5799\n",
      "Pesos óptimos: {'lgb_weight': 1.9516866156032004, 'cb_weight': 0.6156930840292797}\n"
     ]
    }
   ],
   "source": [
    "Voting_X_train = X_train.copy()\n",
    "Voting_Y_train = y_train.copy()\n",
    "Voting_X_test  = X_test.copy()\n",
    "Voting_Y_test  = y_test.copy()\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "def objective(trial):\n",
    "    w1 = trial.suggest_float(\"lgb_weight\", 0.0, 2.0)\n",
    "    w2 = trial.suggest_float(\"cb_weight\", 0.0, 2.0)\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_idx, val_idx in tscv.split(Voting_X_train):\n",
    "        X_tr, X_val = Voting_X_train.iloc[train_idx], Voting_X_train.iloc[val_idx]\n",
    "        y_tr, y_val = Voting_Y_train.iloc[train_idx], Voting_Y_train.iloc[val_idx]\n",
    "\n",
    "        prob_lgb = lgbm_best_params.predict_proba(X_val)\n",
    "        prob_cb  = cat_model_best.predict_proba(X_val)\n",
    "\n",
    "        combined_proba = (w1 * prob_lgb + w2 * prob_cb) / (w1 + w2 + 1e-8)\n",
    "        y_pred = np.argmax(combined_proba, axis=1)\n",
    "\n",
    "        f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "study_name = \"VotingClassifier_Optim\"\n",
    "storage_name = f\"sqlite:///{study_name}.db\"\n",
    "\n",
    "study_VotingClassifier = optuna.create_study(study_name=study_name, storage=storage_name, direction='maximize', load_if_exists=True)\n",
    "study_VotingClassifier.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "\n",
    "best_w1 = study_VotingClassifier.best_params[\"lgb_weight\"]\n",
    "best_w2 = study_VotingClassifier.best_params[\"cb_weight\"]\n",
    "\n",
    "prob_lgb_test = lgbm_best_params.predict_proba(Voting_X_test)\n",
    "prob_cb_test  = cat_model_best.predict_proba(Voting_X_test)\n",
    "combined_proba_test = (best_w1 * prob_lgb_test + best_w2 * prob_cb_test) / (best_w1 + best_w2 + 1e-8)\n",
    "\n",
    "y_pred_voting = np.argmax(combined_proba_test, axis=1)\n",
    "\n",
    "print(f\"\\n\\nMejor F1-Macro: {study_VotingClassifier.best_value:.4f}\")\n",
    "print(f\"Pesos óptimos: {study_VotingClassifier.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62a21099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de entrenamiento: 383.96 segundos\n",
      "\n",
      "\n",
      "Reporte de clasificación (VotingClassifier)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.257     0.293     0.274     24445\n",
      "           1      0.767     0.728     0.747     82928\n",
      "           2      0.124     0.158     0.139      2544\n",
      "           3      0.383     1.000     0.554       110\n",
      "\n",
      "    accuracy                          0.618    110027\n",
      "   macro avg      0.383     0.545     0.428    110027\n",
      "weighted avg      0.638     0.618     0.628    110027\n",
      "\n",
      "\n",
      "Matriz de confusión para VotingClassifier:\n",
      "[[ 7158 16941   332    14]\n",
      " [19956 60344  2522   106]\n",
      " [  687  1397   403    57]\n",
      " [    0     0     0   110]]\n"
     ]
    }
   ],
   "source": [
    "# Crear VotingClassifier \n",
    "voting_model = VotingClassifier( \n",
    "                                estimators=[ ('lgb', lgbm_best_params), ('cb', cat_model_best) ], \n",
    "                                voting='soft', \n",
    "                                weights=[0.5685868882216454, 0.010730550423117324], \n",
    "                                n_jobs=-1 \n",
    "                            ) \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "voting_model.fit(Voting_X_train, Voting_Y_train) \n",
    "\n",
    "train_time = time.time() - start \n",
    "\n",
    "y_pred_voting = voting_model.predict(Voting_X_test) \n",
    "y_proba_voting = voting_model.predict_proba(Voting_X_test) \n",
    "\n",
    "print(f\"Tiempo de entrenamiento: {train_time:.2f} segundos\\n\") \n",
    "print(\"\\nReporte de clasificación (VotingClassifier)\") \n",
    "print(classification_report(Voting_Y_test, y_pred_voting, digits=3)) \n",
    "print(\"\\nMatriz de confusión para VotingClassifier:\") \n",
    "print(confusion_matrix(Voting_Y_test, y_pred_voting))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch311 (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
