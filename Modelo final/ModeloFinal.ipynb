{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c403c50",
   "metadata": {},
   "source": [
    "# EQUIPO 36 | Avance 5: Modelo Final\n",
    "## Proyecto: Predicción de infestaciones de gorgojo del agave\n",
    "## Integrantes equipo 36:\n",
    "\n",
    "| Nombre | Matrícula |\n",
    "| ------ | --------- |\n",
    "| André Martins Cordebello | A00572928 |\n",
    "| Enrique Eduardo Solís Da Costa | A00572678 |\n",
    "| Delbert Francisco Custodio Vargas | A01795613 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130be08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from imblearn.ensemble  import BalancedRandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from bayes_opt import BayesianOptimization\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import BallTree\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import optuna\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ec926e",
   "metadata": {},
   "source": [
    "## Cargamos el dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e928974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data_with_weather_information.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad8b6e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tramp_id                               object\n",
       "sampling_date                  datetime64[ns]\n",
       "lat                                   float64\n",
       "lon                                   float64\n",
       "municipality                           object\n",
       "plantation_age                          int64\n",
       "capture_count                         float64\n",
       "state                                  object\n",
       "square_area_imputed                   float64\n",
       "month                                   int64\n",
       "year                                    int64\n",
       "year-month                             object\n",
       "day_of_year_sin                       float64\n",
       "day_of_year_cos                       float64\n",
       "day_of_week_sin                       float64\n",
       "day_of_week_cos                       float64\n",
       "week_of_year_sin                      float64\n",
       "week_of_year_cos                      float64\n",
       "month_sin                             float64\n",
       "month_cos                             float64\n",
       "critical_season                         int64\n",
       "severity_encoded                        int64\n",
       "distance_to_nearest_hotspot           float64\n",
       "hotspots_within_5km                     int64\n",
       "precipitation                         float64\n",
       "avg_temp                              float64\n",
       "max_temp                              float64\n",
       "min_temp                              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d6ed3",
   "metadata": {},
   "source": [
    "# Modelos individuales: `LightGBM` y `CatBoost`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f22b7",
   "metadata": {},
   "source": [
    "### Carga de nuestro Train y Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d653b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiamos el dataframe con la información\n",
    "train_test_df = df.copy()\n",
    "train_test_df = train_test_df.sort_values(by='sampling_date').reset_index(drop=True)\n",
    "\n",
    "# Hacemos un encoding basico para State y Municipalidad\n",
    "for col in ['state', 'municipality']:\n",
    "    le = LabelEncoder()\n",
    "    train_test_df[col] = le.fit_transform(train_test_df[col])\n",
    "\n",
    "# Generamos la mascara para obtener los datos de antes del 2025 y del 2025 por separado\n",
    "train_mask = train_test_df['sampling_date'].dt.year < 2025\n",
    "test_mask  = train_test_df['sampling_date'].dt.year == 2025\n",
    "\n",
    "# Excluimos la variable objetivo (severity_encoded) y algunos variables o features que ya tenemos contenidos en nuestros\n",
    "# features creados. `capture_count` no podemos tomarlo en cuenta porque se relaciona directamente con la severidad.\n",
    "exclude_cols = [\n",
    "    'severity_encoded','tramp_id', 'capture_count', \n",
    "    'month', 'year-month', 'sampling_date', 'municipality', \n",
    "    'state'\n",
    "]\n",
    "\n",
    "# Cargamos los features a tomar en cuenta (obviamos los features en exclude_cols)\n",
    "features = [col for col in train_test_df.columns if col not in exclude_cols]\n",
    "\n",
    "# Generamos nuestro split de entrenamiento y test por medio de las mascaras train_mask y test_mask\n",
    "X_train, y_train = train_test_df.loc[train_mask, features], train_test_df.loc[train_mask, 'severity_encoded'] # El train dataset es la data historica de 2014 a 2024\n",
    "X_test,  y_test  = train_test_df.loc[test_mask,  features], train_test_df.loc[test_mask,  'severity_encoded'] # El test dataset es la data a partir de 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea68086",
   "metadata": {},
   "source": [
    "### Modelo `LightGBM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "388366bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Resultados para LightGBM después de usar Optuna:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.264     0.328     0.292     24445\n",
      "           1      0.771     0.710     0.739     82928\n",
      "           2      0.138     0.164     0.150      2544\n",
      "           3      0.383     1.000     0.554       110\n",
      "\n",
      "    accuracy                          0.613    110027\n",
      "   macro avg      0.389     0.550     0.434    110027\n",
      "weighted avg      0.643     0.613     0.626    110027\n",
      "\n",
      "\n",
      "\n",
      "Matriz de confusión para LightGBM:\n",
      "\n",
      "[[ 8012 16124   295    14]\n",
      " [21650 58863  2309   106]\n",
      " [  688  1382   417    57]\n",
      " [    0     0     0   110]]\n"
     ]
    }
   ],
   "source": [
    "LightGBM_X_train = X_train.copy()\n",
    "LightGBM_X_test  = X_test.copy()\n",
    "LightGBM_Y_train = y_train.copy()\n",
    "LightGBM_Y_test  = y_test.copy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "LightGBM_X_train[['distance_to_nearest_hotspot']] = scaler.fit_transform(LightGBM_X_train[['distance_to_nearest_hotspot']])\n",
    "LightGBM_X_test[['distance_to_nearest_hotspot']] = scaler.transform(LightGBM_X_test[['distance_to_nearest_hotspot']])\n",
    "\n",
    "lgbm_best_params = LGBMClassifier(\n",
    "    boosting_type = \"gbdt\",\n",
    "    objective = \"multiclass\",\n",
    "    num_class = 4,\n",
    "    class_weight = \"balanced\",\n",
    "    is_unbalance = False,\n",
    "    device_type = \"gpu\",\n",
    "    min_gain_to_split = 0.001,  \n",
    "    random_state = 42,\n",
    "    verbose = -1,\n",
    "    learning_rate= 0.031205207400998834,\n",
    "    num_leaves= 110,\n",
    "    max_depth= 11,\n",
    "    feature_fraction= 0.91959030797181,\n",
    "    bagging_fraction= 0.7694621015318531,\n",
    "    lambda_l1= 1.8616621273598788,\n",
    "    lambda_l2= 2.6453430076619573,\n",
    "    min_child_samples= 70,\n",
    "    n_estimators= 299\n",
    ")\n",
    "\n",
    "lgbm_best_params.fit(LightGBM_X_train, LightGBM_Y_train)\n",
    "\n",
    "y_pred_lgbm_best = lgbm_best_params.predict(LightGBM_X_test, categorical_features=['critical_season'])\n",
    "\n",
    "\n",
    "print(\"\\n\\nResultados para LightGBM después de usar Optuna:\\n\\n\")\n",
    "print(classification_report(LightGBM_Y_test, y_pred_lgbm_best, digits=3))\n",
    "\n",
    "print(\"\\n\\nMatriz de confusión para LightGBM:\\n\")\n",
    "print(confusion_matrix(y_test, y_pred_lgbm_best))\n",
    "y_pred_proba_lgbm_best_params = lgbm_best_params.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15bc4f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo para su futuro uso\n",
    "with open('ligthgbm.pkl', 'wb') as file:\n",
    "    pickle.dump(lgbm_best_params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e90234",
   "metadata": {},
   "source": [
    "### Modelo `CatBoost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36cce033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para CatBoost según Optuna:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.20      0.24     24445\n",
      "           1       0.76      0.70      0.73     82928\n",
      "           2       0.07      0.48      0.12      2544\n",
      "           3       0.33      1.00      0.49       110\n",
      "\n",
      "    accuracy                           0.58    110027\n",
      "   macro avg       0.37      0.59      0.40    110027\n",
      "weighted avg       0.64      0.58      0.61    110027\n",
      "\n",
      "\n",
      "Matriz de confusión para CatBoost:\n",
      "[[ 4965 17050  2411    19]\n",
      " [11127 57787 13869   145]\n",
      " [  161  1105  1214    64]\n",
      " [    0     0     0   110]]\n"
     ]
    }
   ],
   "source": [
    "CatBoost_X_train = X_train.copy()\n",
    "CatBoost_X_test  = X_test.copy()\n",
    "CatBoost_Y_train = y_train.copy()\n",
    "CatBoost_Y_test  = y_test.copy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "CatBoost_X_train[['distance_to_nearest_hotspot']] = scaler.fit_transform(CatBoost_X_train[['distance_to_nearest_hotspot']])\n",
    "CatBoost_X_test[['distance_to_nearest_hotspot']] = scaler.transform(CatBoost_X_test[['distance_to_nearest_hotspot']])\n",
    "\n",
    "\n",
    "class_counts = CatBoost_Y_train.value_counts().sort_index()\n",
    "num_classes = len(class_counts)\n",
    "total = len(CatBoost_Y_train)\n",
    "class_weights = {i: total / (num_classes * count) for i, count in class_counts.items()}\n",
    "\n",
    "weights = CatBoost_Y_train.map(class_weights)\n",
    "\n",
    "cat_model_best = CatBoostClassifier(                 \n",
    "    loss_function='MultiClass',    \n",
    "    eval_metric='TotalF1',         \n",
    "    auto_class_weights='Balanced', \n",
    "    random_seed=42,\n",
    "    task_type='GPU',               \n",
    "    #verbose=100,\n",
    "    iterations= 900,\n",
    "    learning_rate= 0.03169683936081736,\n",
    "    depth= 8,\n",
    "    l2_leaf_reg= 1.0126729009882989,\n",
    "    bagging_temperature= 0.13742770730370382,\n",
    "    border_count= 238,\n",
    "    random_strength= 1.490982729702571,\n",
    "    grow_policy= 'SymmetricTree',\n",
    "    logging_level= 'Silent'\n",
    ")\n",
    "\n",
    "categorical_cols = ['critical_season']\n",
    "\n",
    "cat_model_best.fit(\n",
    "    CatBoost_X_train,\n",
    "    CatBoost_Y_train,\n",
    "    cat_features=categorical_cols if 'categorical_cols' in locals() else None,\n",
    "    eval_set=(CatBoost_X_test, CatBoost_Y_test),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "y_pred_cd_best = cat_model_best.predict(CatBoost_X_test)\n",
    "y_pred_cd_best = y_pred_cd_best.flatten()\n",
    "\n",
    "print(\"Resultados para CatBoost según Optuna:\\n\")\n",
    "print(classification_report(CatBoost_Y_test, y_pred_cd_best))\n",
    "\n",
    "\n",
    "print(\"\\nMatriz de confusión para CatBoost:\")\n",
    "print(confusion_matrix(CatBoost_Y_test, y_pred_cd_best))\n",
    "y_pred_proba_cv_best_params = cat_model_best.predict_proba(CatBoost_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab78f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo para su futuro uso\n",
    "with open('catboost.pkl', 'wb') as file:\n",
    "    pickle.dump(cat_model_best, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ee10b",
   "metadata": {},
   "source": [
    "# Estrategias de ensamble homogeneas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b38b6",
   "metadata": {},
   "source": [
    "Estas estrategias son conocidas por combinar 2 o más modelos del mismo tipo para mejorar la estabilidad y resultados obtenidos en cuánto a las predicciones que un único modelo podría hacer. Estos ensambles se llaman homogéneos debido a que usan una combinación de un único modelo base, y por cada instancia del mismo modelo se utilizan distintos hiperparámetros (Breiman, 1996). Esto, en algunos casos, logra reducir la varianza que pueda tener un modelo y aumenta la generalización que el mismo pueda hacer. Muchas de estas estrategias buscan promediar los resultados obtenidos por cada modelo base del mismo tipo.\n",
    "\n",
    "Algunas técnicas comunes de ensamble homogéneas son las siguientes:\n",
    "\n",
    "- Baggin (Bootstrap Aggregating), donde se entrenan varios modelos con subconjuntos de datos generados por muestreos aleatorios.\n",
    "- Boosting, donde los modelos se entrenan de manera secuencial para que cada nuevo modelo corrija errores que los modelos pasados pudieron tener.\n",
    "- Stacking homogéneo, la cual mezcla distintas instancias del  mismo modelo con hiperparámetros distintos, para luego utilizar un `meta-modelo` sencillo que combine las salidas de éstas instancias.\n",
    "\n",
    "\n",
    "Por ejemplo, varios `DecisionTrees`, `RandomForest` o `CatBoost` con distintos parámetros podrían formar parte de estrategias de ensamble homogéneas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fbbfd1",
   "metadata": {},
   "source": [
    "## `LightGBM Bagging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3640b4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro para Bagging con LightGBM: 0.4326\n",
      "\n",
      "\n",
      "Resultados para LightGBM después de usar Bagging:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.254     0.277     0.265     24445\n",
      "           1      0.765     0.744     0.754     82928\n",
      "           2      0.162     0.158     0.160      2544\n",
      "           3      0.381     1.000     0.551       110\n",
      "\n",
      "    accuracy                          0.627    110027\n",
      "   macro avg      0.390     0.545     0.433    110027\n",
      "weighted avg      0.637     0.627     0.632    110027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Cargamos nuestros train y test sets\n",
    "Bagging_X_train = X_train.copy()\n",
    "Bagging_Y_train = y_train.copy()\n",
    "Bagging_X_test  = X_test.copy()\n",
    "Bagging_Y_test  = y_test.copy()\n",
    "\n",
    "#  Cantidad de modelos a generar para el Bagging\n",
    "n_models = 15\n",
    "models = []\n",
    "oof_preds = []\n",
    "test_preds = []\n",
    "\n",
    "\n",
    "for i in range(n_models):\n",
    "    \n",
    "    # Generamos nuevos sets de entrenamiento y test por cada ciclo\n",
    "    X_train_sub, y_train_sub = resample(Bagging_X_train, Bagging_Y_train, replace=True, random_state=42 + i)\n",
    "    \n",
    "    # Instanciamos LightGBM Classifier\n",
    "    model = lgb.LGBMClassifier( \n",
    "                                boosting_type = \"gbdt\",\n",
    "                                objective = \"multiclass\",\n",
    "                                num_class = 4,\n",
    "                                class_weight = \"balanced\",\n",
    "                                is_unbalance = False,\n",
    "                                device_type = \"gpu\",\n",
    "                                min_gain_to_split = 0.001,  \n",
    "                                random_state = 42,\n",
    "                                verbose = -1,\n",
    "                                learning_rate= 0.031205207400998834,\n",
    "                                num_leaves= 110,\n",
    "                                max_depth= 11,\n",
    "                                feature_fraction= 0.91959030797181,\n",
    "                                bagging_fraction= 0.7694621015318531,\n",
    "                                lambda_l1= 1.8616621273598788,\n",
    "                                lambda_l2= 2.6453430076619573,\n",
    "                                min_child_samples= 70,\n",
    "                                n_estimators= 299\n",
    "                            )\n",
    "    \n",
    "    # Entrenamos sobre los sets de entrenamiento y test por ciclo\n",
    "    model.fit(X_train_sub, y_train_sub)\n",
    "    models.append(model)\n",
    "\n",
    "    # Guardamos las predicciones out-of-fold\n",
    "    y_pred = model.predict_proba(Bagging_X_test)\n",
    "    test_preds.append(y_pred)\n",
    "\n",
    "# Promediamos los resultados\n",
    "avg_proba = np.mean(test_preds, axis=0)\n",
    "y_pred_final = np.argmax(avg_proba, axis=1)\n",
    "\n",
    "# Calculamos el F1 Score\n",
    "f1_macro = f1_score(Bagging_Y_test, y_pred_final, average=\"macro\")\n",
    "print(f\"F1-macro para Bagging con LightGBM: {f1_macro:.4f}\")\n",
    "\n",
    "# Imprimimos resultados\n",
    "print(\"\\n\\nResultados para LightGBM después de usar Bagging:\\n\\n\")\n",
    "print(classification_report(Bagging_Y_test, y_pred_final, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef39f8",
   "metadata": {},
   "source": [
    "Para la estrategia de Bagging, notamos que `LightGBM` no tuvo una mejora, por lo que descartamos que esta sea una estrategia viable por medio de este modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd2a96",
   "metadata": {},
   "source": [
    "## `LightGBM Stacking`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f003205e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados luego de hacer LightGBM Stacking\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2680    0.2619    0.2649     24445\n",
      "           1     0.7679    0.7950    0.7812     82928\n",
      "           2     0.1071    0.0012    0.0023      2544\n",
      "           3     0.3984    0.9091    0.5540       110\n",
      "\n",
      "    accuracy                         0.6583    110027\n",
      "   macro avg     0.3854    0.4918    0.4006    110027\n",
      "weighted avg     0.6412    0.6583    0.6483    110027\n",
      "\n",
      "\n",
      "Matriz de confusion para LightGBM stacking:\n",
      "\n",
      "[[ 6403 18028     6     8]\n",
      " [16895 65930    12    91]\n",
      " [  591  1898     3    52]\n",
      " [    0     3     7   100]]\n"
     ]
    }
   ],
   "source": [
    "X_train_stack = X_train.copy()\n",
    "y_train_stack = y_train.copy()\n",
    "X_test_stack  = X_test.copy()\n",
    "y_test_stack  = y_test.copy()\n",
    "\n",
    "lgbm_1 = lgb.LGBMClassifier(\n",
    "    boosting_type='gbdt', \n",
    "    objective='multiclass', \n",
    "    num_class=4,            \n",
    "    class_weight='balanced',\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    max_depth=-1,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    device='gpu',\n",
    "    is_unbalance=False\n",
    ")\n",
    "\n",
    "lgbm_2 = lgb.LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=4,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=10,\n",
    "    feature_fraction=0.9,\n",
    "    bagging_fraction=0.8,\n",
    "    random_state=42,\n",
    "    device_type='gpu',\n",
    "    is_unbalance=False,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "lgbm_3 = lgb.LGBMClassifier(\n",
    "    boosting_type = \"gbdt\",\n",
    "    objective = \"multiclass\",\n",
    "    num_class = 4,\n",
    "    class_weight = \"balanced\",\n",
    "    is_unbalance = False,\n",
    "    device_type = \"gpu\",\n",
    "    min_gain_to_split = 0.001,  \n",
    "    random_state = 42,\n",
    "    verbose = -1,\n",
    "    learning_rate= 0.031205207400998834,\n",
    "    num_leaves= 110,\n",
    "    max_depth= 11,\n",
    "    feature_fraction= 0.91959030797181,\n",
    "    bagging_fraction= 0.7694621015318531,\n",
    "    lambda_l1= 1.8616621273598788,\n",
    "    lambda_l2= 2.6453430076619573,\n",
    "    min_child_samples= 70,\n",
    "    n_estimators= 299\n",
    ")\n",
    "\n",
    "meta_model = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "\n",
    "\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lgbm1', lgbm_1),\n",
    "        ('lgbm2', lgbm_2),\n",
    "        ('lgbm3', lgbm_3)\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stack_model.fit(X_train_stack, y_train_stack)\n",
    "\n",
    "y_pred = stack_model.predict(X_test_stack)\n",
    "\n",
    "print(\"Resultados luego de hacer LightGBM Stacking\")\n",
    "print(classification_report(y_test_stack, y_pred, digits=4))\n",
    "\n",
    "print(\"\\nMatriz de confusion para LightGBM stacking:\\n\")\n",
    "print(confusion_matrix(y_test_stack, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3834f",
   "metadata": {},
   "source": [
    "Por medio del uso de Stacking, vemos un ligero mejoramiento en la `precision` de la clase 3, a costa de un peor desempeño en el recall de dicha clase. En nuestro caso, tener un recall de más del 90% es aceptable, pero notamos que perdimos un aproximado de 9% en promedio de esta métrica por una mejora del 1% aproximadamente en nuestra `precision`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0810ca",
   "metadata": {},
   "source": [
    "## `CatBoost Bagging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9c96649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro para Bagging con CatBoost: 0.3919\n",
      "\n",
      "\n",
      "Resultados para CatBoost después de usar Bagging:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.286     0.156     0.202     24445\n",
      "           1      0.752     0.737     0.744     82928\n",
      "           2      0.076     0.451     0.131      2544\n",
      "           3      0.325     1.000     0.491       110\n",
      "\n",
      "    accuracy                          0.601    110027\n",
      "   macro avg      0.360     0.586     0.392    110027\n",
      "weighted avg      0.632     0.601     0.609    110027\n",
      "\n",
      "\n",
      "\n",
      "Matriz de confusión para CatBoost Bagging:\n",
      "\n",
      "[[ 3821 18976  1629    19]\n",
      " [ 9445 61088 12250   145]\n",
      " [  113  1220  1147    64]\n",
      " [    0     0     0   110]]\n"
     ]
    }
   ],
   "source": [
    "# Generamos nuestros Train y Test datasets\n",
    "Bagging_X_train = X_train.copy()\n",
    "Bagging_Y_train = y_train.copy()\n",
    "Bagging_X_test  = X_test.copy()\n",
    "Bagging_Y_test  = y_test.copy()\n",
    "\n",
    "#  Cantidad de modelos a generar para el Bagging\n",
    "n_models = 5\n",
    "models = []\n",
    "oof_preds = []\n",
    "test_preds = []\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "Bagging_X_train[['distance_to_nearest_hotspot']] = scaler.fit_transform(Bagging_X_train[['distance_to_nearest_hotspot']])\n",
    "Bagging_X_test[['distance_to_nearest_hotspot']] = scaler.transform(Bagging_X_test[['distance_to_nearest_hotspot']])\n",
    "\n",
    "\n",
    "class_counts = Bagging_Y_train.value_counts().sort_index()\n",
    "num_classes = len(class_counts)\n",
    "total = len(Bagging_Y_train)\n",
    "class_weights = {i: total / (num_classes * count) for i, count in class_counts.items()}\n",
    "\n",
    "weights = Bagging_Y_train.map(class_weights)\n",
    "\n",
    "categorical_cols = ['critical_season']\n",
    "\n",
    "for i in range(n_models):\n",
    "    \n",
    "    X_train_sub, y_train_sub = resample(Bagging_X_train, Bagging_Y_train, replace=True, random_state=42 + i)\n",
    "\n",
    "    model = CatBoostClassifier(                 \n",
    "        loss_function='MultiClass',    \n",
    "        eval_metric='TotalF1',         \n",
    "        auto_class_weights='Balanced', \n",
    "        random_seed=42,\n",
    "        task_type='GPU',               \n",
    "        #verbose=100,\n",
    "        iterations= 900,\n",
    "        learning_rate= 0.03169683936081736,\n",
    "        depth= 8,\n",
    "        l2_leaf_reg= 1.0126729009882989,\n",
    "        bagging_temperature= 0.13742770730370382,\n",
    "        border_count= 238,\n",
    "        random_strength= 1.490982729702571,\n",
    "        grow_policy= 'SymmetricTree',\n",
    "        logging_level= 'Silent'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    model.fit(\n",
    "        X_train_sub,\n",
    "        y_train_sub,\n",
    "        cat_features=categorical_cols if 'categorical_cols' in locals() else None,\n",
    "        eval_set=(Bagging_X_test, Bagging_Y_test),\n",
    "        use_best_model=True,\n",
    "        verbose=False)\n",
    "    \n",
    "    models.append(model)\n",
    "\n",
    "    # Guardamos las predicciones out-of-fold\n",
    "    y_pred = model.predict_proba(Bagging_X_test)\n",
    "    test_preds.append(y_pred)\n",
    "    \n",
    "    \n",
    "avg_proba = np.mean(test_preds, axis=0)\n",
    "y_pred_final = np.argmax(avg_proba, axis=1)\n",
    "\n",
    "f1_macro = f1_score(Bagging_Y_test, y_pred_final, average=\"macro\")\n",
    "print(f\"F1-macro para Bagging con CatBoost: {f1_macro:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nResultados para CatBoost después de usar Bagging:\\n\\n\")\n",
    "print(classification_report(Bagging_Y_test, y_pred_final, digits=3))\n",
    "\n",
    "print(\"\\n\\nMatriz de confusión para CatBoost Bagging:\\n\")\n",
    "print(confusion_matrix(Bagging_Y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091641a",
   "metadata": {},
   "source": [
    "## `Stacking CatBoost` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59356e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CatBoost #1 ...\n",
      "Modelo #1 Listo — F1-macro: 0.4071 | Tiempo: 26.04s\n",
      "\n",
      "CatBoost #2 ...\n",
      "Modelo #2 Listo — F1-macro: 0.3695 | Tiempo: 16.23s\n",
      "\n",
      "CatBoost #3 ...\n",
      "Modelo #3 Listo — F1-macro: 0.3757 | Tiempo: 13.72s\n",
      "              Model  F1-macro  Training Time (s)\n",
      "0  CatBoost_Model_1  0.407078          26.040743\n",
      "1  CatBoost_Model_2  0.369531          16.234849\n",
      "2  CatBoost_Model_3  0.375697          13.723225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CatBoost Stacking resultados:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2302    0.1933    0.2102     24445\n",
      "           1     0.7579    0.8163    0.7860     82928\n",
      "           2     0.1515    0.0039    0.0077      2544\n",
      "           3     0.3717    0.3818    0.3767       110\n",
      "\n",
      "    accuracy                         0.6587    110027\n",
      "   macro avg     0.3778    0.3489    0.3451    110027\n",
      "weighted avg     0.6263    0.6587    0.6397    110027\n",
      "\n",
      "\n",
      "Matriz de confusion:\n",
      "[[ 4726 19711     3     5]\n",
      " [15156 67697    27    48]\n",
      " [  647  1869    10    18]\n",
      " [    0    42    26    42]]\n"
     ]
    }
   ],
   "source": [
    "X_train_stack = X_train.copy()\n",
    "y_train_stack = y_train.copy()\n",
    "X_test_stack  = X_test.copy()\n",
    "y_test_stack  = y_test.copy()\n",
    "\n",
    "\n",
    "cb_1 = CatBoostClassifier(                 \n",
    "    loss_function='MultiClass',    \n",
    "    eval_metric='TotalF1',         \n",
    "    auto_class_weights='Balanced', \n",
    "    random_seed=42,\n",
    "    task_type='GPU',               \n",
    "    iterations=900,\n",
    "    learning_rate=0.03169683936081736,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=1.0126729009882989,\n",
    "    bagging_temperature=0.13742770730370382,\n",
    "    border_count=238,\n",
    "    random_strength=1.490982729702571,\n",
    "    grow_policy='SymmetricTree',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "cb_2 = CatBoostClassifier(\n",
    "    iterations=300,\n",
    "    learning_rate=0.05,\n",
    "    depth=10,\n",
    "    l2_leaf_reg=3,\n",
    "    loss_function=\"MultiClass\",\n",
    "    eval_metric=\"TotalF1\",\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    task_type=\"GPU\",\n",
    "    random_seed=100,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "cb_3 = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.02,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=2,\n",
    "    loss_function=\"MultiClass\",\n",
    "    eval_metric=\"TotalF1\",\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    task_type=\"GPU\",\n",
    "    random_seed=2025,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "meta_model = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "\n",
    "models = [cb_1, cb_2, cb_3]\n",
    "\n",
    "train_probas = []\n",
    "test_probas  = []\n",
    "results = []\n",
    "\n",
    "for i, model in enumerate(models, 1):\n",
    "    print(f\"\\nCatBoost #{i} ...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    model.fit(X_train_stack, y_train_stack)\n",
    "    \n",
    "    y_train_pred = model.predict_proba(X_train_stack)\n",
    "    y_test_pred  = model.predict_proba(X_test_stack)\n",
    "    \n",
    "    train_probas.append(y_train_pred)\n",
    "    test_probas.append(y_test_pred)\n",
    "    \n",
    "    y_pred_class = np.argmax(y_test_pred, axis=1)\n",
    "    f1_macro = f1_score(y_test_stack, y_pred_class, average=\"macro\")\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    results.append([f\"CatBoost_Model_{i}\", f1_macro, elapsed])\n",
    "    print(f\"Modelo #{i} Listo — F1-macro: {f1_macro:.4f} | Tiempo: {elapsed:.2f}s\")\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"F1-macro\", \"Training Time (s)\"])\n",
    "print(results_df)\n",
    "\n",
    "X_meta_train = np.hstack(train_probas)\n",
    "X_meta_test  = np.hstack(test_probas)\n",
    "\n",
    "meta_model = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "meta_model.fit(X_meta_train, y_train_stack)\n",
    "\n",
    "y_pred_stack = meta_model.predict(X_meta_test)\n",
    "\n",
    "print(\"\\nCatBoost Stacking resultados:\")\n",
    "print(classification_report(y_test_stack, y_pred_stack, digits=4))\n",
    "print(\"\\nMatriz de confusion:\")\n",
    "print(confusion_matrix(y_test_stack, y_pred_stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af65c4bb",
   "metadata": {},
   "source": [
    "# Estrategias de ensamble heterogéneas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade24f2a",
   "metadata": {},
   "source": [
    "A diferencia de las estrategias de ensamble homogéneas, para las heterogéneas buscamos la combinación de distintos tipos de modelos o algoritmos de aprendizaje automático. La idea de este enfoque es la de aprovechar las fortalezas, debilidades y complejidades de cada tipo de modelo para tener una diversidad o combinación estructural. Esta diversidad tiende a mejorar el rendimiento de las predicciones y también mejora la robustez que pueda tener una solución de ML (Kuncheva, 2004).\n",
    "\n",
    "Las estrategias más comunes llevan por nombre `Voting Classifier` y `Stacking heterogéneo`. El enfoque del `VotingClassifier` como método de ensamble es realizar predicciones independientes de modelos distintos, para luego combinar dichos resultados y predecir una nueva salida por medio de la combinación de las predicciones independientes; en el caso de clasificación se utiliza una votación mayoritaria, mientras que para el caso de regresiones se utiliza una media ponderada. Ahora bien, para el `Stacking heterogéneo` buscamos usar las predicciones independientes de cada modelo como una entrada para un meta-modelo que se entrena para combinar los resultados; es decir, el meta-modelo predice una nueva salida con base en las predicciones de los modelos independientes, lo que permite que algunas implementaciones encuentren patrones no lineales y mejoren los resultados (Rokach, 2010).\n",
    "\n",
    "Por último, estos ensambles son útiles cuando se tienen modelos \"especializados\", es decir, contamos con modelos base los cuales son buenos en alguna métrica en específico (por ejemplo, un modelo es bueno en `precision` y el otro modelo base es bueno respecto a su `recall`) (Wolpert, 1992). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb8e29",
   "metadata": {},
   "source": [
    "## `VotingClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28328fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-24 20:34:36,526] Using an existing study with name 'VotingClassifier_Optim' instead of creating a new one.\n",
      "[I 2025-10-24 20:34:45,988] Trial 50 finished with value: 0.5764727742530787 and parameters: {'lgb_weight': 1.767713190659428, 'cb_weight': 1.1063827087671954}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:34:55,223] Trial 51 finished with value: 0.5763857884038907 and parameters: {'lgb_weight': 0.19669719140884762, 'cb_weight': 0.09051457329075366}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:35:04,383] Trial 52 finished with value: 0.5765317146539907 and parameters: {'lgb_weight': 0.5916488251805205, 'cb_weight': 0.049424284335735785}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:35:13,590] Trial 53 finished with value: 0.5765105914787857 and parameters: {'lgb_weight': 0.3486027614690162, 'cb_weight': 0.0014462344819332323}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:35:22,811] Trial 54 finished with value: 0.5764207757751006 and parameters: {'lgb_weight': 0.7469761381300128, 'cb_weight': 0.1910899514314799}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:35:32,094] Trial 55 finished with value: 0.5763699763923067 and parameters: {'lgb_weight': 0.5023643119241714, 'cb_weight': 0.47202929003452554}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:35:41,250] Trial 56 finished with value: 0.5764212063122106 and parameters: {'lgb_weight': 0.6886325031579605, 'cb_weight': 0.30829275838482867}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:35:50,437] Trial 57 finished with value: 0.5764206140162765 and parameters: {'lgb_weight': 0.4642588343363065, 'cb_weight': 0.19385959031108602}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:35:59,997] Trial 58 finished with value: 0.5765397173472332 and parameters: {'lgb_weight': 0.6190275979592725, 'cb_weight': 0.07613400879925537}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:36:10,025] Trial 59 finished with value: 0.5764874891785565 and parameters: {'lgb_weight': 0.8421484708655688, 'cb_weight': 0.566797925175296}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:36:19,996] Trial 60 finished with value: 0.5279866039857404 and parameters: {'lgb_weight': 0.007216556397956209, 'cb_weight': 0.4082783283209086}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:36:29,637] Trial 61 finished with value: 0.5764101227795342 and parameters: {'lgb_weight': 0.38448915787442073, 'cb_weight': 0.13682028483584083}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:36:39,336] Trial 62 finished with value: 0.5765039780296983 and parameters: {'lgb_weight': 1.7015758441239115, 'cb_weight': 0.0814454791131757}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:36:48,779] Trial 63 finished with value: 0.5764959316362671 and parameters: {'lgb_weight': 1.257279251958605, 'cb_weight': 0.054644047001668125}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:36:57,990] Trial 64 finished with value: 0.5765669998437742 and parameters: {'lgb_weight': 1.6717335583143749, 'cb_weight': 0.0015848432077161401}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:37:07,114] Trial 65 finished with value: 0.5765352522314823 and parameters: {'lgb_weight': 1.9874772142978472, 'cb_weight': 0.2243503820356785}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:37:16,358] Trial 66 finished with value: 0.5765487132251368 and parameters: {'lgb_weight': 1.552098661534938, 'cb_weight': 0.1639278204116204}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:37:25,582] Trial 67 finished with value: 0.576410199206899 and parameters: {'lgb_weight': 1.4258758792084403, 'cb_weight': 0.2813163579785345}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:37:34,724] Trial 68 finished with value: 0.5765043128782261 and parameters: {'lgb_weight': 1.8281045201637898, 'cb_weight': 0.10632252196742495}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:37:43,945] Trial 69 finished with value: 0.5763116983470944 and parameters: {'lgb_weight': 0.5778852205872318, 'cb_weight': 0.9619326310980545}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:37:53,180] Trial 70 finished with value: 0.5764329583782716 and parameters: {'lgb_weight': 0.467991417176073, 'cb_weight': 0.5105689643717504}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:38:02,370] Trial 71 finished with value: 0.5764147512056668 and parameters: {'lgb_weight': 1.168913355972728, 'cb_weight': 0.3089104343181723}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:38:11,581] Trial 72 finished with value: 0.576417383674068 and parameters: {'lgb_weight': 0.41257243577341784, 'cb_weight': 0.11570895161772421}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:38:20,802] Trial 73 finished with value: 0.5764101808490564 and parameters: {'lgb_weight': 0.3016952681437834, 'cb_weight': 0.06010959838893586}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:38:30,027] Trial 74 finished with value: 0.5764594304140582 and parameters: {'lgb_weight': 0.13159929897458916, 'cb_weight': 0.17092051565127347}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:38:39,251] Trial 75 finished with value: 0.5764966456742378 and parameters: {'lgb_weight': 0.21772880966924935, 'cb_weight': 0.14159939832605425}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:38:48,453] Trial 76 finished with value: 0.5765669998437742 and parameters: {'lgb_weight': 0.7579595742224892, 'cb_weight': 0.0016364284267940121}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:38:57,691] Trial 77 finished with value: 0.5765079821564448 and parameters: {'lgb_weight': 0.3418743343365277, 'cb_weight': 0.2251533197439986}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:39:06,965] Trial 78 finished with value: 0.5764367250672153 and parameters: {'lgb_weight': 0.6648142134999414, 'cb_weight': 0.39823400322325886}. Best is trial 41 with value: 0.5799116275863289.\n",
      "[I 2025-10-24 20:39:16,142] Trial 79 finished with value: 0.5764340588896251 and parameters: {'lgb_weight': 0.9017717427556144, 'cb_weight': 0.647808221329788}. Best is trial 41 with value: 0.5799116275863289.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mejor F1-Macro: 0.5799\n",
      "Pesos óptimos: {'lgb_weight': 1.9516866156032004, 'cb_weight': 0.6156930840292797}\n"
     ]
    }
   ],
   "source": [
    "Voting_X_train = X_train.copy()\n",
    "Voting_Y_train = y_train.copy()\n",
    "Voting_X_test  = X_test.copy()\n",
    "Voting_Y_test  = y_test.copy()\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "def objective(trial):\n",
    "    w1 = trial.suggest_float(\"lgb_weight\", 0.0, 2.0)\n",
    "    w2 = trial.suggest_float(\"cb_weight\", 0.0, 2.0)\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_idx, val_idx in tscv.split(Voting_X_train):\n",
    "        X_tr, X_val = Voting_X_train.iloc[train_idx], Voting_X_train.iloc[val_idx]\n",
    "        y_tr, y_val = Voting_Y_train.iloc[train_idx], Voting_Y_train.iloc[val_idx]\n",
    "\n",
    "        prob_lgb = lgbm_best_params.predict_proba(X_val)\n",
    "        prob_cb  = cat_model_best.predict_proba(X_val)\n",
    "\n",
    "        combined_proba = (w1 * prob_lgb + w2 * prob_cb) / (w1 + w2 + 1e-8)\n",
    "        y_pred = np.argmax(combined_proba, axis=1)\n",
    "\n",
    "        f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "study_name = \"VotingClassifier_Optim\"\n",
    "storage_name = f\"sqlite:///{study_name}.db\"\n",
    "\n",
    "study_VotingClassifier = optuna.create_study(study_name=study_name, storage=storage_name, direction='maximize', load_if_exists=True)\n",
    "study_VotingClassifier.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "\n",
    "best_w1 = study_VotingClassifier.best_params[\"lgb_weight\"]\n",
    "best_w2 = study_VotingClassifier.best_params[\"cb_weight\"]\n",
    "\n",
    "prob_lgb_test = lgbm_best_params.predict_proba(Voting_X_test)\n",
    "prob_cb_test  = cat_model_best.predict_proba(Voting_X_test)\n",
    "combined_proba_test = (best_w1 * prob_lgb_test + best_w2 * prob_cb_test) / (best_w1 + best_w2 + 1e-8)\n",
    "\n",
    "y_pred_voting = np.argmax(combined_proba_test, axis=1)\n",
    "\n",
    "print(f\"\\n\\nMejor F1-Macro: {study_VotingClassifier.best_value:.4f}\")\n",
    "print(f\"Pesos óptimos: {study_VotingClassifier.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62a21099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de entrenamiento: 72.04 segundos\n",
      "\n",
      "\n",
      "Reporte de clasificación (VotingClassifier)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.264     0.326     0.292     24445\n",
      "           1      0.771     0.711     0.740     82928\n",
      "           2      0.138     0.164     0.150      2544\n",
      "           3      0.383     1.000     0.554       110\n",
      "\n",
      "    accuracy                          0.613    110027\n",
      "   macro avg      0.389     0.550     0.434    110027\n",
      "weighted avg      0.643     0.613     0.626    110027\n",
      "\n",
      "\n",
      "Matriz de confusión para VotingClassifier:\n",
      "[[ 7963 16178   290    14]\n",
      " [21511 58996  2315   106]\n",
      " [  686  1383   418    57]\n",
      " [    0     0     0   110]]\n"
     ]
    }
   ],
   "source": [
    "# Crear VotingClassifier \n",
    "voting_model = VotingClassifier( \n",
    "                                estimators=[ ('lgb', lgbm_best_params), ('cb', cat_model_best) ], \n",
    "                                voting='soft', \n",
    "                                weights=[0.5685868882216454, 0.010730550423117324], \n",
    "                                n_jobs=-1 \n",
    "                            ) \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "voting_model.fit(Voting_X_train, Voting_Y_train) \n",
    "\n",
    "train_time = time.time() - start \n",
    "\n",
    "y_pred_voting = voting_model.predict(Voting_X_test) \n",
    "y_proba_voting = voting_model.predict_proba(Voting_X_test) \n",
    "\n",
    "print(f\"Tiempo de entrenamiento: {train_time:.2f} segundos\\n\") \n",
    "print(\"\\nReporte de clasificación (VotingClassifier)\") \n",
    "print(classification_report(Voting_Y_test, y_pred_voting, digits=3)) \n",
    "print(\"\\nMatriz de confusión para VotingClassifier:\") \n",
    "print(confusion_matrix(Voting_Y_test, y_pred_voting))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f14e422",
   "metadata": {},
   "source": [
    "## `Stacking heterogéneo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9e36e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape del meta-train set: (717829, 8)\n",
      "Shape del meta-test set: (110027, 8)\n"
     ]
    }
   ],
   "source": [
    "# Creamos copias de seguridad de nuestro train y test set\n",
    "Stacking_X_train = X_train.copy()\n",
    "Stacking_Y_train = y_train.copy()\n",
    "Stacking_X_test  = X_test.copy()\n",
    "Stacking_Y_test  = y_test.copy()\n",
    "\n",
    "# Generamos las probabilidades de las predicciones por cada clase y por cada modelo\n",
    "y_pred_lgb_train = lgbm_best_params.predict_proba(Stacking_X_train)\n",
    "y_pred_lgb_test  = lgbm_best_params.predict_proba(Stacking_X_test)\n",
    "y_pred_cb_train = cat_model_best.predict_proba(Stacking_X_train)\n",
    "y_pred_cb_test  = cat_model_best.predict_proba(Stacking_X_test)\n",
    "\n",
    "# Unimos (stack) las probabilidades\n",
    "X_meta_train = np.hstack([y_pred_lgb_train, y_pred_cb_train])\n",
    "X_meta_test  = np.hstack([y_pred_lgb_test,  y_pred_cb_test])\n",
    "\n",
    "# Copiamos las clases correctas\n",
    "y_meta_train = Stacking_Y_train.copy()\n",
    "y_meta_test  = Stacking_Y_test.copy()\n",
    "\n",
    "# Vemos la forma de cada dataset\n",
    "print(\"Shape del meta-train set:\", X_meta_train.shape)\n",
    "print(\"Shape del meta-test set:\", X_meta_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0860b22",
   "metadata": {},
   "source": [
    "#### Usando `LightGBM` como meta-modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12da650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Resultados para Stacking usando LightGBM como meta-modelo:\n",
      "F1-macro: 0.4098087515931915\n",
      "\n",
      "Métricas para LightGBM como meta-modelo:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.33      0.28     24445\n",
      "           1       0.76      0.65      0.70     82928\n",
      "           2       0.08      0.16      0.11      2544\n",
      "           3       0.40      0.90      0.55       110\n",
      "\n",
      "    accuracy                           0.57    110027\n",
      "   macro avg       0.37      0.51      0.41    110027\n",
      "weighted avg       0.63      0.57      0.59    110027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definimos un LightGBM pequeño como meta-learner\n",
    "meta_model = lgb.LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='multiclass',\n",
    "    num_class=len(np.unique(y_meta_train)),\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=250,\n",
    "    max_depth=10,\n",
    "    num_leaves=20,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    device_type='gpu',\n",
    "    min_split_gain=1\n",
    ")\n",
    "\n",
    "# Entrenamos el meta-modelo\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Predicciones finales\n",
    "y_meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Evaluamos el rendimiento\n",
    "print(\"\\n\\nResultados para Stacking usando LightGBM como meta-modelo:\")\n",
    "print(\"F1-macro:\", f1_score(y_meta_test, y_meta_pred, average='macro'))\n",
    "print(\"\\nMétricas para LightGBM como meta-modelo:\")\n",
    "print(classification_report(y_meta_test, y_meta_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baae174a",
   "metadata": {},
   "source": [
    "#### Usando `CatBoost` como meta-modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08ffd827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro (CatBoost meta-model): 0.4182804733601922\n",
      "\n",
      "Reporte de clasificación (CatBoost meta-model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.32      0.28     24445\n",
      "           1       0.76      0.67      0.71     82928\n",
      "           2       0.09      0.16      0.11      2544\n",
      "           3       0.41      0.96      0.57       110\n",
      "\n",
      "    accuracy                           0.58    110027\n",
      "   macro avg       0.37      0.53      0.42    110027\n",
      "weighted avg       0.63      0.58      0.60    110027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_model_cb = CatBoostClassifier(\n",
    "    loss_function='MultiClass',\n",
    "    eval_metric='TotalF1',\n",
    "    auto_class_weights='Balanced',\n",
    "    task_type='GPU',\n",
    "    iterations=250,        \n",
    "    learning_rate=0.02,  \n",
    "    depth=15,         \n",
    "    l2_leaf_reg=3,     \n",
    "    random_strength=1.0,\n",
    "    verbose=False,  \n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Entrenamos el meta-modelo\n",
    "meta_model_cb.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Predicciones finales\n",
    "y_meta_cb_pred = meta_model_cb.predict(X_meta_test)\n",
    "\n",
    "# Evaluación\n",
    "print(\"F1-macro (CatBoost meta-model):\", f1_score(y_meta_test, y_meta_cb_pred, average='macro'))\n",
    "print(\"\\nReporte de clasificación (CatBoost meta-model):\")\n",
    "print(classification_report(y_meta_test, y_meta_cb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c43fe8",
   "metadata": {},
   "source": [
    "#### Usando `RandomForestClassifier` como meta-modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88117574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro (RandomForest meta-model): 0.38575328862258507\n",
      "\n",
      "Reporte de clasificación (RandomForest meta-model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.09      0.13     24445\n",
      "           1       0.75      0.80      0.77     82928\n",
      "           2       0.07      0.31      0.12      2544\n",
      "           3       0.39      0.81      0.52       110\n",
      "\n",
      "    accuracy                           0.63    110027\n",
      "   macro avg       0.35      0.50      0.39    110027\n",
      "weighted avg       0.61      0.63      0.61    110027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_model_rf = RandomForestClassifier(\n",
    "    n_estimators=150,          # Número reducido de árboles\n",
    "    max_depth=6,               # Árboles poco profundos\n",
    "    class_weight='balanced',   # Para manejar desbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Entrenamiento del meta-modelo\n",
    "meta_model_rf.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Predicciones finales\n",
    "y_meta_rf_pred = meta_model_rf.predict(X_meta_test)\n",
    "\n",
    "# Evaluación\n",
    "print(\"F1-macro (RandomForest meta-model):\", f1_score(y_meta_test, y_meta_rf_pred, average='macro'))\n",
    "print(\"\\nReporte de clasificación (RandomForest meta-model):\")\n",
    "print(classification_report(y_meta_test, y_meta_rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25749d1b",
   "metadata": {},
   "source": [
    "#### Usando `LogistciRegression` como meta-modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e44785f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro (LogisticRegression meta-model): 0.411033734713384\n",
      "\n",
      "Reporte de clasificación (LogisticRegression meta-model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.24      0.24     24445\n",
      "           1       0.76      0.74      0.75     82928\n",
      "           2       0.06      0.11      0.08      2544\n",
      "           3       0.41      0.98      0.58       110\n",
      "\n",
      "    accuracy                           0.61    110027\n",
      "   macro avg       0.37      0.52      0.41    110027\n",
      "weighted avg       0.63      0.61      0.62    110027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_model_lr = LogisticRegression(\n",
    "    max_iter=2000,             # Iteraciones suficientes para convergencia\n",
    "    class_weight='balanced',\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenamiento del meta-modelo\n",
    "meta_model_lr.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Predicciones finales\n",
    "y_meta_lr_pred = meta_model_lr.predict(X_meta_test)\n",
    "\n",
    "# Evaluación\n",
    "print(\"F1-macro (LogisticRegression meta-model):\", f1_score(y_meta_test, y_meta_lr_pred, average='macro'))\n",
    "print(\"\\nReporte de clasificación (LogisticRegression meta-model):\")\n",
    "print(classification_report(y_meta_test, y_meta_lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccea91fc",
   "metadata": {},
   "source": [
    "#### Usando `XGBoost` como meta-modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0d66a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro (XGBoost meta-model): 0.36256954749024595\n",
      "\n",
      "Reporte de clasificación (XGBoost meta-model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.13      0.16     24445\n",
      "           1       0.75      0.84      0.80     82928\n",
      "           2       0.12      0.00      0.01      2544\n",
      "           3       0.37      0.69      0.49       110\n",
      "\n",
      "    accuracy                           0.67    110027\n",
      "   macro avg       0.36      0.42      0.36    110027\n",
      "weighted avg       0.61      0.67      0.64    110027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Meta-modelo basado en XGBoost\n",
    "meta_model_xgb = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=len(np.unique(y_meta_train)),\n",
    "    tree_method='hist',\n",
    "    device='cuda',              # Usa GPU\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=250,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=1,\n",
    "    random_state=42,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "# Entrenamiento del meta-modelo\n",
    "meta_model_xgb.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Predicciones finales\n",
    "y_meta_xgb_pred = meta_model_xgb.predict(X_meta_test)\n",
    "\n",
    "# Evaluación\n",
    "print(\"F1-macro (XGBoost meta-model):\", f1_score(y_meta_test, y_meta_xgb_pred, average='macro'))\n",
    "print(\"\\nReporte de clasificación (XGBoost meta-model):\")\n",
    "print(classification_report(y_meta_test, y_meta_xgb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99985f4b",
   "metadata": {},
   "source": [
    "### Usando `MLP` como meta-modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76bf2e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para MLP:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.16      0.21     24445\n",
      "           1       0.75      0.66      0.70     82928\n",
      "           2       0.04      0.40      0.08      2544\n",
      "           3       0.41      0.98      0.58       110\n",
      "\n",
      "    accuracy                           0.54    110027\n",
      "   macro avg       0.37      0.55      0.39    110027\n",
      "weighted avg       0.63      0.54      0.58    110027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled = X_meta_train.astype('float32')\n",
    "X_valid_scaled = X_meta_test.astype('float32')\n",
    "\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_meta_train)\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 64, 4),\n",
    "    activation='tanh',\n",
    "    solver='adam',\n",
    "    learning_rate_init=0.0001,\n",
    "    alpha=0.001,\n",
    "    batch_size=256,\n",
    "    max_iter=50,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=20,\n",
    "    random_state=42,\n",
    "    warm_start=True,\n",
    "    validation_fraction=0.05\n",
    ")\n",
    "\n",
    "# Armamos un ciclo para asegurar que nuestro modelo MLP pueda\n",
    "# ver la mayoria de casos del test de entrenamiento.\n",
    "for i in range(6):\n",
    "    mlp.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
    "\n",
    "y_pred = mlp.predict(X_valid_scaled)\n",
    "\n",
    "y_pred_proba_mlp = mlp.predict_proba(X_valid_scaled)\n",
    "\n",
    "print(\"Resultados para MLP:\\n\")\n",
    "print(classification_report(y_meta_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3960450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados por modelo:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Tipo de Ensamble</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>Precision-macro</th>\n",
       "      <th>Recall-macro</th>\n",
       "      <th>Tiempo Entrenamiento (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM Individual</td>\n",
       "      <td>Individual</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoost Individual</td>\n",
       "      <td>Individual</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bagging LightGBM</td>\n",
       "      <td>Ensamble homogéneo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bagging CatBoost</td>\n",
       "      <td>Ensamble homogéneo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stacking LightGBM</td>\n",
       "      <td>Ensamble homogéneo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stacking CatBoost</td>\n",
       "      <td>Ensamble homogéneo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Voting (LGBM + CatBoost)</td>\n",
       "      <td>Ensamble heterogéneo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Stacking Heterogéneo (Meta: LightGBM)</td>\n",
       "      <td>Ensamble heterogéneo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Stacking Heterogéneo (Meta: CatBoost)</td>\n",
       "      <td>Ensamble heterogéneo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Stacking Heterogéneo (Meta: RandomForest)</td>\n",
       "      <td>Ensamble heterogéneo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Stacking Heterogéneo (Meta: LogisticRegression)</td>\n",
       "      <td>Ensamble heterogéneo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Stacking Heterogéneo (Meta: XGBoost)</td>\n",
       "      <td>Ensamble heterogéneo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Stacking Heterogéneo (Meta: MLP)</td>\n",
       "      <td>Ensamble heterogéneo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Modelo      Tipo de Ensamble  \\\n",
       "0                               LightGBM Individual            Individual   \n",
       "1                               CatBoost Individual            Individual   \n",
       "2                                  Bagging LightGBM    Ensamble homogéneo   \n",
       "3                                  Bagging CatBoost    Ensamble homogéneo   \n",
       "4                                 Stacking LightGBM    Ensamble homogéneo   \n",
       "5                                 Stacking CatBoost    Ensamble homogéneo   \n",
       "6                          Voting (LGBM + CatBoost)  Ensamble heterogéneo   \n",
       "7             Stacking Heterogéneo (Meta: LightGBM)  Ensamble heterogéneo   \n",
       "8             Stacking Heterogéneo (Meta: CatBoost)  Ensamble heterogéneo   \n",
       "9         Stacking Heterogéneo (Meta: RandomForest)  Ensamble heterogéneo   \n",
       "10  Stacking Heterogéneo (Meta: LogisticRegression)  Ensamble heterogéneo   \n",
       "11             Stacking Heterogéneo (Meta: XGBoost)  Ensamble heterogéneo   \n",
       "12                 Stacking Heterogéneo (Meta: MLP)  Ensamble heterogéneo   \n",
       "\n",
       "    F1-macro  Precision-macro  Recall-macro Tiempo Entrenamiento (s)  \n",
       "0        0.0              0.0           0.0                     None  \n",
       "1        0.0              0.0           0.0                     None  \n",
       "2        0.0              0.0           0.0                     None  \n",
       "3        0.0              0.0           0.0                     None  \n",
       "4        0.0              0.0           0.0                     None  \n",
       "5        0.0              0.0           0.0                     None  \n",
       "6        0.0              0.0           0.0                     None  \n",
       "7        0.0              0.0           0.0                     None  \n",
       "8        0.0              0.0           0.0                     None  \n",
       "9        0.0              0.0           0.0                     None  \n",
       "10       0.0              0.0           0.0                     None  \n",
       "11       0.0              0.0           0.0                     None  \n",
       "12       0.0              0.0           0.0                     None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model_results = [\n",
    "    # --- Modelos individuales ---\n",
    "    [\"LightGBM Individual\", \"Individual\", 0.000, 0.000, 0.000, None],\n",
    "    [\"CatBoost Individual\", \"Individual\", 0.000, 0.000, 0.000, None],\n",
    "\n",
    "    # --- Modelos con Bagging ---\n",
    "    [\"Bagging LightGBM\", \"Ensamble homogéneo\", 0.000, 0.000, 0.000, None],\n",
    "    [\"Bagging CatBoost\", \"Ensamble homogéneo\", 0.000, 0.000, 0.000, None],\n",
    "\n",
    "    # --- Modelos con Stacking homogéneo ---\n",
    "    [\"Stacking LightGBM\", \"Ensamble homogéneo\", 0.000, 0.000, 0.000, None],\n",
    "    [\"Stacking CatBoost\", \"Ensamble homogéneo\", 0.000, 0.000, 0.000, None],\n",
    "\n",
    "    # --- VotingClassifier heterogéneo ---\n",
    "    [\"Voting (LGBM + CatBoost)\", \"Ensamble heterogéneo\", 0.000, 0.000, 0.000, None],\n",
    "\n",
    "    # --- Stacking heterogéneo (meta-modelos) ---\n",
    "    [\"Stacking Heterogéneo (Meta: LightGBM)\", \"Ensamble heterogéneo\", 0.000, 0.000, 0.000, None],\n",
    "    [\"Stacking Heterogéneo (Meta: CatBoost)\", \"Ensamble heterogéneo\", 0.000, 0.000, 0.000, None],\n",
    "    [\"Stacking Heterogéneo (Meta: RandomForest)\", \"Ensamble heterogéneo\", 0.000, 0.000, 0.000, None],\n",
    "    [\"Stacking Heterogéneo (Meta: LogisticRegression)\", \"Ensamble heterogéneo\", 0.000, 0.000, 0.000, None],\n",
    "    [\"Stacking Heterogéneo (Meta: XGBoost)\", \"Ensamble heterogéneo\", 0.000, 0.000, 0.000, None],\n",
    "    [\"Stacking Heterogéneo (Meta: MLP)\", \"Ensamble heterogéneo\", 0.000, 0.000, 0.000, None]\n",
    "]\n",
    "\n",
    "\n",
    "df_comparativa = pd.DataFrame(\n",
    "    model_results,\n",
    "    columns=[\"Modelo\", \"Tipo de Ensamble\", \"F1-macro\", \"Precision-macro\", \"Recall-macro\", \"Tiempo Entrenamiento (s)\"]\n",
    ")\n",
    "\n",
    "# Ordenar por F1-macro (mayor a menor)\n",
    "df_comparativa = df_comparativa.sort_values(by=\"F1-macro\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Redondear métricas a 4 decimales\n",
    "df_comparativa[[\"F1-macro\", \"Precision-macro\", \"Recall-macro\"]] = df_comparativa[[\"F1-macro\", \"Precision-macro\", \"Recall-macro\"]].round(4)\n",
    "\n",
    "# Mostrar tabla\n",
    "print(\"\\nResultados por modelo:\\n\")\n",
    "display(df_comparativa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3249c4",
   "metadata": {},
   "source": [
    "# Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7234989",
   "metadata": {},
   "source": [
    "- Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), 123–140.\n",
    "- Freund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1), 119–139.\n",
    "- Kuncheva, L. I. (2004). Combining pattern classifiers: Methods and algorithms. Wiley-Interscience.\n",
    "- Rokach, L. (2010). Ensemble-based classifiers. Artificial Intelligence Review, 33(1), 1–39.\n",
    "- Wolpert, D. H. (1992). Stacked generalization. Neural Networks, 5(2), 241–259."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch311 (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
