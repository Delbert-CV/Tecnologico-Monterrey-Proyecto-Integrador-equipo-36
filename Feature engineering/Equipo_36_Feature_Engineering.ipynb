{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64209f1",
   "metadata": {},
   "source": [
    "# Reporte de problemas fitosanitarios en plantaciones de agave\n",
    "--------------------\n",
    "\n",
    "## Equipo 36\n",
    "\n",
    "| Nombre | Matrícula |\n",
    "| ------ | --------- |\n",
    "| André Martins Cordebello | A00572928 |\n",
    "| Enrique Eduardo Solís Da Costa | A00572678 |\n",
    "| Delbert Francisco Custodio Vargas | A01795613 |\n",
    "\n",
    "## Avance 2: Feature engineering\n",
    "\n",
    "- Crear nuevas características para mejorar el rendimiento de los modelos.\n",
    "- Mitigar el riesgo de características sesgadas y acelerar la convergencia de algunos algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a992969",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe8c324",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c141355a",
   "metadata": {},
   "source": [
    "Durante el avance 1 fue posible comprender ciertos comportamientos de nuestro dataset, así como la selección de columnas que sí aportaban información para entender de mejor manera las características de infestación de gorgojos del agave en predios del mismo.\n",
    "\n",
    "Con esto, usando como base los datasets compartidos por la CNIT fue posible obtener el dataset llamada `all_historic_captures.xlsx`, el cual contiene información del 2014 a Agosto de 2025 sobre el nivel de incidencia o infestación encontrado  en predios de agave. Estas muestras se obtuvieron con base a las condiciones definidas en el **Manual Operativo de la campaña contra plagas reglamentadas del Agave**, disponible en https://www.gob.mx/cms/uploads/attachment/file/234136/Manual_Operativo_de_la_campa_a_contra_plagas_reglamentadas_del_agave_2017.compressed.pdf, lo cual fue confirmado por nuestro Sponsor (CNIT).\n",
    "\n",
    "Por lo anterior, el objetivo principal de este proyecto integrador es el desarrollo de un ChatBot el cual:\n",
    "\n",
    "- Pueda recibir reportes que incluyan ubicación, breve descripción de lo encontrado, fotografías y clasificación de riesgo de parte del cuerpo técnico y ciudadanía en general.\n",
    "- Este ChatBot también debe tener la capacidad de responder y alertar a los usuarios sobre focos de infestacion reportados o confirmados en las cercanías.\n",
    "\n",
    "La recepción de esta información es importante ya que permitirá tomar decisiones sobre lo que las brigadas de desinfección deben atacar primero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a65f675",
   "metadata": {},
   "source": [
    "Por lo anterior, durante nuestro EDA (análisis exploratorio) encontramos los siguientes hallazgos:\n",
    "\n",
    "- La captura de gorgojos del agave en las trampas tiende a aumentar en épocas lluviosas.\n",
    "- Los focos de infestación severos y moderados tienen una dispersión geográfica menor.\n",
    "- No es normal encontrar predios de más de 8 años de antiguedad, lo que nos da una idea de cuáles predios podrían estar posiblemente abandonados.\n",
    "- La mayor densidad de trampas se encuentra en el estado de Jalisco.\n",
    "- No es normal encontrar focos severos de infestación, pero la presencia de éstos aumenta en la época lluviosa. Es posible confirmar que los casos severos de infestación son casos atípicos, ya que el valor del percentil 95 se encuentra en 17.5 capturas por trampa.\n",
    "- Durante la pandemia (2020 a 2024 aproximadamente), el muestreo de las trampas colocadas no fue tan constante como en años posteriores o previos a la pandemia. Esto causa un efecto de sesgo en nuestro dataset.\n",
    "\n",
    "\n",
    "Y al transformar un poco nuestro dataset obtuvimos estas columnas finales:\n",
    "\n",
    "| Feature  | Tipo | Notas |\n",
    "| -------  | ---- | ----- |\n",
    "| tramp_id | object | Es el ID único que se le da a la trampa al colocarse en alguna parcela o predio. Una misma trampa puede colocarse en distintos predios, pero la identificación de la misma cambia acorde a dónde se colocó. |\n",
    "| sampling_date | datetime64[ns] | Es la fecha en la que se llevó a cabo el conteo de cadáveres de gorgojo en la trampa.|\n",
    "| lat y lon | float64 | La `latitud` y `longitud` permiten conocer la ubicación de donde se llevó a cabo el muestreo. |\n",
    "| municipality | object | Municipio al que pertenecía la trampa al momento de hacer el muestreo. |\n",
    "| square_area | float64 | Area que cubre el predio donde se colocó la trampa. |\n",
    "| plantation_age | float64 | Años que una plantación de agave tiene desde la última purga. |\n",
    "| capture_count | float64 | Cantidad de gorgojos del agave encontrados dentro de la trampa. |\n",
    "| state | object | Estado de México donde se encontraba la trampa colocada. |\n",
    "| severity | object | lLa severidad de la infestación encontrada durante el muestreo. Estos niveles fueron definidos por la SICAFI.|\n",
    "| Month | int32 | Mes en que se llevó a cabo el muestreo de la trampa. |\n",
    "| Year | int32 | Año en que se llevó a cabo el muestreo de la trampa. |\n",
    "| MonthName | object | Nombre del   mes en que se llevó a cabo el muestreo de la trampa. |\n",
    "| MonthYear | datetime64[ns] |  Combinación del año y mes en que se llevó a cabo el muestreo de la trampa. Esta ya es una característica creada a partir de otras columnas del Dataset. |\n",
    "\n",
    "Con esto, estaremos trabajando la ingeniería de característiscas con base en estas columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5124de",
   "metadata": {},
   "source": [
    "### Librerías  a importar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "423bba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b290f6",
   "metadata": {},
   "source": [
    "### Cargamos el dataset obtenido anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1525496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de dataset\n",
    "all_historic_captures_df = pd.read_excel( \"all_historic_captures.xlsx\", sheet_name=\"historic_captures\", header= 0)\n",
    "\n",
    "# Eliminamos las columnas que usamos para llevar a cabo una mejor comprensión en el EDA,\n",
    "# esto porque debemos decidir más adelante si serán necesarias para nuestro proceso de FE.\n",
    "all_historic_captures_df.drop(labels=['Month', 'Year', 'MonthName', 'MonthYear', 'plantation_age_group', 'surface_group', 'no_area'],axis=1, inplace=True)\n",
    "\n",
    "all_historic_captures_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf3c8b",
   "metadata": {},
   "source": [
    "### `Feature engineering` sobre `square_area`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24517029",
   "metadata": {},
   "source": [
    "Al revisar nuevamente los valores de `square_area` (o `Superficie (ha)` originalmente), es posible notar que casi el 48% de éstos es igual a `0.00`. Esto, como discutimos con anterioridad, indica que durante el proceso de muestreo de años previos a 2024 y 2025 no hubo un control de calidad para mitigar la falta de información en esta columna. Esto fue confirmado por nuestro Sponsor.\n",
    "\n",
    "Como consideramos que esta información es importante, hemos decidido trabajar la siguiente estrategia de inserción para eliminar valores nulos:\n",
    "\n",
    "- Reducir la cantidad de valores `0.00` por medio del `ID de cada trampa`. Como el ID de la trampa está en función del predio muestreado, es posible asumir que con que un ID de trampa contenga información del área, esta se puede replicar a las demás instancias de ese mismo ID de trampa en el tiempo.\n",
    "  - El reto con esta implementación existe si para un mismo ID de trampa se encuentran valores distintos de área. En estos casos, usaremos el valor del área más cercano en el tiempo.\n",
    "\n",
    "- Luego de esto, usaremos dos algoritmos, conocidos como `RadiusNeighborsRegressor` y `K-NN (K nearest neighbour)` para verificar cuales trampas con un registro de superficie distinto a `0.00` se encuentran cerca de otras trampas con un área distinta a `0.00`. La distancia entre éstas trampas debe ser de un mínimo de 100m hasta 500m en época de jima según el manual de operación. \n",
    "\n",
    "- Por último, analizaremos cuántos registros quedan pendientes aún de tener un valor distinto a `0.00` después de aplicar la estrategia anterior, y decidiremos más adelante qué hacer con ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8935499",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_historic_captures_df\n",
    "\n",
    "print(f\"Trampas registradas con área mayor a 0.00: { (df['square_area'] > 0 ).sum()}\")\n",
    "print(f\"Trampas registradas con área igual a 0.00: { (df['square_area'] == 0 ).sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d33e33",
   "metadata": {},
   "source": [
    "##### Usando el ID de trampa para llenar valores vacíos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ae5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_nearest_temporal_area_group(group_to_verify):\n",
    "\n",
    "    # Casteamos datetime sobre sampling_date (fecha de muestreo)\n",
    "    group = group_to_verify.copy()\n",
    "    group['sampling_date_dt'] = pd.to_datetime(group['sampling_date'])\n",
    "    \n",
    "    # Tomamos solo las areas mayores a 0.00\n",
    "    areas_known = group.loc[group['square_area'] > 0.00, ['sampling_date_dt', 'square_area']]\n",
    "    if areas_known.empty:\n",
    "        return pd.Series(np.nan, index=group.index)\n",
    "    \n",
    "    # Calculamos la diferencia de tiempo en dias\n",
    "    diffs = np.abs(\n",
    "        group['sampling_date_dt'].values[:, None] - areas_known['sampling_date_dt'].values[None, :]\n",
    "    )\n",
    "    \n",
    "    # Tomamos la muestra con menor tiempo en comparación con la fecha que estamos revisando\n",
    "    idxmin = diffs.argmin(axis=1)\n",
    "    \n",
    "    return pd.Series(areas_known['square_area'].iloc[idxmin].values, index=group.index)\n",
    "\n",
    "\n",
    "# Generamos una copia de  la columna square_area por precaucion\n",
    "df['square_area_imputed'] = df['square_area'].copy()\n",
    "\n",
    "# Generamos la columna de imputation_method: con lo siguiente:\n",
    "# Si el area es mayor a 0.00, entonces colocamos 'original'\n",
    "# Si el area es menor a 0.00, la marcamos para insertar valores con `none`\n",
    "df['imputation_method'] = np.where(df['square_area'] > 0, 'original', 'none')\n",
    "\n",
    "# Generamos una máscara para filtrar el dataframe\n",
    "mask = df['square_area'] == 0\n",
    "\n",
    "# Usamos la función para encontrar el area más próxima a cada tramp_id con area 0.00\n",
    "nearest_values = (\n",
    "    df.groupby('tramp_id', group_keys=False)\n",
    "      .apply(get_nearest_temporal_area_group)\n",
    ")\n",
    "\n",
    "# Insertamos los valores al dataframe original \n",
    "# Insertamos la forma en que se llenó la informacion del area\n",
    "df.loc[mask, 'square_area_imputed'] = nearest_values[mask]\n",
    "df.loc[mask & nearest_values.notna(), 'imputation_method'] = 'same_trap_id_temporal'\n",
    "\n",
    "rows_affected = df[ df['imputation_method'] == 'same_trap_id_temporal']\n",
    "print(f\"Se llenaron { len(rows_affected) } registros usando el mismo ID de la trampa en el tiempo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf31b271",
   "metadata": {},
   "source": [
    "Y logramos llenar aproximadamente el 9.57% de datos faltantes respecto al área usando esta técnica. Por lo tanto, debemos llenar aún 320,135 registros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d31338",
   "metadata": {},
   "source": [
    "##### Ahora usaremos KNN para rellenar el resto de información"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a643e0d",
   "metadata": {},
   "source": [
    "En este momento tenemos 3  distintos valores en la columna `imputation_method`, los cuales son:\n",
    "\n",
    "- `original`: son todas los registros de area que originalmente son distintos a 0.00 (la mayoría en 2024 y 2025)\n",
    "- `none`: son los registros que aún no hemos cambiado.\n",
    "- `samte_trap_id_temporal`: son los registros que tienen un valor inseratdo en la columna `square_area_imputed` por medio de usar el `trap_id` y verificar la muestra más cercana en el tiempo que tenga area mayor a 0.00.\n",
    "\n",
    "\n",
    "Entonces, entrenaremos 3 distintos modelos para seleccionar cuál de éstos utilizaremos para predecir el área de cada predio de agave que se registró mal. Algo importante es que debemos asignarle al parámetro `weights` la opción de `distance`, ya que de ésta forma se pondera la distancia para darle más importancia a los vecinos más cercanos al punto que estamos analizando.\n",
    "\n",
    "También es importante mencionar que usaremos la `latitud` y `longitud`, junto a la métrica de `Haversine` para calcular la distancia entre los puntos. La métrica de `Haversine` toma en cuenta la curvatura de una esfera en las distancias, por lo que es necesario conocer el radio de la Tierra para obtener distancias coherentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c2f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import RadiusNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "import warnings\n",
    "\n",
    "print(\"\\nOrigen de los valores de la columna square_area_imputed:\")\n",
    "print(df['imputation_method'].value_counts())\n",
    "\n",
    "# Ignoraremos warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# No debemos  sobre-escribir lo que ya imputamos anterioremente\n",
    "# Entonces usaremos esos indexes como mascara\n",
    "train_mask = df['imputation_method'].isin(['original', 'same_trap_id_temporal'])\n",
    "\n",
    "# Tomamos los indexes que ya sabemos que en teoría están bien\n",
    "X_train = df[train_mask][['lat', 'lon']].values\n",
    "y_train = df[train_mask]['square_area_imputed'].values\n",
    "\n",
    "# Nuestro test serán los registros de los que sabemos que tienen 0.00 en square_area\n",
    "# y que no han sido imputados por el paso previo.\n",
    "test_mask = df['imputation_method'] == 'none'\n",
    "X_test = df[test_mask][['lat', 'lon']].values\n",
    "\n",
    "print(f\"\\n\\nRegistros a predecir su quare_area: {len(X_test):,}\")\n",
    "\n",
    "# Usaremos Harversine para tomar en cuenta la curvatura de la tierra en la distancia que usemos para conocer cuales trampas están\n",
    "# cerca unas de otras, y se necesita que las longitudes y latitudes estén en radianes\n",
    "X_train_rad = np.radians(X_train)\n",
    "X_test_rad = np.radians(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683a0974",
   "metadata": {},
   "source": [
    "Procedemos a probar con Cross-Validation nuestro `KneighborsRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [3, 4, 5]:\n",
    "    knn_k = KNeighborsRegressor(\n",
    "        n_neighbors=k,\n",
    "        weights='distance',\n",
    "        metric='haversine'\n",
    "    )\n",
    "    scores = cross_validate(knn_k, X_train_rad, y_train, cv=5,  scoring={'mae': 'neg_mean_absolute_error', 'r2': 'r2'}, return_train_score=False)\n",
    "    \n",
    "    print(f\"Con K={k} obtenemos: MAE={-scores['test_mae'].mean():.4f}, R²={scores['test_r2'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8ba0fd",
   "metadata": {},
   "source": [
    "Y nos damos cuenta que tenemos modelos basados en KNN que no tienen poder predictivo, esto debido a que el R^2 mostrado es negativo. Por lo tanto, es casi seguro que es mejor usar la `media` o `mediana` para de los datos para rellenar la información de `square_area` en lugar de estos resultados.\n",
    "\n",
    "Pero a continuación hacemos uso de `RadiusNeighborsRegressor` para determinar si es posible rellenar la columna de área."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3257e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import RadiusNeighborsRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Configuramos el modelo\n",
    "r_regressor = RadiusNeighborsRegressor(\n",
    "    radius = 0.500 / 6378,  # 500 metros (convertidos a radianes)\n",
    "    weights = 'distance',\n",
    "    metric = 'haversine'\n",
    ")\n",
    "\n",
    "# Configuramos la validacion cruzada\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "mae_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_rad)):\n",
    "    \n",
    "    X_tr, X_val = X_train_rad[train_idx], X_train_rad[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    # Entrenamos\n",
    "    r_regressor.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Predecimos\n",
    "    preds = r_regressor.predict(X_val)\n",
    "    \n",
    "    # Filtramos los casos donde no hubo vecinos, ya que arruina las métricas\n",
    "    mask_valid = ~np.isnan(preds)\n",
    "    \n",
    "    # Si tenemos algun fold que no obtuvo algun match, debemos informarlo\n",
    "    if np.sum(mask_valid) == 0:\n",
    "        print(f\"Fold {fold + 1}: sin vecinos válidos.\")\n",
    "        continue\n",
    "    \n",
    "    # Calculamos las metricas para comparar\n",
    "    mae = mean_absolute_error(y_val[mask_valid], preds[mask_valid])\n",
    "    r2 = r2_score(y_val[mask_valid], preds[mask_valid])\n",
    "    \n",
    "    mae_scores.append(mae)\n",
    "    r2_scores.append(r2)\n",
    "    \n",
    "    # Imprimimos las metricas\n",
    "    print(f\"Fold {fold + 1}: MAE={mae:.4f}, R²={r2:.4f} (n={np.sum(mask_valid)})\")\n",
    "\n",
    "# Resultados promedio\n",
    "print(\"\\nPromedio de métricas solo con vecinos válidos:\")\n",
    "print(f\"MAE medio = {np.mean(mae_scores):.4f}\")\n",
    "print(f\"R² medio = {np.mean(r2_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "radius_predictions = r_regressor.predict(X_test_rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9558fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_descriptive_stats_for_knn_model( title, preds ):\n",
    "    \n",
    "    print('='*70)\n",
    "    print(f\"{title}\")\n",
    "    print('='*70)\n",
    "    print(f\"Promedio: {np.nanmean(preds)}\")\n",
    "    print(f\"Mediana: {np.nanmedian(preds)}\")\n",
    "    print(f\"Desviacion estandar: {np.nanstd(preds)}\")\n",
    "    print(f\"Valor maximo: {np.nanmax(preds)}\")\n",
    "    print(f\"Valor mínimo: {np.nanmin(preds)}\")\n",
    "    print(f\"Cantidad de trampas sin área registrada: {np.isnan(preds).sum()}\\n\")\n",
    "\n",
    "\n",
    "print_descriptive_stats_for_knn_model('Estadísticas para el modelo RadiusNeighborsRegressor sobre el area:', radius_predictions)\n",
    "\n",
    "already_imputed_mask = df['imputation_method'].isin(['original', 'same_trap_id_temporal'])\n",
    "print_descriptive_stats_for_knn_model('Estadísticas de los datos originales con area mayor a 0.00:', df[already_imputed_mask][df['square_area'] > 0.00000].square_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be4cf13",
   "metadata": {},
   "source": [
    "#### Justificación sobre uso de modelos RadiusRegressor y KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caa6763",
   "metadata": {},
   "source": [
    "Al revisar los resultados con cada modelo para obtener el area en los predios donde no se contaba con esta información, nos dimos cuenta de lo siguiente:\n",
    "\n",
    "- El modelo `RadiusNeighborsRegressor` reportó las estadísticas descriptivas más parecidas a la información original, y sobre todo, funcionó de mejor manera durante nuestr cross-validation. La desventaja de este modelo es que nos deja `8358` trampas como aisladas, y por lo tanto, sin área. Estos resultados fueron con base a usar 500 metros de radio para encontrar las trampas cercanas, lo que se decidió con base a la distancia máxima documentada entre trampas en el manual operativo.\n",
    "\n",
    "- Los modelos `KNN` no tuvieron buenos resultados, al usar un algoritmo `KNeighborsRegressor` con `K=3`, `K=4` y `K=5` respectivamente, logran rellenar toda la información faltante, pero usando Cross-Validation fue posible encontrar que es mejor utilizar la media o mediana en los `8358` datos faltantes que utilizar alguno de estos modelos para predecir el área del predio.\n",
    "\n",
    "Por lo anterior, decidimos hacer uso de la siguiente estrategia:\n",
    "\n",
    "- Usar `RadiusNeighborsRegressor` para rellenar la información del área en las trampas posibles.\n",
    "- Las trampas restantes que queden sin valor (`8358`) harán uso de la mediana, la cual es más resistente a outliers, del área para rellenar en `square_area_imputed`.\n",
    "\n",
    "Se decidió mantener la columna original `square_area` para términos de mantener la información original para estudios posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = df[test_mask].index\n",
    "\n",
    "# Creamos otra máscara para insertar valores predecidos por los modelos #2 y #3\n",
    "# Como el modelo #3 es el que tiene las características descriptivas más parecidas a los valores originales,\n",
    "# usaremos ese de primero para insertar valores.\n",
    "RadiusRegressor_mask = ~np.isnan(radius_predictions)\n",
    "model3_nan_mask = np.isnan(radius_predictions)\n",
    "\n",
    "df.loc[test_indices[RadiusRegressor_mask], 'square_area_imputed'] = radius_predictions[RadiusRegressor_mask]\n",
    "df.loc[test_indices[RadiusRegressor_mask], 'imputation_method'] = 'radius_regressor'\n",
    "\n",
    "# Luego insertaremos las predicciones hechas con el modelo KNN con 4 vecinos\n",
    "df.loc[test_indices[model3_nan_mask], 'square_area_imputed'] = np.nanmedian(df['square_area_imputed'])\n",
    "df.loc[test_indices[model3_nan_mask], 'imputation_method'] = 'median'\n",
    "\n",
    "# Imprimimos cuántos valores NaN o None nos quedan por rellenar aun\n",
    "remaining_nan = df['square_area_imputed'].isna().sum()\n",
    "remaining_none = (df['imputation_method'] == 'none').sum()\n",
    "\n",
    "print(f\"Valores NaN en square_area_imputed: {remaining_nan}\")\n",
    "print(f\"Registros con método 'none': {remaining_none}\")\n",
    "\n",
    "if remaining_nan == 0 and remaining_none == 0:\n",
    "    print(\"Todos los valores se han rellenado.\")\n",
    "else:\n",
    "    print(\"\\nSe deben rellenar aún más valores.\\n\\n\")\n",
    "    \n",
    "df['imputation_method'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7184207",
   "metadata": {},
   "source": [
    "### `Feature engineering` sobre `sampling_date`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c04c0",
   "metadata": {},
   "source": [
    "Primero obtendremos de nuevo las columnas de `Year` y `Year-Month` desde la fecha de muestreo. Estas variables son lineales, por lo tanto solo debemos normalizarlas o escalarlas.\n",
    "\n",
    "Respecto a `month`, `day_of_year`, `day_of_week` y `week_of_year` usaremos la representación trigonométrica (coseno y seno) para mantener las distancias entre mese, incluyendo el fin de año e inicio de año. Esto debido a que en el EDA fue bastante notorio que existe  un ciclo en la presencia del gorgojo de la siguiente manera:\n",
    "\n",
    "- De Enero a Mayo la cantidad de gorgojos encontrados fue menor que la cantidad de gorgojos muestreados de Junio a Diciembre.\n",
    "- Nuestro Sponsor considera que esto se debe a las lluvias, y  por lo tanto más adelante incluiremos la cantidad de mm de lluvia registrada por municipio y estado (esto con la ayuda del sitio oficial del gobierno de México)\n",
    "- Es necesario mantener la relación de Diciembre a Enero, y para esto se presta el uso de un encoding cíclico.\n",
    "- Al usar un encoding ciclico en los meses, días y semanas normalizamos la data en  un solo paso.\n",
    "- Una razón importante de implementar estos cambios es que todos se pueden obtener de la fecha de muestreo o reporte, por lo que al implementar el ChatBot esta información se podrá obtener solo con la fecha de reporte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe9f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month']        = df['sampling_date'].dt.month\n",
    "df['year']         = df['sampling_date'].dt.year\n",
    "df['year_month']   = df['sampling_date'].dt.strftime('%Y-%m')\n",
    "\n",
    "# Hacemos el encoding trigonometrico para day_of_year (preserva la distancia de Diciembre a Enero tambien)\n",
    "df['day_of_year_sin'] = np.sin( 2 * np.pi * df['sampling_date'].dt.dayofyear / 365 )\n",
    "df['day_of_year_cos'] = np.cos( 2 * np.pi * df['sampling_date'].dt.dayofyear / 365 )\n",
    "\n",
    "# Es probable que el día de la semana funcione también, y es necesario preservar la distancia de domingo a lunes y viceversa\n",
    "df['day_of_week_sin'] = np.sin( 2 * np.pi * df['sampling_date'].dt.day_of_week / 7 )\n",
    "df['day_of_week_cos'] = np.cos( 2 * np.pi * df['sampling_date'].dt.day_of_week / 7 )\n",
    "\n",
    "# Trabajamos lo mismo con week_of_year\n",
    "df['week_of_year_sin'] = np.sin( (df['sampling_date'].dt.isocalendar().week/ 52 * 2 * np.pi) )\n",
    "df['week_of_year_cos'] = np.cos( (df['sampling_date'].dt.isocalendar().week/ 52 * 2 * np.pi) )\n",
    "\n",
    "# Encodeamos el mes tambien\n",
    "df['month_sin'] = np.sin( (df['sampling_date'].dt.month / 12 * 2 * np.pi) )\n",
    "df['month_cos'] = np.cos( (df['sampling_date'].dt.month / 12 * 2 * np.pi) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda451d0",
   "metadata": {},
   "source": [
    "El manual operativo de protección fitosanitaria menciona también que la época crítica de monitoreo se encuentra definida de Mayo a Septiembre, por lo que agregaremos una columna la cual tenga los siguientes valores:\n",
    "\n",
    "- `critica` para la temporada de lluvias definida en el manual operativo.\n",
    "- `normal` para la temporada que no se considera como de lluvia en el manual operativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de846e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "critical_months = [ 5, 6, 7, 8, 9 ]\n",
    "\n",
    "#  Usamos list comprehension\n",
    "df['critical_season'] = [ 1 if m in critical_months else 0 for m in df['month'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d373e8",
   "metadata": {},
   "source": [
    "### `Feature engineering` para características espaciales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a43fb",
   "metadata": {},
   "source": [
    "Es importante conocer la densidad de la presencia de trampas para que nuestro ChatBot pueda responder si la ubicación enviada se encuentra cerca de alguno de los puntos de infestación severos recientes, y dar un mejor contexto a los usuarios.\n",
    "\n",
    "Para esto, hemos decidido crear los siguientes features:\n",
    "\n",
    "- `distance_to_nearest_hotspot`\n",
    "- `hotspots_within_1km`\n",
    "- `hotspots_within_5km`\n",
    "\n",
    "Y para definir un hotspot tenemos la siguiente opción:\n",
    "\n",
    "- Usar la actual escala `severity` de nuestro dataset y hacer encoding sobre ésta, ya que según el manual operativo se cuenta con 4 niveles de infestación:\n",
    "    - `Primer nivel`: se presenta cuando en las trampas no hay presencia de gorgojos.\n",
    "    - `Segundo nivel`: se da cuando en las trampas se halla presencia de entre 1 a 24 gorgojos.\n",
    "    - `Tercer nivel`: se considera cuando en una trampa se encuentran 25 a 75 gorgojos.\n",
    "    - `Cuarto nivel`: se presenta cuando en las trampas se encuentran más de 75 gorgojos. Esto se considera como un foco de infestación, porque representa una media de 5 gorgojos muestreados por día en 15 días.\n",
    "\n",
    "Entonces el primer paso es hacer encoding sobre estos niveles, los cuales son una variable ordinal (ya que se presenta una escala de mejor a peor escenario), y debemos codificarlos de manera que el LLM a implementar pueda relacionar el nivel de riesgo que cada foco implica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nivel original de severidad:\\n\\n{df['severity'].value_counts()}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "severity_mapping = {\n",
    "     '0'    : 0,\n",
    "     '1-25' : 1,\n",
    "     '25-75': 2,\n",
    "     '>75'  : 3\n",
    "}\n",
    "\n",
    "df['severity_encoded'] = df['severity'].map(severity_mapping)\n",
    "df['severity_encoded'] = df['severity_encoded'].astype(int)\n",
    "print( f\"Nivel codificado de severidad:\\n\\n{df['severity_encoded'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8fe576",
   "metadata": {},
   "source": [
    "Y ahora que ya tenemos definido el nivel de riesgo por cada trampa muestreada, es posible  iniciar a definir las variables `distance_to_nearest_hotspot`, `hotspots_between_5km` y `hotspots_between_10km` respectivamente, ya que éstas le darán más contexto al LLM. Y para reducir la ventana de búsqueda, haremos uso de una ventana de tiempo de 15 días."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08fc6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "# Funcion para usar la distancia usando la formula de Harversine\n",
    "\n",
    "def distance_between_traps(lat1, lon1, lat2, lon2):\n",
    "    # Radio de la tierra\n",
    "    r = 6371.0\n",
    "    \n",
    "    # Volvemos radianos las ubicaciones\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Diferencia entre latitudes y longitudes en radianes\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    # Formular de Harversine\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    distance = r * 2 * np.arcsin(np.sqrt(a))\n",
    "    # Resultado\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para tomar en  cuenta la cantidad de hotspots a 5km y 10km que estuvieron presentes durante los 15 días previos y posteriores a la muestra\n",
    "# si se requiere, se pueden cambiar los dias previos y posteriores a analizar por fecha de muestreo.\n",
    "# Si no se encuentra alguna trampa cercana como hotspot a la trampa a analizar, entonces usaremos el valor 9999 km para\n",
    "# que nuestro LLM entienda que el riesgo de un hotspot está lejos\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def proximity_to_hotspots(df, days_before=15, days_after=15):\n",
    "    \n",
    "    df = df.sort_values(\"sampling_date\").reset_index(drop=True)\n",
    "    df[\"sampling_date\"] = pd.to_datetime(df[\"sampling_date\"])\n",
    "\n",
    "    hotspots = df[df[\"is_hotspot\"]][[\"sampling_date\", \"lat\", \"lon\"]].copy()\n",
    "    hotspots[\"sampling_date\"] = pd.to_datetime(hotspots[\"sampling_date\"])\n",
    "\n",
    "    # Inicializamos las salidas (se crean las nuevas columnas)\n",
    "    # Si no se encuentra alguna trampa cercana, dejaremos 9999 como el valor predeterminado\n",
    "    df[\"distance_to_nearest_hotspot\"] = 9999\n",
    "    df[\"hotspots_within_1km\"] = 0\n",
    "    df[\"hotspots_within_5km\"] = 0\n",
    "\n",
    "    # Usaremos fechas unicas para procesar de a poco cada valor\n",
    "    unique_dates = df[\"sampling_date\"].sort_values().unique()\n",
    "\n",
    "    # Iniciamos el ciclo por fecha unica\n",
    "    for current_date in unique_dates:\n",
    "        \n",
    "        # Fecha minima y maxima a calcular por cada trampa\n",
    "        date_min = current_date - timedelta(days=days_before)\n",
    "        date_max = current_date + timedelta(days=days_after)\n",
    "\n",
    "        # Tomamos las trampas por fecha unica\n",
    "        df_subset = df[df[\"sampling_date\"] == current_date]\n",
    "        \n",
    "        # buscamos los hotspots en los 15 dias previos y posteriores\n",
    "        hot_subset = hotspots[\n",
    "            (hotspots[\"sampling_date\"] >= date_min) & \n",
    "            (hotspots[\"sampling_date\"] <= date_max)\n",
    "        ]\n",
    "\n",
    "        if len(hot_subset) == 0 or len(df_subset) == 0:\n",
    "            continue\n",
    "\n",
    "        # Armamos un arbol KDT\n",
    "        tree = cKDTree(np.radians(hot_subset[[\"lat\", \"lon\"]].values))\n",
    "\n",
    "        # \n",
    "        trap_coords = np.radians(df_subset[[\"lat\", \"lon\"]].values)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            # Para Scipy 1.9 o superior\n",
    "            dists, _ = tree.query(trap_coords, k=1, workers=-1)\n",
    "        except TypeError:\n",
    "            # Para versiones antiguas de SciPy (como la nuestra)\n",
    "            dists, _ = tree.query(trap_coords, k=1)\n",
    "\n",
    "        dists_km = dists * 6371  # convert radians to km\n",
    "\n",
    "        \n",
    "        count_1km = tree.query_ball_point(trap_coords, r=1/6371, workers=-1)\n",
    "        count_5km = tree.query_ball_point(trap_coords, r=5/6371, workers=-1)\n",
    "\n",
    "        idx = df_subset.index\n",
    "        df.loc[idx, \"distance_to_nearest_hotspot\"] = dists_km\n",
    "        df.loc[idx, \"hotspots_within_1km\"] = [len(c) for c in count_1km]\n",
    "        df.loc[idx, \"hotspots_within_5km\"] = [len(c) for c in count_5km]\n",
    "\n",
    "    return df\n",
    "\n",
    "severe_risk_capture_threshold = 75\n",
    "\n",
    "df['is_hotspot'] = (\n",
    "    (df['capture_count'] > severe_risk_capture_threshold) | (df['severity_encoded'] == 3 )\n",
    ")\n",
    "\n",
    "df = proximity_to_hotspots(df) # Obtendremos la cantidad de hotspots a 5km y 10km en los 15 dias previos y futuros por defecto, y la distancia al hotspot mas cercano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc2006",
   "metadata": {},
   "source": [
    "### ¿`Feature engineering` sobre `Estado` y `Municipalidad`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e9e216",
   "metadata": {},
   "source": [
    "Antes de analizar estas columnas, debemos corregir 2 aspectos que se encuentran presentes en el Dataset:\n",
    "\n",
    "- Cambiar `Michoacan de Ocampo` por `Michoacan`.\n",
    "- Cambiar el estado de `Ixtlan del Rio` a `Nayarit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f90e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Primero corregimos Michoacan de Ocampo para que forme parte de la categoría 'Michoacan'\n",
    "df['state'].replace('MICHOACÁN DE OCAMPO', 'MICHOACAN', inplace=True)\n",
    "\n",
    "# Corregimos las instancias de Ixtlan del \n",
    "df.loc[df['municipality'] == 'IXTLÁN DEL RÍO', 'state'] = 'NAYARIT'\n",
    "\n",
    "# Aprovechamos a eliminar tildes\n",
    "\n",
    "def remove_accents(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    return ''.join(c for c in unicodedata.normalize('NFKD', str(text)) if not unicodedata.combining(c))\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = remove_accents(text)\n",
    "    return re.sub(r'[^A-Za-z0-9\\s]', '', text).upper()\n",
    "\n",
    "state_municipality_cols = ['state', 'municipality']\n",
    "\n",
    "for col in state_municipality_cols:\n",
    "    df[col] = df[col].apply(remove_accents)\n",
    "    \n",
    "for col in state_municipality_cols:\n",
    "    df[col] = df[col].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Valores únicos para State: {df['state'].nunique()}\")\n",
    "print(f\"Valores únicos para Municipality: {df['municipality'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e2f2f7",
   "metadata": {},
   "source": [
    "Luego de esto, hemos decidido no llevar a cabo algún encoding para estas columnas por los siguientes motivos:\n",
    "\n",
    "- El LLM que se utilice para desarrollar el ChatBot puede hacer uso de estos textos para dar respuestas descriptivas. No es lo mismo alimentar al ChatBot haciendo uso de valores como `1`, `2` o `3` que `Jalisco`, `Nayarit` y `Michoacan`, por ejemplo.\n",
    "- La investigación de Koloski *et al.* indica que el uso de text embeddings para LLMs ayuda a que el LLM tenga un mejor resultado acorde a lo que se le cuestione. Por lo tanto, tendremos que convertir nuestra data tabular a embeddings para lograr que nuestro ChatBot responda de mejor manera usando el contexto  que nuestro dataset le dé.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92a391d",
   "metadata": {},
   "source": [
    "### `Feature engineering` para `Text Embeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d57f6a",
   "metadata": {},
   "source": [
    "Ahora que hemos generado múltiples features (16 aproximadamente), se procederá a crear **features de texto normalizadas** para el entrenamiento del LLM y el sistema RAG:\n",
    "\n",
    "**Features de texto principales:**\n",
    "1. **`text_feature_location`**: Ubicación geográfica la cual incluye el municipio, estado y coordenadas junto al tamaño del predio y edad de plantación.\n",
    "\n",
    "2. **`text_feature_risk`**: Evaluación de riesgo basada en densidad de focos de infestación cercanos (distancia al foco severo más cercano, focos severos a 5km y 10km), y la condición propia del foco de infestación. \n",
    "\n",
    "3. **`text_feature_capture`**: Reporte de captura con fecha, cantidad de gorgojos, nivel de severidad descriptivo, y contexto de temporada crítica o no. \n",
    "\n",
    "4. **`text_feature_plantation`**: Características del predio (área con método de obtención, edad del cultivo, estado de desarrollo) contextualizadas en términos agrícolas.\n",
    "\n",
    "5. **`text_feature_all_things`**: Columna compuesta que integra información temporal, espacial y de riesgo.\n",
    "\n",
    "\n",
    "**Normalización de texto aplicada:**\n",
    "- Conversión de valores discretos (como la severidad) a texto.\n",
    "- Estandarización de categorías por medio del uso de diccionarios.\n",
    "- Inclusión explícita de unidades (kilómetros, hectáreas, años, etc.) para dar contexto al LLM.\n",
    "- Origen de los valores faltantes como texto interpretable\n",
    "- Todo se hizo en idioma español.\n",
    "\n",
    "**Propósito de estas columnas:**\n",
    "1. **Fine-tuning del LLM**: Se utilizarán las features como contexto para crear datasets de instrucción y respuesta que enseñen al modelo patrones de evaluación de riesgo.\n",
    "\n",
    "2. **Sistema RAG (embeddings)**: Las columnas `text_feature` se convertirán en embeddings para relacionar palabras, permitiendo al chatbot recuperar registros históricos relevantes durante la inferencia. De esta forma convertimos información tabular a algo útil para que nuestro Chatbot, por medio un LLM, pueda interpretar y usar esta información.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d7f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_dict = {\n",
    "    0: 'sin riesgo',\n",
    "    1: 'de riesgo leve',\n",
    "    2: 'de riesgo moderado',\n",
    "    3: 'de riesgo severo'\n",
    "}\n",
    "\n",
    "critical_season_dict = {\n",
    "    0: 'normal',\n",
    "    1: 'critica'\n",
    "}\n",
    "\n",
    "\n",
    "area_information_dict = {\n",
    "    'original': \" (area historica registrada correctamente)\", \n",
    "    'radius_regressor' : \"(el area en hectareas se calculo usando trampas cercanas)\", \n",
    "    'median' : \"(el area en hectareas se calculo usando la mediana de los datos de muestra)\", \n",
    "    'same_trap_id_temporal' : \" (valor del area en hectarea obtenido usando registros historicos)\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "df['text_feature_location'] = (\n",
    "    \"La trampa con identificacion \" + df['tramp_id'].astype(str) +\n",
    "    \" se ubicaba en la latitud \" + df['lat'].astype(str) +\n",
    "    \" y longitud \" + df['lon'].astype(str) +\n",
    "    \" en el dia \" + df['sampling_date'].dt.day.astype(str) +\n",
    "    \" del mes \" + df['sampling_date'].dt.month_name(locale='es_ES.utf8') +\n",
    "    \" del año \" + df['sampling_date'].dt.year.astype(str) + \". \" +\n",
    "    \"La ubicacion pertenece a la municipalidad de \" + df['municipality'].astype(str) +\n",
    "    \" del Estado de \" + df['state'].astype(str) +\n",
    "    \" y la plantación de agave tenia \" + df['plantation_age'].astype(str) +\n",
    "    \" años desde la ultima jima. El predio tiene un area superficial en hectareas de \" +\n",
    "    df['square_area_imputed'].astype(str) + \" \" +\n",
    "    df['imputation_method'].map(area_information_dict).astype(str)\n",
    ")\n",
    "\n",
    "df['text_feature_risk'] = (\n",
    "    \"La trampa con identificacion \" + df['tramp_id'].astype(str) +\n",
    "    \" en el dia \" + df['sampling_date'].dt.day.astype(str) +\n",
    "    \" del mes \" + df['sampling_date'].dt.month_name(locale='es_ES.utf8') +\n",
    "    \" del año \" + df['sampling_date'].dt.year.astype(str) +\n",
    "    \" se consideraba como un punto \" + df['severity_encoded'].map(severity_dict).astype(str) +\n",
    "    \". Tambien se tenia al foco de infestacion severa mas cercano a \" + df['distance_to_nearest_hotspot'].astype(str) +\n",
    "    \" kilometros, con \" + df['hotspots_within_1km'].astype(str) +\n",
    "    \" focos severos a 1 kilometro de distancia y \" + df['hotspots_within_5km'].astype(str) +\n",
    "    \" focos severos de infestacion a 5 kilometros de distancia.\"\n",
    ")\n",
    "\n",
    "df['text_feature_capture'] = (\n",
    "    \"Durante la temporada \" + df['critical_season'].map(critical_season_dict).astype(str) +\n",
    "    \" el dia \" + df['sampling_date'].dt.day.astype(str) +\n",
    "    \" del mes \" + df['sampling_date'].dt.month_name(locale='es_ES.utf8') +\n",
    "    \" del año \" + df['sampling_date'].dt.year.astype(str) +\n",
    "    \" se encontro que la trampa \" + df['tramp_id'].astype(str) +\n",
    "    \" capturo \" + df['capture_count'].astype(str) +\n",
    "    \" gorgojos del agave lo que se considera una infestacion \" +\n",
    "    df['severity_encoded'].map(severity_dict).astype(str)\n",
    ")\n",
    "\n",
    "df['text_feature_plantation'] = (\n",
    "    \"En una plantacion de agave azul con area de \" + df['square_area_imputed'].astype(str) +\n",
    "    \" hectareas (\" + df['imputation_method'].map(area_information_dict).astype(str) + \"), \" +\n",
    "    df['plantation_age'].astype(str) + \" años de edad se capturaron \" +\n",
    "    df['capture_count'].astype(str) + \" gorgojos del agave lo que se considera una infestacion \" +\n",
    "    df['severity_encoded'].map(severity_dict).astype(str)\n",
    ")\n",
    "\n",
    "df['text_feature_all_things'] = (\n",
    "    \"En una plantacion de agave azul con area de \" + df['square_area_imputed'].astype(str) +\n",
    "    \" hectareas (\" + df['imputation_method'].map(area_information_dict).astype(str) + \n",
    "    \") y \" + df['plantation_age'].astype(str) + \" años de edad desde la ultima jima se capturaron \" +\n",
    "    df['capture_count'].astype(str) + \" gorgojos del agave lo que se considera una infestacion \" +\n",
    "    df['severity_encoded'].map(severity_dict).astype(str) + \". La trampa \" + df['tramp_id'].astype(str) +\n",
    "    \" estaba ubicada en latitud \" + df['lat'].astype(str) +\n",
    "    \" y longitud \" + df['lon'].astype(str) +\n",
    "    \" en el dia \" + df['sampling_date'].dt.day.astype(str) +\n",
    "    \" del mes \" + df['sampling_date'].dt.month_name(locale='es_ES.utf8') +\n",
    "    \" del año \" + df['sampling_date'].dt.year.astype(str) +\n",
    "    \", jurisdiccion de la municipalidad de \" + df['municipality'].astype(str) +\n",
    "    \" del Estado de \" + df['state'].astype(str) +\n",
    "    \". Tambien se tenia al foco de infestacion severa mas cercano a \" +\n",
    "    df['distance_to_nearest_hotspot'].astype(str) +\n",
    "    \" kilometros, con \" + df['hotspots_within_1km'].astype(str) +\n",
    "    \" focos severos a 1 kilometro de distancia y \" +\n",
    "    df['hotspots_within_5km'].astype(str) +\n",
    "    \" focos severos de infestacion a 5 kilometros de distancia.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f939634",
   "metadata": {},
   "source": [
    "# Métodos de filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe7968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import (\n",
    "    VarianceThreshold,\n",
    "    chi2,\n",
    "    f_classif,\n",
    "    SelectKBest,\n",
    "    mutual_info_classif\n",
    ")\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd5b277",
   "metadata": {},
   "source": [
    "### Features numéricos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f9f86",
   "metadata": {},
   "source": [
    "Revisaremos la varianza de cada una de las variables numéricas finales, esto porque:\n",
    "\n",
    "- Si tienen una varianza bastante baja (menor a 0.01 por ejemplo), entonces es probable que ese feature no nos aporte valor en nuestro dataset.\n",
    "- Para este primer análisis excluiremos a `distance_to_nearest_hotspot` por las siguientes razones:\n",
    "    - Al calcular este valor, se definió la cantidad de 9999 cuando no se encontrara algún hotspot cercano en los 15 días previos y posteriores al muestreo de la trampa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb287cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [ 'lat', 'lon', 'plantation_age', 'capture_count', 'square_area_imputed', 'hotspots_within_1km', 'hotspots_within_5km']\n",
    "\n",
    "variance_results = []\n",
    "for col in numerical_features:\n",
    "    variance_results.append( df[col].var() )\n",
    "    \n",
    "distance_to_neares_hotspot_mask = df['distance_to_nearest_hotspot'] < 9999\n",
    "variance_results.append(df[distance_to_neares_hotspot_mask]['distance_to_nearest_hotspot'].var())\n",
    "\n",
    "# Imprimimos todo junto\n",
    "numerical_features = [ 'lat', 'lon', 'plantation_age', 'capture_count', 'square_area_imputed', 'hotspots_within_1km', 'hotspots_within_5km', 'distance_to_nearest_hotspot']\n",
    "for col, v_result in zip(numerical_features, variance_results ):\n",
    "    print(f\"{col} tiene una varianza de {v_result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1524557",
   "metadata": {},
   "source": [
    "También revisaremos si existe alguna correlación entre éstas para evitar la multicolinearidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "numerical_features = [ 'lat', 'lon', 'plantation_age', 'capture_count', 'square_area_imputed', 'hotspots_within_1km', 'hotspots_within_5km']\n",
    "\n",
    "# Calculamos la matriz de correlación\n",
    "corr_matrix = df[numerical_features].corr()\n",
    "\n",
    "# Configuramos el tamaño del gráfico\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Generamos el mapa de calor\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,          \n",
    "    fmt=\".2f\",           \n",
    "    cmap='coolwarm',     \n",
    "    linewidths=0.5,      \n",
    "    square=True          \n",
    ")\n",
    "\n",
    "# Título del gráfico\n",
    "plt.title(\"Correlaciones entre variables numéricas\", fontsize=14, pad=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1eb7f5",
   "metadata": {},
   "source": [
    "Para `distance_to_nearest_hotspot` tomaremos en cuenta solamente los valores menores a 9999, ya  que este valor era el default para indicar que no  se encontró alguna trampa o punto catalogado como severo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee5e08",
   "metadata": {},
   "source": [
    "Y notamos que las correlaciones más fuertes se encuentran entre estas variables:\n",
    "\n",
    "- `hotspots_within_5km` & `hotspots_within_1km`, que no es una correlación fuerte (porque no llega a 0.70 o -0.70), pero tiene sentido que se dé un valor cercano debido a que las trampas o puntos de infestación severa contenidos en 1km también se encuentran contenidos en 5km. Por lo tanto, entre más puntos severos de infestación existan a 1km, es probable que el número de focos de infestación severos a 5km crezca también.    \n",
    "- `latitud` y `longitud`, los cuales son parte de la ubicación donde se llevó  a cabo la muestra y muestran una correlación de 0.39. Esto no es algo de notar ya que conocemos que todas las muestras han sido tomadas en 5 estados definidos.\n",
    "\n",
    "Entonces, con lo anterior, consideramos que podemos eliminar la columna `hotspots_within_1km`, ya que la información de éstas trampas se encuentra contenida en `hotspots_within_5km` y su varianza es bastante menor que la varianza de `hotspots_within_5km`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29761d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels = ['hotspots_within_1km'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d0a4f",
   "metadata": {},
   "source": [
    "### Features categóricos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3313c3c4",
   "metadata": {},
   "source": [
    "#### Chi Cuadrado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e48a5",
   "metadata": {},
   "source": [
    "Iniciaremos con una prueba Chi Cuadrado para nuestas variables categóricas. Usaremos un `p = 0.05` para definir si existe una asociación significativa entre nuestras variables, y en el caso de no haberla estaremos eliminando dichos features.\n",
    "\n",
    "Por lo anterior, definimos que la `severidad` (`severity_encode`) es la variable categórica más importante, y por lo tanto nos interesa saber si tiene alguna relación con nuestras demás variables categóricas. Por lo tanto, haremos nuestro test `Chi^2` con base en esta. \n",
    "\n",
    "Algo de notar es que tenemos algunas variables categóricas las cuales solo nos sirvieron para calcular otros features. Estas variables las usaremos como  de control, y son las siguientes:\n",
    "  - `imputation_method`\n",
    "  - `is_hotspot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af5f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "categorical_features = [  'state', 'municipality', 'imputation_method', 'critical_season', 'is_hotspot', 'severity', ]\n",
    "target = 'severity_encoded'\n",
    "\n",
    "for col in categorical_features:\n",
    "    print('=' * 70)\n",
    "    print(f'Prueba Chi Cuadrado para {col} y {target}')\n",
    "    print('=' * 70)\n",
    "    \n",
    "    contingency_table = pd.crosstab( df[col], df[target] )\n",
    "    \n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    print(f\"Chi2 Statistic: {chi2}\")\n",
    "    print(f\"Grados de libertad: {dof}\")\n",
    "    print(f\"Valor-p: {p}\")\n",
    "    \n",
    "    if p < 0.05:\n",
    "        print(\"✅ Existe relación significativa con base a valor-p = 0.05\")\n",
    "    else:\n",
    "        print(\"❌ No existe relación significativa con base a valor-p = 0.05\")\n",
    "        \n",
    "    print('=' * 70)    \n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0418576c",
   "metadata": {},
   "source": [
    "Los resultados anteriores nos muestran lo siguiente:\n",
    "\n",
    "- Todas nuestras variables categóricas presentan una relación significtiva. Esto es señal de que necesitamos otro tipo de prueba para verificar si realmente podemos eliminar o no algunos features. \n",
    "- `state` tiene una relación significativa con `severity_encode`, lo que tiene sentido ya que la severidad entre estados puede variar.\n",
    "- Respecto a la variable `municipality`, al tener una cardinalidad alta, no se justifica su permanencia solo con base a `Chi^2`. Esto lo deducimos por el valor de `Chi^2` obtenido, el cual es demasiado alto.\n",
    "- `imputation_method` parece tener una relación significativa con `severity_code`, lo que nos llama la atención porque esta variable fue creada a partir de rellenar las áreas faltantes de cada predio. Esto nos hace pensar de nuevo que necesitamos de una segunda prueba de correlación entre nuestras variables categóricas.\n",
    "- `critical_season` demuestra un efecto moderado sobre `severity_encode`, lo que tiene sentido debido a que existen meses que se consideran críticos en las plantaciones de agave.\n",
    "- `is_hotspot` y `severity` tienen un valores elevadísimos debido a que `severity_encode` es una derivación de éstas columnas. Estas fueron nuestras variables de control, y nos hace pensar que nuestra implementación se encuentra hecha correctamente.\n",
    "\n",
    "En resumen, esta prueba nos indica que las relaciones entre nuestras variables categóricas no es aleatoria, y aunque esta prueba nos indica que tenemos una relación significativa entre nuestras variables categóricas, probablemente no sea una relación relevante.\n",
    "\n",
    "Otro aspecto a tomar en cuenta es que Chi^2 es susceptible al tamaño de la muestra, y por lo tanto podríamos creer erróneamente que todas las variables anteriores son significativas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c074fec",
   "metadata": {},
   "source": [
    "#### Cramer's V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb0ac6",
   "metadata": {},
   "source": [
    "Para descartar completamente la correlación de nuestras variables categóricas, haremos uso de Cramer's V, lo cual puede darnos una idea de qué tan asociadas están las variables categóricas entre sí.\n",
    "\n",
    "Este indicador nos terminará de dar una idea de qué variables podemos desechar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f807d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    \n",
    "    # Calculamos la tabla de contingencia\n",
    "    contingency_table = pd.crosstab(x, y)\n",
    "    \n",
    "    # Calculamos Chi^2\n",
    "    chi2, _, _, _ = chi2_contingency(contingency_table)\n",
    "    n = contingency_table.sum().sum()\n",
    "    \n",
    "    min_dim = min(contingency_table.shape) - 1\n",
    "    \n",
    "    # Debemos evitar la division por 0\n",
    "    if min_dim == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # Retornamos el indicador de Cramer\n",
    "    return np.sqrt(chi2 / (n * min_dim))\n",
    "\n",
    "                   \n",
    "categorical_features = ['state', 'municipality', 'severity', 'imputation_method', 'critical_season', 'is_hotspot']\n",
    "target = 'severity_encoded'\n",
    "\n",
    "cramers_results = []\n",
    "\n",
    "for feature in categorical_features:\n",
    "    v = cramers_v(df[feature], df[target])\n",
    "    cramers_results.append(v)\n",
    "    \n",
    "for feature, result in zip(categorical_features, cramers_results):\n",
    "    print(f\"{feature} tiene un indice de Cramer de {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babfa3cd",
   "metadata": {},
   "source": [
    "- Con esto, nos damos cuenta de que variables o features como `imputation_method` tienen una asociación débil con la severidad. Esto era de esperarse ya que esta contiene solo \n",
    "- `state` y `municipality` tienen una asociación débil con la severidad también.\n",
    "- `critical_season` muestra una relación débil con la severidad. Esto parece interesante porque, probablemente, la definición de meses críticos del manual operativo debe cambiar.\n",
    "- `severity` tiene un indicador de Cramer's V perfecto, lo que es de esperarse. Por lo tanto, podremos eliminar esta columna.\n",
    "\n",
    "Ahora bien, eliminaremos las siguientes variables de nuestro DataFrame:\n",
    "\n",
    "- `imputation_method`, porque ya la utilizamos para dar contexto en los `text_features`.\n",
    "- `severity`, que es una variable de control, la podemos eliminar ya que contamos con `severity_encoded`.\n",
    "- `is_hotspot` se puede eliminar también, ya que se deriva de `severity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels=['severity', 'is_hotspot', 'imputation_method'], inplace=True, axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b9b0a",
   "metadata": {},
   "source": [
    "# Columnas o features finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c5e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7b636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ac9568",
   "metadata": {},
   "source": [
    "Con las acciones anteriores, es posible confirmar que nuestro dataframe contaba con 9 columnas originalmente, las cuales se expandieron a 30 columnas finales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0fbd4",
   "metadata": {},
   "source": [
    "# Conlusiones específicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d387e1",
   "metadata": {},
   "source": [
    "#### Sobre `imputation_method` y `square_area_imputed`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080084a2",
   "metadata": {},
   "source": [
    "- El algoritmo `RadiusNeighborsRegressor` demostró ser la mejor opción en cuanto a la forma de insertar valores para tomar en cuenta el área de aquellos predios donde no se registró correctamente ésta. Esto podría deberse a que el rendimiento de este modelo depende directamente del radio a utilziar para encontrar los puntos vecinos al que se está analizando. Con el uso de `Haversine` como métrica el rendimiento del modelo funcionó de mejor forma, en gran parte porque esta métrica toma en cuenta la curvatura de la tierra. Por lo tanto, se recomienda que para detectar vecinos cercanos respecto a la ubicación de un punto se utilice este tipo de métrica.\n",
    "  - En nuestro caso, utilizamos un radio de 500 metros, ya que la cantidad máxima entre trampas no  está definida en el manual de operaciones cuando se da el proceso de jima o de monitoreo. Con esto, la única distancia definida es la distancia mínima entre trampas, por lo que decidimos usar un factor de 5 para detectar trampas \"cercanas\".\n",
    "\n",
    "<br>\n",
    "\n",
    "- El método `KNighborsRegressor`, aunque demostró ser más rápido y no presentaba valores `NaN` en las  predicciones finales, no fue un modelo adecuado para insertar valores en nuestro dataset. Esto se comprobó por medio del `R^2` obtenido al llevar a cabo el entrenamiento por medio de distintos valores de `K`. Dichos valores se seleccionaron pensando en las condiciones operativas definidas por el *Manual operativo de la campaña contra plagas reglamentadas del agave*.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Como el modelo `RadiusNeighborsRegressor` no encontró puntos cercanos en `8358` de los casos analizados (aproximadamente el 1% de registros), decidimos utilizar la mediana de los datos ya conocidos e insertados. Esto se debió a que la mediana tiende a no ser tan afectada por los outliers, y a que ésta fue mejor opción que utilizar los modelos `KNeighborsRegressor` que desarrollamos.\n",
    "\n",
    "<br>\n",
    "\n",
    "- La variable `imputation_method`, aunque se descartó en el dataset final, se utilizó para dar más contexto al LLM que utilizaremos. Este contexto se le dará por medio del feature `text_feature_location` y `text_feature_all_things`.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Otra posible opción para insertar valores en el área de los predios donde la misma no se documentó correctamente es hacer uso de fotografías satélitales históricas (usando una fuente como `SentinelHub`) junto al fine-tuning de modelos como `DinoV3`. La idea de esta opción es detectar la cantidad de plantas de agave en las ubicaciones de las trampas donde no se documentó el valor del área superficial correcta en la fecha que se está muestreando o una cerca, para luego estimar el área del predio con base en la presencia de éstas plantas.\n",
    "  - Un aspecto importante a tomar en cuenta para esta recomendación es la resolución en metros por pixel. Con esto, por ejemplo, SentinelHub maneja resoluciones de 0.5m, 10m, 20 y 60m por pixel, pero el poder de procesamiento y cantidad de memoria que se necesita puede llegar a ser astronómica. \n",
    "  - También es necesario tomar en cuenta que las imágenes satelitales pueden tener desafíos como las nubes o fenómenos atmosféricos diversos que puedan dificultar el cálculo del área estimada.\n",
    "  - Esta ide fue proporcionada por el Dr. Gerardo Jesús Camacho.\n",
    "  - Una alternativa a esta opción podría ser lo presentado en el paper `Bit-STED: A lightweight transformer for accurate agave counting with UAV imagery`, el cual es un proyecto donde se desarrolló una técnica para reconocer plantas de agave por medio de fotografías tomadas por un dron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd8e86",
   "metadata": {},
   "source": [
    "#### Sobre los `features temporales`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f96ca0",
   "metadata": {},
   "source": [
    "- Se crearon referencias cíclicas para futuras iteraciones del proyecto. La idea es que la referencia cíclica permite a los modelos de ML relacionar que, por ejemplo, diciembre y enero no están lejos uno del otro, lo cual no es tan fácil de implementar haciendo uso  de relaciones lineales. \n",
    "\n",
    "- Es posible capturar tendencias con estacionalidad por medio de este tipo de variables, por si en el futuro es posible desarrollar algun algoritmo de regresión o clasificación. Una idea a trabajar para el futuro es si es posible predecir la severidad de los focos de infestación para coordinar brigadas de prevención acorde a ésto.\n",
    "\n",
    "- Para la implementación de un ChatBot o uso de LLM, estas columnas cíclicas deben convertirse a valores usables por los mismos. Por lo tanto, recomendamos que se puedan llevar a cabo tareas de agrupamiento o métricas conocidas como de tipo `Rolling Window`. Este tipo de features tienen su base en que ayudan a encontrar relaciones o patrones en data que sea de tipo secuencial en el tiempo. Incluso es posible crear features que puedan ayudar a medir la volatilidad de un fenómeno.\n",
    "  - Este tipo de features podrían funcionar bastante bien en conjunto con datos climáticos como la temperatura y precipitación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70bfac",
   "metadata": {},
   "source": [
    "#### Sobre las `características espaciales`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea57b9a",
   "metadata": {},
   "source": [
    "- Es importante relacionar las trampas con otras trampas cercanas, ya que esta información será importante para nuestro ChatBot o LLM a hacerle fine-tuning. Este tipo de columnas permite que el ChatBot contextualice situaciones de riesgo con base en la distancia hacia focos de infestación severos. \n",
    "\n",
    "- Los features `distance_to_nearest_hotspot` y `hotspots_within_5km` apoyan en capturar el riesgo local de los municipios o estados incluidos en nuestro dataset. Esto se debe a que la distancia es una forma de gradiente o métrica que, entre menor sea  respecto a un foco de infestación severo, más probable es que el riesgo de infestación del punto de referencia aumente. Con esto, es importante mencionar también que este tipo de dato aporta más información que una etiqueta de Municipio o Estado.\n",
    "\n",
    "- El cálculo de la distancia hacia las trampas catalagodas como con una infestación severa se debe hacer por medio de Haversine. En nuestro caso, utilizamos estructuras como `cKDTree` y `RadiusNeighborsRegressor`, los cuales por medio de la métrica de Haversine y, conociendo el radio de la tierra, es posible calcular de mejor manera la distancia entre los puntos. Este tipo de métrica permitió generar un mejor contexto para nuestro futuro ChatBot, ya que es posible generar contexto como \"a 0.5 kilometros de distancia se encuentra un foco severo de infestación\", lo que semánticamente es más valioso para un LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0692b3bd",
   "metadata": {},
   "source": [
    "#### Sobre `Estado` y `Municipalidad`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6043b",
   "metadata": {},
   "source": [
    "- `municipaliy` y `state` muestran una relación baja respecto a la clasificación de los focos de infestación que se consideran como severos. Por lo tanto, estas variables funcionan de mejor manera para:\n",
    "  - Estratificar o agrupar nuetros datos.\n",
    "  - Dar información a nuestro LLM como una forma de dato contextual para un sistema RAG. La idea es que por medio de este contexto, el LLM pueda generar mejores respuestas que, aparte de utilizar solamente información geoespacial, pueda incluir información agrupaciones por municipio o Estado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef14c94",
   "metadata": {},
   "source": [
    "### Sobre `text_feature_...`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c213293",
   "metadata": {},
   "source": [
    "- Los `text_feature` que se generaron a partir de data tabular podrían aumentan la calidad explicativa del LLM que implementaremos. En resumen, al generar este tipo de texto para un LLM, será posible hacer el fine-tuning del mismo y generar los `text_embeddings` necesarios para esto. La idea general es que nuestro ChatBot pueda relacionar de mejor manera los datos presentados y que pueda dar respuestas coherentes.\n",
    "\n",
    "- El sistema RAG a implementar se ve beneficiado con este tipo de features ya que será posible recuperar información por municipalidad, estado, cantidad de capturas, ID de trampa, temporada o proximidad de focos de infestación severos. Lo importante de este aspecto que se logró convertir la data tabular a texto uniforme, lo que le dará estructura y ayuda a reducir el ruido que recibe el LLM al ser resúmenes compactos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32647969",
   "metadata": {},
   "source": [
    "# Conclusiones generales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b50bac",
   "metadata": {},
   "source": [
    "- El feature engineering desarrollado cumple el propósito operativo del proyecto: habilitar un dataset para el fine-tuning de un chatbot capaz de recibir reportes georreferenciados, contextualizarlos con el histórico validado y emitir alertas cercanas para priorizar acciones cuando se le solicite. El EDA confirmó estacionalidad marcada, concentraciones geográficas específicas y atipicidades en los focos severos, lo que justificó una representación que combina tiempo, espacio y semántica para preservar señal predictiva y facilitar explicabilidad en producción.\n",
    "\n",
    "- Las pruebas de asociación (Chi-cuadrado / V de Cramer) para respaldar la selección de features categóricas vinculadas a la severidad soportan decisiones de inclusión/exclusión con evidencia y evita sobreajuste.\n",
    "\n",
    "- Se llevó a cabo la ingeniería de texto orientada a LLM por medio de la consolidación de campos (ubicación, riesgo, captura, rasgos del predio, etc.) en representaciones textuales normalizadas. Esto habilita tanto fine-tuning como RAG y mejora la recuperación semántica durante la inferencia del chatbot.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "*¿Qué nos deja listos para producción?*\n",
    "\n",
    "- Un dataset explicable (cada feature mapea a un fenómeno observado: estacionalidad, concentración espacial, severidad atípica).\n",
    "- Un esquema compatible con LLMs (texto normalizado + índice semántico) y con modelos clásicos (features numéricas bien comportadas).\n",
    "- Una base operable para el chatbot: puede citar evidencia histórica, explicar por qué un foco se prioriza (vecindad/temporada/severidad) y alertar en tiempo y forma.\n",
    "\n",
    "\n",
    "*Siguientes pasos:*\n",
    "\n",
    "- Validar relevancia de categóricas con Information Gain / Mutual Information y reconfirmar exclusiones.\n",
    "- Afinar ventanas temporales con umbrales específicos de temporada (p. ej., lluvias) y evaluar lags semanales.\n",
    "- Probar variantes de densidad espacial (DBSCAN/HDBSCAN) como features adicionales y comparar contra Radius/KNN.\n",
    "- Construir y evaluar el índice de embeddings a partir de la columna compuesta; generar pares instrucción-respuesta para un fine-tuning ligero del chatbot.\n",
    "- Monitorear sesgo de muestreo por periodos de baja captura y aplicar ponderadores o estrategias de validación por bloques temporales. En suma, la estrategia de feature engineering aterriza el contexto del problema en variables temporales, espaciales y semánticas que el modelo puede aprender y explicar, maximizando utilidad operativa del chatbot sin sacrificar rigor técnico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f0ac38",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "\n",
    "- Amazon Web Services. (s.f.). What is RAG? - Retrieval-augmented generation explained. https://aws.amazon.com/what-is/retrieval-augmented-generation/\n",
    "- Cuervo-Parra, J. A., Romero-Contreras, Y. J., Ramirez-Suero, M., Lopez-Pacheco, I. Y., Loera-Corral, O., Saucedo-Castañeda, G., & Ramirez-Lepe, M. (2019). Insect pests of the Agave tequilana Weber var. azul. Southwest Entomologist, 44(3), 563–576.\n",
    "- Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 4171–4186.\n",
    "- Géron, A. (2019). Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow (2nd ed.). O'Reilly Media.\n",
    "- Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: Data mining, inference, and prediction (2nd ed.). Springer.\n",
    "- Koloski, B., Pollak, S., Škrlj, B., & Martinc, M. (2025). LLM embeddings for deep learning on tabular data. arXiv preprint arXiv:2502.11596. https://arxiv.org/abs/2502.11596\n",
    "- Kuhn, M., & Johnson, K. (2013). Applied predictive modeling. Springer.\n",
    "- Servicio Nacional de Sanidad, Inocuidad y Calidad Agroalimentaria. (2017). Manual operativo de la campaña contra plagas reglamentadas del agave (Versión 4, Clave MOP-DPF-PRAV). Dirección de Protección Fitosanitaria, Secretaría de Agricultura, Ganadería, Desarrollo Rural, Pesca y Alimentación.\n",
    "- Sentinel Hub. (s.f.). Sentinel-2 L1C: Available bands and data. Retrieved October 5, 2025, from https://docs.sentinel-hub.com/api/latest/data/sentinel-2-l1c/#available-bands-and-data\n",
    "- Villatoro-Geronimo, D., Sanchez-Ante, G., & Falcon-Morales, L. E. (2025). Bit-STED: A lightweight transformer for accurate agave counting with UAV imagery. Computers and Electronics in Agriculture, 228, Article \n",
    "    109536. https://doi.org/10.1016/j.compag.2024.109536"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
