{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817e56ec",
   "metadata": {},
   "source": [
    "# Reporte de problemas fitosanitarios en plantaciones de agave\n",
    "--------------------\n",
    "\n",
    "## Equipo 36\n",
    "\n",
    "| Nombre | Matrícula |\n",
    "| ------ | --------- |\n",
    "| André Martins Cordebello | A00572928 |\n",
    "| Enrique Eduardo Solís Da Costa | A00572678 |\n",
    "| Delbert Francisco Custodio Vargas | A01795613 |\n",
    "\n",
    "## Avance 3: baseline\n",
    "\n",
    "### Instrucciones\n",
    "Este avance consiste en construir un modelo de referencia que permita evaluar la viabilidad de la solución plantada. Si el baseline tiene un rendimiento similar al azar, podría indicar que el problema es intrínsecamente difícil o que los datos no contienen suficiente información para predecir el objetivo. De lo contrario, el baseline podría funcionar como una solución mínima aceptable cuando se trabaja en escenarios donde incluso un modelo simple puede proporcionar valor.\n",
    "\n",
    "Un baseline facilita también la gestión de expectativas, tanto dentro del equipo como con los stakeholders, pues proporciona una comprensión inicial de lo que se puede lograr con métodos simples antes de invertir tiempo y recursos en enfoques más complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a98ac0",
   "metadata": {},
   "source": [
    "# ChatBot para reporte de infestaciones fitosanitarias en plantaciones de Agave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077ac607",
   "metadata": {},
   "source": [
    "Para conectar nuestro LLM entrenado sobre una porción de datos de nuestros `text_features`, es necesario crear una conexión hacia WhatsApp.\n",
    "\n",
    "Respecto a la conexión hacia WhatsApp, es importante mencionar que Meta maneja criterios bastante estrictos para poder hacer un uso directo de su API. Estos criterios se pueden resumir en lo siguiente:\n",
    "\n",
    "- Se debe contar con una cuenta de Meta Business y ser un integrador verificado. Este proceso puede demorar semanas y por lo tanto, no es buena idea llevarlo a cabo para el prototipo.\n",
    "- Luego de estar verificado, se pueden hacer uso de distintas formas de mensajes:\n",
    "  - Freeform Messages: son aquellos que permiten enviar y recibir texto libre de parte de los usuarios.\n",
    "  - Template Messages: son los mensajes que permiten enviar opciones predefinidas hacia un usuario. Estos deben ser aprobados por Meta.\n",
    "  - Auth Messages: son notificaciones para enviar códigos para iniciar sesión de 2 Factores, o parecido.\n",
    "\n",
    "Con lo anterior, debido a que no contamos con el tiempo suficiente para llevar a cabo un registro, pago y posterior entrevista de verificación de Meta, en esta entrega haremos uso de algunos modelos LLM locales para comparar las respuestas y seleccionar el modelo que nos servirá de base (Baseline) para llevar a cabo el fine-tuning del mismo.\n",
    "\n",
    "Con esto, haremos pruebas con los siguientes modelos en este Notebook:\n",
    "\n",
    "- `Mistral-7B`\n",
    "- `Llama-3-8B`\n",
    "- `Microsoft-Phi-3`\n",
    "\n",
    "Por último, como cada modelo abarca casi por completo la memoria RAM de nuestra GPU, es necesario que durante el run de nuestra Baseline se libere esta memoria para asegurar que la Notebook pueda funcionar correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd1accd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import Field\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from typing import Optional, List, Any\n",
    "from langchain.schema import Document\n",
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0f2f5",
   "metadata": {},
   "source": [
    "## Comparación de LLMs sin fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21030512",
   "metadata": {},
   "source": [
    "Para esta sección es importante mencionar la obteción de estos modelos:\n",
    "\n",
    "- Primero, es necesario contar con una cuenta de HuggingFace.\n",
    "- Segundo, se debe contar con el `auth_token` generado de HuggingFace para acceder a la descarga de estos modelos.\n",
    "- Tercero, se puede hacer uso del script `download_llm.py` para descargar los modelos que se quieran recrear en este Notebook.\n",
    "- Cuarto, es posible revisar el archivo `requirements.txt` para instalar las librerías necesarias con las versiones compatibles para que este código funcione correctamente.\n",
    "- Quinto, por medio de Gradio es posible simular o ver una interfaz `Query-Result`, de forma que podamos evaluar la forma de contestar de cada modelo. Esto lo usaremos con el modelo que mejor responda en español (esta selección será subjetiva).\n",
    "\n",
    "Con esto,  los prompts que usaremos son los siguientes:\n",
    "\n",
    "- `Describe al gorgojo del agave`\n",
    "- `Describe la planta de agave azul`\n",
    "- `Describe como es un predio de agave azul y que riesgos presenta`\n",
    "\n",
    "Bajo las siguientes condiciones:\n",
    "\n",
    "- Que responda en una cantidad máxima de 128 tokens\n",
    "- La cantidad mínima de tokens a generar será de 64\n",
    "\n",
    "La idea es encontrar algún modelo que presente la mejor respuesta en español, pero también debemos tomar en cuenta el apartado de memoria y rendimiento. Al trabajar con LLMs, la VRAM de nuestra GPU debe ser lo suficientemente grande para soportar correrlo, ya que de lo contrario corremos el riesgo de procesar las respuestas a nuestros prompt desde el CPU. Esto último es *demasiado lento* como para considerar trabajar desde este componente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8749e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"Describe al gorgojo del agave brevemente\"\n",
    "prompt_2 = \"Describe la planta de agave azul brevemente\"\n",
    "prompt_3 = \"Describe como es un predio de agave azul y cuales riesgos presenta brevemente\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d76209",
   "metadata": {},
   "source": [
    "### `Mistral-7B-Instruct-v0.3`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c51c5",
   "metadata": {},
   "source": [
    "Algo importante sobre `Mistral-7B-Instruct-v0.3` es que se requiere de al menos 14GB de memoria dedicada en la GPU. Esto, para nosotros en este momento, no es lograble a menos de que se use un servicio como Google Colab Pro.\n",
    "\n",
    "Por lo tanto, hemos decidido aplicar `quantization` a este modelo, que en resumen es \"hacer más pequeño\" este LLM. El trade-off es que perderemos capacidad de mantener el contexto, pero tendremos respuestas más rápidas.\n",
    "\n",
    "Para dar un ejemplo, al usar el modelo `Mistral-7B` sin quantization, debíamos esperar entre 20 a 30 minutos por respuesta de cada prompt. Al usar `quantization` el tiempo de espera se redujo a segundos (incluso menos de 1 minuto por respuesta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5ca027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:18<00:00, 26.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# Ruta local del modelo\n",
    "model_path = \"D:/LLM Models/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Aplicamos quantization para hacer el modelo más pequeño\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Carga del modelo y tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a8725",
   "metadata": {},
   "source": [
    "Procedemos a crear la `pipeline()` y a verificar que esté corriendo el modelo en GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd53334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Usamos pipeline() para integrar el tokenizer, modelo y el task o tarea\n",
    "# que debe llevar a cabo nuestro bot.\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    min_new_tokens=64,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e3618a",
   "metadata": {},
   "source": [
    "#### Respuestas de `Mistral-7B-Instruct-V0.3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a106e229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe al gorgojo del agave brevemente.\n",
      "\n",
      "El algorojo del agave, también conocido como cochineal rojo o insecto carminativo, es una especie de insecto escarabajo que se encuentra en México y América Central. Es un pequeño escarabajo que vive en las plantas del agave. Los adultos son de color rojo brillante y tienen un cuerpo cubierto de pelos. Se utilizan sus escalas para obtener el colorante natural rojo denominado carminio, que se utiliza en la industria alimentaria, cosmética y\n"
     ]
    }
   ],
   "source": [
    "response = pipe(prompt_1)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95f18882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe la planta de agave azul brevemente.\n",
      "La planta de agave azul, científicamente conocida como Agave tequilana Weber azul, es una especie de agave nativa de México que se cultiva principalmente en los estados de Jalisco, Nayarit y Tamaulipas. Es una agave robusta con hojas azules-verdosas de hasta 1,2 metros de largo y 7 centímetros de ancho, con bordes fuertemente espinosos. La planta produce una flor de color amarillo\n"
     ]
    }
   ],
   "source": [
    "response = pipe(prompt_2)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0423f7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe como es un predio de agave azul y cuales riesgos presenta brevemente.\n",
      "\n",
      "Un predio de agave azul, comúnmente conocido como mezcalero, se encuentra en regiones secas y desérticas de México, especialmente en Oaxaca, Guerrero, y Durango. Los campos de agaves azules son caracterizados por sus plantaciones bien separadas con espacios abiertos para permitir el crecimiento de la planta. Las agaves azules pueden alcanzar una edad de hasta 28 años antes de ser recolectadas para fabricar el mezcal.\n",
      "\n",
      "Sin\n"
     ]
    }
   ],
   "source": [
    "response = pipe(prompt_3)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db239a",
   "metadata": {},
   "source": [
    "#### Liberacion de memoria RAM de nuestra GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650afe72",
   "metadata": {},
   "source": [
    "Como solo contamos con 8GB de RAM en la GPU actual, es necesario liberar la misma. Como estamos usando torch + CUDA, es necesario mover el modelo al CPU y eliminarlo de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd7eaee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liberamos la memoria de nuestra GPU para evitar un overflow por mantener más de \n",
    "# un LLM en memoria\n",
    "\n",
    "model.cpu() # Movemos el modelo al CPU\n",
    "del model   # Eliminamos el modelo de memmoria\n",
    "\n",
    "# Limpiamos la cache de Torch para evitar problemas\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b6bad",
   "metadata": {},
   "source": [
    "### `meta-llamaMeta-Llama-3-8B`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad19edb",
   "metadata": {},
   "source": [
    "Usaremos el LLM desarrollado por Meta también. Este se caracteriza por contar con 8 mil millones de parámetros. Por lo tanto, debemos cuantizarlo también para evitar un overflow en la memoria de nuestra GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59c3b058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:25<00:00, 21.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# Ruta local del modelo\n",
    "model_path = \"D:/LLM Models/meta-llamaMeta-Llama-3-8B\"\n",
    "\n",
    "# Aplicamos quantization para hacer el modelo más pequeño\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Carga del modelo y tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "701f45b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    min_new_tokens=64,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4820696a",
   "metadata": {},
   "source": [
    "#### Respuestas de `meta-llamaMeta-Llama-3-8B`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a90cbc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe al gorgojo del agave brevemente en espanol\n",
      "The Agave is a type of succulent plant, that is, it stores water in its leaves. The species grows in tropical and subtropical regions of the world.\n",
      "These plants are found mainly in Mexico, where they are used to produce tequila and mezcal.\n",
      "In addition to these alcoholic beverages, the agave fibers are used as ropes or for making clothing, among other things. It also has medicinal properties and can be eaten raw or cooked.\n",
      "We know that there are several species of agave plants. Among them we find the Agave americana (commonly known as \"century plant\"), which can\n"
     ]
    }
   ],
   "source": [
    "response = pipe(prompt_1)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0ad8977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe la planta de agave azul brevemente. The Blue Agave Plant, or Agave Tequilana, is a succulent plant that has blueish green leaves with sharp spines on the edges of its leaves. It is native to Mexico and is grown in many other countries as well.\n",
      "The plant can grow up to 5 feet tall and has a long lifespan of about 10 years. The agave plant produces flowers only once during its lifetime which then produce seeds that fall off the plant after flowering. These seeds germinate into new plants called pups which are smaller versions of their parent plants but still capable of growing into adult plants themselves if they receive enough water and nutrients\n"
     ]
    }
   ],
   "source": [
    "response = pipe(prompt_2)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cfcb532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe como es un predio de agave azul y cuales riesgos presenta brevemente.\n",
      "The project is located in the north of Mexico, in a town called Tepehuanes. It is an area with an altitude of 1000 meters above sea level and it is surrounded by mountains that are very high. The region is characterized by being arid because there is little rain and most of the precipitation occurs during summer. This has caused that the vegetation of this region consists mainly of shrubs like sagebrush and cactus. There is also a large amount of trees such as pine, oak and cedar. In addition to these plants, there is a plant called Agave Azul (Agave tequilana)\n"
     ]
    }
   ],
   "source": [
    "response = pipe(prompt_3)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuevamente eliminamos el modelo de la memoria y de la cache para hacer espacio al siguiente LLM\n",
    "\n",
    "model.cpu()\n",
    "del model\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097fc21",
   "metadata": {},
   "source": [
    "### `microsoft-phi-3-mini-4k-instruct`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5353bc",
   "metadata": {},
   "source": [
    "Este modelo, que se considera como lightweight, fue posible cuantizarlo a una resolución mayor (8Bits). Por lo tanto, haremos uso del cuantizado en 8bits para llevar a cabo la carga del modelo ya que de esta forma se reducen menos las cualidades del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18a4bd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 20.90s/it]\n"
     ]
    }
   ],
   "source": [
    "# Ruta local del modelo\n",
    "model_path = \"D:/LLM Models/microsoft-phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Aplicamos quantization para hacer el modelo más pequeño\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "# Carga del modelo y tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config, # Descomentar si se quiere cuantizar\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dba1035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    min_new_tokens=64,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf5d04f",
   "metadata": {},
   "source": [
    "#### Respuestas de `microsoft-phi-3-mini-4k-instruct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b9b890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe al gorgojo del agave brevemente. Gusano de la hoja (Dactylopius coccus). El gusano es un insecto escamoso que vive en las hojas y tallos de los agaves, especialmente el Agave americana; mide aproximadamente 4 mm x 2 mm. La larva se alimenta de células parenquimatosas produciendo una mancha amarilla o blanca característica a lo largo del tallo interno cuando crece demasiado grande para adaptarse dentro del tejido vegetal. Las plantas infectadas por este á\n"
     ]
    }
   ],
   "source": [
    "response = pipe(prompt_1)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95078b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe la planta de agave azul brevemente en 50 palabras.\n",
      "Agave americana 'Blue Glow' es una variedad ornamental con hojas grandes y coloridas que se utilizan a menudo como plantas decorativas debido a su belleza estética única, especialmente durante el otoño cuando las hojas tiñen un tono brillante azulado.\n"
     ]
    }
   ],
   "source": [
    "response = pipe(prompt_2)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63afefca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe como es un predio de agave azul y cuales riesgos presenta brevemente. La planta de agave Azul, conocida por su nombre científico Agave tequilana var. azul o A. salmiana, se cultiva principalmente en el estado mexicano de Jalisco para la producción del Tequila. Este predio debe cumplir con ciertas condiciones ambientales específicas que favorecen el crecimiento óptimo de esta especie:\n",
      "\n",
      "- Clima semiárido subtropical caracterizado por veranos calurosos e inviernos templados.\n",
      "- Suelo arenoso bien drenado a base\n"
     ]
    }
   ],
   "source": [
    "response = pipe(prompt_3)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60368c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "del model\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00059a0",
   "metadata": {},
   "source": [
    "### Selección de LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b218a5",
   "metadata": {},
   "source": [
    "| Modelo | Ventajas | Desventajas |\n",
    "| ------ | -------- | ----------- |\n",
    "| `Mistral-7B-Instruct-v0.3`  | Da respuestras bastante estructuradas y concisas. El modelo responde bien en Español. | Es un LLM pesado, ocupa aproximadamente 30GB de espacio en disco y debemos cuantizarlo en 4bits. Esto generó una reducción del 75% del modelo completo. |\n",
    "| `meta-llamaMeta-Llama-3-8B` | El modelo es más rápido que `Mistral-7B`. | Se debe cuantizar, y por lo tanto puede perder granularidad en sus respuestas o textos generados. Otra desventaja es que los prompts en Español los respondió en Inglés, lo que indica que el proceso de fine-tuning tendrá que involucrar demasiados textos en español| \n",
    "| `microsoft-phi-3-mini-4k-instruct` | Es más ligero que `Mistral-7B` y `LLama-3-8B`. Responde en español consisamente. | No es maneja un contexto tan especializado como `Mistral-7B` o `Llama-3-8B`. |\n",
    "\n",
    "\n",
    "Por lo tanto, tomando en cuenta los resultados anteriores, decidimos trabajar sobre el modelo `Microsoft-phi-3-mini-4k-instruct` por lo siguiente:\n",
    "\n",
    "- Es un modelo ligero que responde bien en español. \n",
    "- Sus respuestas cargaron más rápido que las de `Mistral-7B`, lo que es una ventaja al tener  un ChatBot.\n",
    "- Existe documentación amplia sobre como llevar a cabo el fine-tuning de este modelo.\n",
    "- Al tener una arquitectura `instruct`, es posible entrenarlo con archivos `jsonl` los cuales contengan diccionarios con valores de `instruction`, `input` y `output`.\n",
    "- Aunque `Mistral-7B` fue el modelo que mejor rendimiento tuvo, `Phi-3` ocupa solamente el 23% del espacio en disco de lo que `Mistral-7B` ocupa.  Esto hace que `Phi-3` sea un modelo  más portable al poder correr en hardware más débil (como es en nuestro caso). Esto reduce el costo de montar un ChatBot nativo por medio de un LLM, y permite justificar el no desarrollar un wraper de APIs de marcas como OpenAI y Claude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1957d5b2",
   "metadata": {},
   "source": [
    "## Generación de archivos para fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe1d173",
   "metadata": {},
   "source": [
    "Ahora que elegimos nuestro modelo (`Phi-3`), debemos generar los archivos necesarios para llevar a cabo el fine-tuning del mismo.\n",
    "\n",
    "Para esto, Daniel Voigt Godoy publicó su libro `A Hands-On Guide to Fine-Tuning Large Language Models with PyTorch and Hugging Face` (Voigt, 2025), en el cual generó un ejemplo de cómo llevar a cabo el fine-tuning de un LLM.\n",
    "\n",
    "Por lo tanto, hicimos uso del código de Daniel Voigt en el notebook `LL-Fine-Tuning.ipynb`, en el cual:\n",
    "\n",
    "- Cargamos el modelo `Phi-3` de Microsoft\n",
    "- Por medio de librerías de HuggingFace se lleva a cabo un proceso conocido como LoRA (Low Rank Adaptation) para *cambiar solo los pesos de las capas finales del modelo a hacer fine-tuning`.\n",
    "\n",
    "Lo anterior permitió \"especializar\" a Phi-3 con base en nuestro dataset trabajado en entregas anteriores. Por lo anterior, todo el proceso de Fine-Tuning se encuentra en la notebook mencionada anteriormente, y en este notebook solo haremos uso del modelo obtenido por este proceso.\n",
    "\n",
    "Por último, quisiéramos aclarar que recomendamos altamente correr el notebook `LL-Fine-Tuning.ipynb` en Google Colab u otro servicio que ofrezca GPUs potentes. Como ejemplo, al querer  llevar a cabo el fine-tuning localmente, debíamos esperar aproximadamente 3 días; pero al usar Google Colab en horarios donde pocos usuarios se conectan, fue posible llevar a cabo el fine-tuning en 5 horas. Algo  importante es que solo llevamos a cabo el proceso de fine-tuning usando información de Julio de 2025 en nuestro dataset.\n",
    "\n",
    "Ahora bien, procedemos con la generación de archivos `jsonl` que servirán para hacer el fine-tuning the `Phi-3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96f493cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tramp_id', 'sampling_date', 'lat', 'lon', 'municipality',\n",
       "       'square_area', 'plantation_age', 'capture_count', 'state',\n",
       "       'square_area_imputed', 'month', 'year', 'year_month', 'day_of_year_sin',\n",
       "       'day_of_year_cos', 'day_of_week_sin', 'day_of_week_cos',\n",
       "       'week_of_year_sin', 'week_of_year_cos', 'month_sin', 'month_cos',\n",
       "       'critical_season', 'severity_encoded', 'distance_to_nearest_hotspot',\n",
       "       'hotspots_within_5km', 'text_feature_location', 'text_feature_risk',\n",
       "       'text_feature_capture', 'text_feature_plantation',\n",
       "       'text_feature_all_things'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos el dataset que contiene los text_features\n",
    "df = pd.read_excel('baseline.xlsx')\n",
    "\n",
    "# Eliminamos el index de nuestro dataset\n",
    "df.drop(labels=['Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "# Revisamos las columnas\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05a36875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Delbert\\AppData\\Local\\Temp\\ipykernel_17416\\1499313213.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[month_mask]\n"
     ]
    }
   ],
   "source": [
    "# Utilizaremos solamente la informacion a partir de Julio de 2025 para esta entrega\n",
    "# ya que de lo contrario tendríamos más de 1,000,000 de archivos para hacer el fine-tuning\n",
    "year_mask = df['year'] > 2024\n",
    "month_mask = df['month'] > 6\n",
    "df = df[year_mask]\n",
    "df = df[month_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bab8eb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_dict = {\n",
    "    0: 'sin riesgo',\n",
    "    1: 'de riesgo leve',\n",
    "    2: 'de riesgo moderado',\n",
    "    3: 'de riesgo severo'\n",
    "}\n",
    "\n",
    "critical_season_dict = {\n",
    "    0: 'normal',\n",
    "    1: 'critica'\n",
    "}\n",
    "\n",
    "\n",
    "area_information_dict = {\n",
    "    'original': \" (area historica registrada correctamente)\", \n",
    "    'radius_regressor' : \"(el area en hectareas se calculo usando trampas cercanas)\", \n",
    "    'median' : \"(el area en hectareas se calculo usando la mediana de los datos de muestra)\", \n",
    "    'same_trap_id_temporal' : \" (valor del area en hectarea obtenido usando registros historicos)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44afc2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Esta función permite crear los archivos jsonl que se usaran para hacer el\n",
    "### fine-tuning de Phi-3.\n",
    "\n",
    "def generate_jsonl(name, instruction_variants, input_col_name, out_col_name):\n",
    "\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        for instruction in instruction_variants:\n",
    "            records.append({\n",
    "                \"instruction\": instruction,\n",
    "                \"input\": row[input_col_name],\n",
    "                \"output\": row[out_col_name]\n",
    "            })\n",
    "            \n",
    "\n",
    "    output_path = f\"{name}\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for record in records:\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Archivo JSONL generado correctamente: {output_path}\")\n",
    "    print(f\"Total de ejemplos creados: {len(records)} (≈ {len(instruction_variants)} × {len(df)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d65af",
   "metadata": {},
   "source": [
    "#### Generamos la información de ubicación (Estado, Municipalidad, lat y lon, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06a34079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo JSONL generado correctamente: agave_location.jsonl\n",
      "Total de ejemplos creados: 62030 (≈ 2 × 31015)\n"
     ]
    }
   ],
   "source": [
    "# Empezamos a generar los archivos jsonl para usarlos en el fine-tuning de nuestro LLM \n",
    "\n",
    "def build_location_input(x):\n",
    "    return (\n",
    "        f\"tramp_id: {x['tramp_id']}, \"\n",
    "        f\"lat: {x['lat']}, lon: {x['lon']}, \"\n",
    "        f\"sampling_date: {x['sampling_date'].strftime('%d-%m-%Y')}, \"\n",
    "        f\"municipality: {x['municipality']}, \"\n",
    "        f\"state: {x['state']}, \"\n",
    "        f\"plantation_age: {x['plantation_age']}, \"\n",
    "        f\"square_area_imputed: {x['square_area_imputed']}, \"\n",
    "    )\n",
    "    \n",
    "\n",
    "instruction_variants_for_location = [\n",
    "    \"Convierte los datos estructurados del reporte de trampa en una descripcion en lenguaje natural.\",\n",
    "    \"Describe la ubicacion y las condiciones del cultivo de agave con base en los datos del reporte.\"\n",
    "]\n",
    "\n",
    "df[\"input_for_location\"] =  df.apply(build_location_input, axis= 1)\n",
    "df[\"output_for_location\"] = df[\"text_feature_location\"]\n",
    "\n",
    "generate_jsonl('agave_location.jsonl', instruction_variants_for_location, 'input_for_location', 'output_for_location')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd8a13b",
   "metadata": {},
   "source": [
    "#### Generamos el set de instrucciones para tomar en cuenta el rieso presente en algunos predios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44eeaaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo JSONL generado correctamente: agave_risk.jsonl\n",
      "Total de ejemplos creados: 62030 (≈ 2 × 31015)\n"
     ]
    }
   ],
   "source": [
    "def build_risk_input(x):\n",
    "    return (\n",
    "        f\"tramp_id: {x['tramp_id']}, \"\n",
    "        f\"sampling_date: {x['sampling_date'].strftime('%d-%m-%Y')}, \"\n",
    "        f\"severity_encoded: {x['severity_encoded']} ({severity_dict.get(x['severity_encoded'], 'Desconocido')}), \"\n",
    "        f\"distance_to_nearest_hotspot: {x['distance_to_nearest_hotspot']} km, \"\n",
    "        f\"hotspots_within_5km: {x['hotspots_within_5km']}\"\n",
    "    )\n",
    "\n",
    "instruction_variants_for_risk = [\n",
    "    \"Analiza los datos de una trampa y genera una descripción del nivel de riesgo de infestación por picudo del agave.\",\n",
    "    \"Describe el riesgo de infestación considerando la severidad y la distancia a focos severos.\"\n",
    "]\n",
    "\n",
    "\n",
    "df[\"input_for_risk\"] =  df.apply(build_risk_input, axis= 1)\n",
    "df[\"output_for_risk\"] = df[\"text_feature_risk\"]\n",
    "\n",
    "\n",
    "generate_jsonl('agave_risk.jsonl', instruction_variants_for_risk, 'input_for_risk', 'output_for_risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce13e8d9",
   "metadata": {},
   "source": [
    "#### Tomamos en cuenta la información de captura por cada trampa que sobrevivió los filtros de año y fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e094b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo JSONL generado correctamente: agave_capture.jsonl\n",
      "Total de ejemplos creados: 62030 (≈ 2 × 31015)\n"
     ]
    }
   ],
   "source": [
    "def build_capture_input(x):\n",
    "    return (\n",
    "        f\"tramp_id: {x['tramp_id']}, \"\n",
    "        f\"sampling_date: {x['sampling_date'].strftime('%d-%m-%Y')}, \"\n",
    "        f\"capture_count: {x['capture_count']}, \"\n",
    "        f\"severity_encoded: {x['severity_encoded']} \"\n",
    "        f\"({severity_dict.get(x['severity_encoded'], 'Desconocido')}), \"\n",
    "        f\"critical_season: {x['critical_season']} \"\n",
    "        f\"({critical_season_dict.get(x['critical_season'], 'No especificado')})\"\n",
    "    )\n",
    "\n",
    "instruction_variants_for_capture = [\n",
    "    \"Describe en lenguaje natural los resultados de captura obtenidos por la trampa durante la temporada indicada.\",\n",
    "    \"Genera una descripción completa de la cantidad de gorgojos capturados y el nivel de infestación detectado.\"\n",
    "]\n",
    "\n",
    "df[\"input_for_capture\"] = df.apply(build_capture_input, axis=1)\n",
    "df[\"output_for_capture\"] = df[\"text_feature_capture\"]\n",
    "\n",
    "generate_jsonl('agave_capture.jsonl', instruction_variants_for_capture, 'input_for_capture', 'output_for_capture')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a920ed12",
   "metadata": {},
   "source": [
    "#### Genramos el jsonl para describir a las plantaciones en función de los años de operación, área, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31f82966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo JSONL generado correctamente: agave_plantation.jsonl\n",
      "Total de ejemplos creados: 62030 (≈ 2 × 31015)\n"
     ]
    }
   ],
   "source": [
    "def build_plantation_input(x):\n",
    "    return (\n",
    "        f\"square_area_imputed: {x['square_area_imputed']} ha, \"\n",
    "        f\"plantation_age: {x['plantation_age']} años, \"\n",
    "        f\"capture_count: {x['capture_count']} gorgojos, \"\n",
    "        f\"severity_encoded: {x['severity_encoded']} \"\n",
    "        f\"({severity_dict.get(x['severity_encoded'], 'Desconocido')})\"\n",
    "    )\n",
    "    \n",
    "instruction_variants_for_plantation = [\n",
    "    \"Describe en lenguaje natural las características de la plantación de agave, incluyendo su tamaño, edad y nivel de infestación.\",\n",
    "    \"Genera una descripción completa del estado de la plantación considerando el área, la edad y la cantidad de gorgojos capturados.\"\n",
    "]\n",
    "\n",
    "df[\"input_for_plantation\"] = df.apply(build_plantation_input, axis=1)\n",
    "df[\"output_for_plantation\"] = df[\"text_feature_plantation\"]\n",
    "\n",
    "generate_jsonl('agave_plantation.jsonl', instruction_variants_for_plantation, 'input_for_plantation', 'output_for_plantation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3982888",
   "metadata": {},
   "source": [
    "#### Por último, creamos un jsonl que contenga la combinación de todos los demás text features para dar más contexto a nuestro LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4655ea41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo JSONL generado correctamente: agave_allThings.jsonl\n",
      "Total de ejemplos creados: 93045 (≈ 3 × 31015)\n"
     ]
    }
   ],
   "source": [
    "def build_allThings_input(x):\n",
    "    return (\n",
    "        f\"tramp_id: {x['tramp_id']}, \"\n",
    "        f\"lat: {x['lat']}, lon: {x['lon']}, \"\n",
    "        f\"sampling_date: {x['sampling_date'].strftime('%d-%m-%Y')}, \"\n",
    "        f\"municipality: {x['municipality']}, state: {x['state']}, \"\n",
    "        f\"square_area_imputed: {x['square_area_imputed']} ha, \"\n",
    "        f\"plantation_age: {x['plantation_age']} años, \"\n",
    "        f\"capture_count: {x['capture_count']} gorgojos, \"\n",
    "        f\"severity_encoded: {x['severity_encoded']} \"\n",
    "        f\"({severity_dict.get(x['severity_encoded'], 'Desconocido')}), \"\n",
    "        f\"distance_to_nearest_hotspot: {x['distance_to_nearest_hotspot']} km, \"\n",
    "        f\"hotspots_within_5km: {x['hotspots_within_5km']}\"\n",
    "    )\n",
    "    \n",
    "instruction_variants_for_all_things = [\n",
    "    \"Genera una descripción completa de la plantación, la ubicación de la trampa y el nivel de riesgo de infestación combinando toda la información disponible.\",\n",
    "    \"Describe en lenguaje natural los detalles de la trampa, la plantación y las condiciones de infestación, incluyendo ubicación, distancia a focos y severidad.\",\n",
    "    \"Redacta un resumen integral que combine los datos de captura, características de la plantación y proximidad a focos severos de infestación.\"\n",
    "]\n",
    "\n",
    "\n",
    "df[\"input_for_allThings\"] = df.apply(build_allThings_input, axis=1)\n",
    "df[\"output_for_allThings\"] = df[\"text_feature_all_things\"]\n",
    "\n",
    "generate_jsonl('agave_allThings.jsonl', instruction_variants_for_all_things, 'input_for_allThings', 'output_for_allThings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be09b8",
   "metadata": {},
   "source": [
    "#### Función para unir todos los text_features en un solo jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20186e04",
   "metadata": {},
   "source": [
    "Esta función es opcional, pero recomendamos generar los archivos por separado ya que durante el proceso de fine-tuning fue necesario contextualizar al LLM por `chunks`o pedazos.\n",
    "\n",
    "Esto permitió que el tiempo de fine-tuning fuera menor en comparación de cargar un archivo con todos los features, y habilitó también la mezcla de distintas `instructions` y `outputs` sin la necesidad de usar shuffling o selecciones aleatorias de data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921efab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# files_to_merge = [\n",
    "#     \"/content/drive/MyDrive/LLM-training/agave_location.jsonl\",\n",
    "#     \"/content/drive/MyDrive/LLM-training/agave_risk.jsonl\",\n",
    "#     \"/content/drive/MyDrive/LLM-training/agave_capture.jsonl\",\n",
    "#     \"/content/drive/MyDrive/LLM-training/agave_capture.jsonl\",\n",
    "#     \"/content/drive/MyDrive/LLM-training/agave_allThings.jsonl\"\n",
    "# ]\n",
    "\n",
    "# output_path = \"/content/drive/MyDrive/LLM-training/agave_combined_multitask.jsonl\"\n",
    "\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "#     for path in files_to_merge:\n",
    "#         with open(path, \"r\", encoding=\"utf-8\") as infile:\n",
    "#             for line in infile:\n",
    "#                 outfile.write(line)\n",
    "\n",
    "# print(f\"✅ Combined file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a7754b",
   "metadata": {},
   "source": [
    "### Ahora sí probaremos nuestro LoRA y Phi-3 personalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9191d2",
   "metadata": {},
   "source": [
    "Ahora cargamos los mismos parámetros que usamos para llevar a cabo el fine-tune de `Phi-3`.\n",
    "\n",
    "En esta sección es importante mencionar que para reducir el tiempo de fine-tuning en Google-Colab, decidimos cuantizar a `Phi-3` en 4bits para generar los LoRA adapters en un tiempo prudente. Con esto, aunque estos LoRA adapters fueron creados con base en la cuantización de 4bits de Phi-3, es posible cargar a Phi-3 cuantizado en 8bits y acoplarle los LoRa que generamos.\n",
    "\n",
    "Por último, en este punto recomendamos reiniciar el kernel de Python y cargar de nuevo las librerías a utilizar. De esta manera podemos asegurar que la memoria RAM de nuestra GPU estará al mínimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e5d42eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "del model\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "169ee809",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "405cbe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.11s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Modelo local \n",
    "model_path = \"D:/LLM Models/microsoft-phi-3-mini-4k-instruct\"\n",
    "\n",
    "# LoRA adapters generados con el archivo LL_Fine_Tune_agave.ipynb\n",
    "lora_path = \"D:/LLM Models/agave_V001/agave_baseline_phi3_V01\"   \n",
    "\n",
    "# 8 bits de cuantizacion\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "# Usamos el tokenizer de Phi-3\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a137390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Implementamos los pesos LoRA a Phi-3\n",
    "model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "\n",
    "# Colocamos el  modelo en evaluacion para que no cambie los pesos por cada prompt que\n",
    "# se le envíe\n",
    "model.eval()\n",
    "\n",
    "# Definimos el pipeline de nuevo\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=64,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d11b37",
   "metadata": {},
   "source": [
    "Ahora, durante el fine-tuning de Phi-3 se usó el siguiente template de conversación:\n",
    "\n",
    "```\n",
    "<|user|>\n",
    "`Pregunta o  prompt del usuario`.<|end|>\n",
    "<|assistant|>\n",
    "`Respusta de Phi-3 al usar el Chat template`\n",
    "```\n",
    "\n",
    "Esto se hizo con base en la documentación mostrada por (Voigt, 2025), y por lo tanto para que nuestro modelo fine-tuned funcione correctamente, debemos convertir nuestros prompts a esta forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6505b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(tokenizer, sentence):\n",
    "    converted_sample = [{\"role\": \"user\", \"content\": sentence}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        converted_sample, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acbab8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=False):\n",
    "  \n",
    "  tokenized_input = tokenizer(\n",
    "  prompt, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "  model.eval()\n",
    "  gen_output = model.generate(**tokenized_input,\n",
    "  eos_token_id=tokenizer.eos_token_id,\n",
    "  max_new_tokens=max_new_tokens)\n",
    "  output = tokenizer.batch_decode(gen_output, skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "  return output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55c03fd",
   "metadata": {},
   "source": [
    "### Con `prompt_1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f713b0",
   "metadata": {},
   "source": [
    "Notamos que ahora con `prompt_1`, nuestro LLM agrega contexto tomando en cuenta a México."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d69fedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Describe al gorgojo del agave brevemente<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "<|user|> Describe al gorgojo del agave brevemente<|end|><|assistant|> El gorgojo del agave es una especie de insecto escamudo perteneciente a la familia de los Chrysomelidae, comúnmente conocida como la lagartija del agave. Se distribuye principalmente en México y América Central. Este insecto es conocido por su capacidad\n"
     ]
    }
   ],
   "source": [
    "sentence = prompt_1\n",
    "prompt = gen_prompt(tokenizer, sentence)\n",
    "print(prompt)\n",
    "\n",
    "print(generate(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae9a4fd",
   "metadata": {},
   "source": [
    "### Con `prompt_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aa44956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Describe la planta de agave azul brevemente<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "<|user|> Describe la planta de agave azul brevemente<|end|><|assistant|> La planta de agave azul, conocida científicamente como Agave tequilana, es una especie de planta suculenta nativa de México. Es ampliamente cultivada por su capacidad de producir agave, un ingrediente clave en la elabora Convierte\n"
     ]
    }
   ],
   "source": [
    "sentence = prompt_2\n",
    "prompt = gen_prompt(tokenizer, sentence)\n",
    "print(prompt)\n",
    "\n",
    "print(generate(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee09c9b",
   "metadata": {},
   "source": [
    "Con `prompt_3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "349b28f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Describe como es un predio de agave azul y cuales riesgos presenta brevemente<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "<|user|> Describe como es un predio de agave azul y cuales riesgos presenta brevemente<|end|><|assistant|> Un predio de agave azul, conocido como maguey, se caracteriza por su vegetación xerófila y su estructura de tramp_id: 11390-11909, ubicado en la región de Jalisco, México. La\n"
     ]
    }
   ],
   "source": [
    "sentence = prompt_3\n",
    "prompt = gen_prompt(tokenizer, sentence)\n",
    "print(prompt)\n",
    "\n",
    "print(generate(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f509b64b",
   "metadata": {},
   "source": [
    "Al mencionar un ID de trampa que se usó para entrenar el modelo, es posible observar que ahora nuestro ChatBot puede conocer la información general de la misma.\n",
    "\n",
    "En esta etapa, aunque el ChatBot no contestó con la información de la trampa en sí, fue posible obtener algo de información de él. Ahora bien, es indispensable llevar a cabo el fine-tune de nuestro LLM con más muestras de nuestro dataset, tomando en cuenta que se requiere de más tiempo para que se procese correctamente la creación de LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5703bdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "gorgojos capturados por tramp id: 1387<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "<|user|> gorgojos capturados por tramp id: 1387<|end|><|assistant|> tramp_id: 1387, lat: 20.4713, lon: -104.173, sampling_date: 15-07-2025, municipality: JUCHITLAN, state: JALISCO\n"
     ]
    }
   ],
   "source": [
    "sentence = \"gorgojos capturados por tramp id: 1387\"\n",
    "prompt = gen_prompt(tokenizer, sentence)\n",
    "print(prompt)\n",
    "\n",
    "print(generate(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eea6bb7",
   "metadata": {},
   "source": [
    "Al trabajar con un prompt más específico, notamos que ahora el LLM puede recuperar correctamente las variables `lat`, `lon`, `sampling_date` y `state` (este ultimo fue truncado por la cantidad maxima de Tokens a generar).\n",
    "\n",
    "Con esto confirmamos que nuestro modelo \"aprendió\" la forma de \"escribir\" o predecir las respuestas. Es importante notar que el prompt utilizado fue bastante parecido a lo que se usó como `instruction` en su entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bce7bf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Genera una descripción completa de la plantación, la ubicación de la trampa y el nivel de riesgo de infestación combinando toda la información disponible para tramp_id: 1387.<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "<|user|> Genera una descripción completa de la plantación, la ubicación de la trampa y el nivel de riesgo de infestación combinando toda la información disponible para tramp_id: 1387.<|end|><|assistant|> tramp_id: 1387\n",
      "\n",
      "lat: 20.483333\n",
      "\n",
      "lon: -103.083333\n",
      "\n",
      "sampling_date: 15-07-2025\n",
      "\n",
      "state: JALA\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Genera una descripción completa de la plantación, la ubicación de la trampa y el nivel de riesgo de infestación combinando toda la información disponible para tramp_id: 1387.\"\n",
    "prompt = gen_prompt(tokenizer, sentence)\n",
    "print(prompt)\n",
    "\n",
    "print(generate(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b46786a",
   "metadata": {},
   "source": [
    "Al especificar mucho más nuestra consulta con base en la estructura `instruction`-`input`-`output` generados, es posible obtener respuestas más concretas.\n",
    "\n",
    "En este caso, obtener la respuesta correcta: `JALISCO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80204386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Eres un asistente agricultor que permite consultar información sobre la ubicación de trampas. Las trampas se identifican con un tramp_id. Dime en qué estado se encontraba la tramp_id: 1402.<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "<|user|> Eres un asistente agricultor que permite consultar información sobre la ubicación de trampas. Las trampas se identifican con un tramp_id. Dime en qué estado se encontraba la tramp_id: 1402.<|end|><|assistant|> La tramp_id: 1402 se encuentra en el estado de Jalisco.<|end|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Eres un asistente agricultor que permite consultar información sobre la ubicación de trampas. Las trampas se identifican con un tramp_id. Dime en qué estado se encontraba la tramp_id: 1402.\"\n",
    "prompt = gen_prompt(tokenizer, sentence)\n",
    "print(prompt)\n",
    "\n",
    "print(generate(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf6ce6",
   "metadata": {},
   "source": [
    "En este punto, notamos que es necesario un sistema RAG para evitar que nuestro LLM genere respuestas que no tengan mucha relación con lo que se le preguntó, y sobre todo que el formato de las mismas sea coherente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c08c979c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Genera una descripción completa de la cantidad de gorgojos capturados y el nivel de infestación detectado de la tramp_id: 1387 y cuántos gorgojos capturó el 15-07-2024.<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "<|user|> Genera una descripción completa de la cantidad de gorgojos capturados y el nivel de infestación detectado de la tramp_id: 1387 y cuántos gorgojos capturó el 15-07-2024.<|end|><|assistant|> Tramp_id: 1387\n",
      "\n",
      "Lat, Long: 20.900833, -104.183333\n",
      "\n",
      "Precipitation: 0.00 mm\n",
      "\n",
      "Temperature: 26.00 °\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Genera una descripción completa de la cantidad de gorgojos capturados y el nivel de infestación detectado de la tramp_id: 1387 y cuántos gorgojos capturó el 15-07-2024.\"\n",
    "prompt = gen_prompt(tokenizer, sentence)\n",
    "print(prompt)\n",
    "\n",
    "print(generate(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49850fd8",
   "metadata": {},
   "source": [
    "# Sistema RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b36479",
   "metadata": {},
   "source": [
    "Ahora que ya hicimos un fine-tuning sobre `Phi-3`, aunque este fue bastante superficial porque solo usamos 12,500 registros `instruction`-`input`-`output`, podemos indicar que nuestro LLM ahora \"escribirá\" de manera más propia la información con base en las respuestras de los prompts anteriores.\n",
    "\n",
    "Por lo anterior, lo que haremos ahora será imlementar un sistema RAG el cual le dará las siguientes características a nuetro  LLM:\n",
    "\n",
    "- Lo volverá más estable al responder. Por medio del sistema RAG, contextos y demás, será posible reforzar la forma en que el modelo responderá y demostrará la información.\n",
    "- Permite darle instrucciones al LLM para no halucinar. Por ejemplo, es posible indicar que si la información no se encuentra disponible en el contexto, que no responda la pregunta.\n",
    "\n",
    "Con esto, montar el sistema RAG sobre nuestro Phi-3 con fine-tuning permite que, aparte de que nuestro Bot hable más propiamente, pueda usar información histórica para responder dudas comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e571d181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargaron 44 paginas desde el Manual Operativo.\n",
      "Y los primeros 100 caracteres son los siguientes:\n",
      "\n",
      "DIRECCIÓN DE PROTECCIÓN FITOSANITARIA \n",
      "  Clave: MOP-DPF-PRAV \n",
      "  Versión: 4 \n",
      " \n",
      "I \n",
      "  Emisión: 04/2017 \n"
     ]
    }
   ],
   "source": [
    "# Cargamos el Manual Operativo como un documento para consulta del bot.\n",
    "loader = PyPDFLoader(r'C:/Users/Delbert/Documents/Maestria/Proyecto Integrador/Data/Manual_Operativo_Agave.pdf')\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Se cargaron {len(docs)} paginas desde el Manual Operativo.\")\n",
    "print(f\"Y los primeros 100 caracteres son los siguientes:\\n\")\n",
    "print(docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64e08eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargaron 341165 documentos de 5 archivos JSONL.\n",
      "Instrucción: Genera una descripción completa de la plantación, la ubicación de la trampa y el nivel de riesgo de infestación combinando toda la información disponible.\n",
      "Entrada: tramp_id: 6538-145, lat: 20.92757, lon: -103.8551, sampling_date: 04-07-2025, municipality: TEQUILA, state: JALISCO, square\n"
     ]
    }
   ],
   "source": [
    "# Cargamos todos los text_features previamente generados que podrán ser consultados de nuevo.\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def load_jsonl_as_documents(jsonl_paths):\n",
    "\n",
    "    all_docs = []\n",
    "\n",
    "    for file_path in jsonl_paths:\n",
    "        path = Path(file_path)\n",
    "        if not path.exists():\n",
    "            print(f\"No se encontró el archivo siguiente: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line_num, line in enumerate(f, start=1):\n",
    "                try:\n",
    "                    record = json.loads(line.strip())\n",
    "                    # Usamos de nuevo la estructura de Alpaca\n",
    "                    text = (\n",
    "                        f\"Instrucción: {record.get('instruction', '')}\\n\"\n",
    "                        f\"Entrada: {record.get('input', '')}\\n\"\n",
    "                        f\"Respuesta: {record.get('output', '')}\"\n",
    "                    )\n",
    "\n",
    "                    # Tomamos metadata\n",
    "                    doc = Document(\n",
    "                        page_content=text,\n",
    "                        metadata={\n",
    "                            \"source\": path.name,\n",
    "                            \"line\": line_num\n",
    "                        }\n",
    "                    )\n",
    "                    all_docs.append(doc)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Se dio un error en la linea {line_num} en {path.name}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    print(f\"Se cargaron {len(all_docs)} documentos de {len(jsonl_paths)} archivos JSONL.\")\n",
    "    return all_docs\n",
    "\n",
    "jsonl_files = [\n",
    "    \"agave_allThings.jsonl\",\n",
    "    \"agave_capture.jsonl\",\n",
    "    \"agave_location.jsonl\",\n",
    "    \"agave_plantation.jsonl\",\n",
    "    \"agave_risk.jsonl\"\n",
    "]\n",
    "\n",
    "docs_jsonl = load_jsonl_as_documents(jsonl_files)\n",
    "print(docs_jsonl[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8dbb04",
   "metadata": {},
   "source": [
    "#### Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730d173e",
   "metadata": {},
   "source": [
    "Una parte ***importante*** de un sistema RAG son los embeddings que permitirán que nuestro LLM relacione de mejor manera las palabras. Estos vectores no son más que \"pesos\" que relacionan palabras entre sí y permitirán que el LLM pueda predecir de mejor manera las respuestas.\n",
    "\n",
    "Estos embeddings terminan siendo como el \"alma\" que permite que nuestro Bot no alucine y tampoco conteste algo que no sea real. En este paso, debido la cantidad de documentos, es normal esperar entre 60 a 120 minutos de generación de embeddings. Este proceso se puede acelerar por medio de Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e20a8e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Delbert\\AppData\\Local\\Temp\\ipykernel_7524\\854352266.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "chunked_docs = splitter.split_documents(docs)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-base\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbde9dfd",
   "metadata": {},
   "source": [
    "El siguiente código se debe des-comentar para generar los embeddings de forma local. Solo debemos tener en mente que es un proceso que conlleva de 60 a 120 minutos para generarlos.\n",
    "\n",
    "En nuestro caso, los generamos una sola vez y a partir de eso solo los cargamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51570e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore = FAISS.from_documents(chunked_docs, embedding=embeddings)\n",
    "# vectorstore.add_documents(docs_jsonl)\n",
    "\n",
    "# # Guardamos la vector store para cargas mas rapidas despues\n",
    "# vectorstore.save_local(\"rag_faiss_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2245ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Para agilizar otras  corridas, podemos volver a cargar los embeddings generados a partir de la última sesión\n",
    "vectorstore = FAISS.load_local(\"rag_faiss_store\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce1172c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 619918\n"
     ]
    }
   ],
   "source": [
    "vs = FAISS.load_local(\n",
    "    \"C:/Users/Delbert/Documents/Maestria/Proyecto Integrador/Avance 1/Tecnologico-Monterrey-Proyecto-Integrador-equipo-36/Baseline/rag_faiss_store\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "print(\"Total docs:\", len(vs.docstore._dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555cd0ed",
   "metadata": {},
   "source": [
    "Ahora bien, como nuestro LLM tuvo un fine-tuning con base en 2 templates (`instruction, input & output` y el `chat-template` de Phi-3), es importante que nuestro sistema RAG pueda encapsular las consultas que se le hagan de forma que el LLM lo entienda.\n",
    "\n",
    "En el caso en que cambiemos el LLM utilizado, deberíamos de generar una nueva clase para lograr lo mismo. Por lo tanto, en este punto, ya estamos practicamente comprometidos con continuar con Phi-3 ya que:\n",
    "\n",
    "1. Hicimos un proceso de fine-tuning sobre `Phi-3` por varias horas\n",
    "2. Creamos una clase para hacer wraping del sistema RAG para que `Phi-3` comprenda los queries haciendo uso  del template `ChatBot`.\n",
    "3. Realizar de nuevo estos entrenamientos sobre otro modelo puede ser computacionalmente más caro. Por ejemplo, `Mistral-7B` aun cuantizado en 4bits consume bastante más VRAM que `Phi-3` cuantizado en 8bits\n",
    "\n",
    "\n",
    "Por último, configuramos nuestro LLM para producir un máximo de 32 tokens de salida. Si se quiere tener respustas más largas, debemos definir un `max_new_tokens` distinto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eebc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phi3ChatTemplateLLM(LLM):\n",
    "    model: Any = Field(exclude=True)\n",
    "    tokenizer: Any = Field(exclude=True)\n",
    "    max_new_tokens: int = 32\n",
    "    skip_special_tokens: bool = True\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"phi3-chat-template\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs: Any) -> str:\n",
    "        chat_prompt = gen_prompt(self.tokenizer, prompt)\n",
    "        out = generate(\n",
    "            self.model,\n",
    "            self.tokenizer,\n",
    "            chat_prompt,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            skip_special_tokens=self.skip_special_tokens\n",
    "        )\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d128d1c",
   "metadata": {},
   "source": [
    "Cargamos el modelo custom (contenido en la clase) para hacer pruebas.\n",
    "\n",
    "Usaremos el tokenizer incluido con la versión original de `Phi-3` para ahorrarnos el proceso de definición de tokens (por palabra, oración, párrafo, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28801e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_llm = Phi3ChatTemplateLLM(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f012d9",
   "metadata": {},
   "source": [
    "Configuramos el sistema `retriever` para implementar el RAG.\n",
    "\n",
    "Y generamos un prompt el cual, por medio de distintas pruebas, fue posible definir para obtener respuestas concretas de nuestro bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea7b8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"Eres un asistente operativo especializado en el picudo del agave. \"\n",
    "        \"Usa **exclusivamente** la información del CONTEXTO para responder. \"\n",
    "        \"Si la respuesta no aparece en el contexto, di exactamente: \"\n",
    "        \"\\\"No aparece en el contexto.\\\" No inventes datos.\\n\\n\"\n",
    "        \"CONTEXTO:\\n{context}\\n\\n\"\n",
    "        \"PREGUNTA:\\n{question}\\n\\n\"\n",
    "        \"RESPUESTA DE ASISTENTE:\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=custom_llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": my_prompt},\n",
    "    return_source_documents=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea46a7",
   "metadata": {},
   "source": [
    "Y ahora generamos un par de prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e73d366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un agave es una planta suculenta originaria de México y América Central, perteneciente a la familia de las agaváceas. Se\n"
     ]
    }
   ],
   "source": [
    "query = \"¿Qué es un agave?\"\n",
    "response = rag_chain.invoke({\"query\": query})\n",
    "\n",
    "raw_output = response[\"result\"]\n",
    "\n",
    "\n",
    "# Limpiamos la respuesta\n",
    "if \"RESPUESTA DE ASISTENTE:\" in raw_output:\n",
    "    clean_output = raw_output.split(\"RESPUESTA DE ASISTENTE:\")[-1].strip()\n",
    "else:\n",
    "    clean_output = raw_output.strip()\n",
    "\n",
    "print(clean_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38538c2c",
   "metadata": {},
   "source": [
    "Por último, dejamos las versiones de Torch y CUDA que utilizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1de0add8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Version de Pytorch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# \n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f42cff",
   "metadata": {},
   "source": [
    "# Conexión a WhatsApp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85be4fd",
   "metadata": {},
   "source": [
    "Por medio de Twilio y su integración como ISV de Meta, es posible hacer pruebas en una sandbox la cual no tiene costo, pero sí un límite diario de mensajes.\n",
    "\n",
    "Con lo anterior, debemos instalar varias librerías en nuestro ambiente:\n",
    "\n",
    "- Twilio\n",
    "- Flask (esto servirá para recibir los mensajes desde Twilio por medio de un webhook)\n",
    "- dotenv (para cargar las variables de entorno para autenticarse en Twilio)\n",
    "- Ngrok (aunque no es un módulo en este notebook, se tuvo que instalar para hacer hosting de nuestra aplicación de Flask en un endpoint que Ngrok genera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19822a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from twilio.twiml.messaging_response import MessagingResponse\n",
    "from flask import Flask, request, Response\n",
    "from twilio.rest import Client\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "\n",
    "\n",
    "load_dotenv(dotenv_path=\"chatbot.env\")\n",
    "\n",
    "account_sid = os.getenv('account_sid_2')  # Variable de entorno para usar la API de Twilio\n",
    "auth_token  = os.getenv('auth_token_2')   # Variable de entorno para autenticarse ante la API de Twilio\n",
    "FROM_NUMBER  = \"\"                         # Numero del sandbox que Twilio proporciona\n",
    "client = Client(account_sid, auth_token ) # Intanciamos el objeto de Twilio para probar nuestro ChatBot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d5af6",
   "metadata": {},
   "source": [
    "## Flujo principal para reporte de predios infestados con el gorgojo del agave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8651b4ba",
   "metadata": {},
   "source": [
    "Definimos un diccionario para conocer el estado de cada chat en el prototipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd63773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global session storage (temporary — resets if the app restarts)\n",
    "user_session = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4035c11e",
   "metadata": {},
   "source": [
    "#### Envío de mensajes por medio de Twilio-WhatsApp-Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d7e3e",
   "metadata": {},
   "source": [
    "Definimos una función para enviar mensajes sin formato a WhatsApp, los cuale se conocen como FreeForm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ee318f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_whatsapp_message(From, To, message):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        msg = client.messages.create(\n",
    "            from_=From,\n",
    "            to=To,\n",
    "            body=message\n",
    "        )\n",
    "        # print(f\"Message sent: {msg.sid}\") # Descomentar esta linea para ver el ID del mensaje enviado por WhatsApp\n",
    "        return msg.sid\n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        print(f\"Error sending message: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a889e9",
   "metadata": {},
   "source": [
    "Definimos también una función para mostrar el menú principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "484599dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_menu(From, To):\n",
    "    \n",
    "    menu_message = (\n",
    "        \"👋 *Bienvenido, será un gusto atenderte.*\\n\\n\"\n",
    "        \"¿Qué te interesa llevar a cabo?\\n\\n\"\n",
    "        \"🅰️ *Hacer un reporte de un predio infectado*\\n\"\n",
    "        \"🅱️ *Preguntar por información*\"\n",
    "        \"\\n\\n\"\n",
    "        \"Actualmente, solo estas 2 opciones tenemos disponibles.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        msg = client.messages.create(\n",
    "            from_=From,\n",
    "            to=To,\n",
    "            body=menu_message\n",
    "        )\n",
    "        # print(f\"✅ Se envio correctamente el menu: {msg.sid}\") # Descomentar esta linea para ver el ID del mensaje enviado por WhatsApp\n",
    "        return msg.sid\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error enviando menu: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3139060d",
   "metadata": {},
   "source": [
    "Se define también el flujo principal para recabar información.\n",
    "\n",
    "Esto no incluye algo de NLP ya que el flujo se encuentra bastante definido:\n",
    "\n",
    "- Debemos solicitar la ubicación\n",
    "- Una fotografía de la infestacion o predio\n",
    "- La severidad que el usuario considera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f617874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_message_flow( sender, state, req ):\n",
    "\n",
    "\t# Cargamos el mensaje\n",
    "\tmessage = request.form.get(\"Body\", \"\").strip().lower() # Texto enviado por el usuario\n",
    "\tlabel   = request.form.get(\"Label\")  \t\t\t\t   # Label de la ubicacion enviada\n",
    "\tlat = request.form.get(\"Latitude\") \t\t\t\t\t   # Si envian ubicacion, cargamos la latitud\n",
    "\tlon = request.form.get('Longitude') \t\t\t\t   # Si envian ubicacion, cargamos la longitud\n",
    "\taddress = request.form.get('Address') \t\t\t\t   # Tomamos la dirección generada por WhatsApp\n",
    "\tphoto_url = req.form.get('MediaUrl0')                  # Si envia una fotografia, obtendremos un URL de la misma\n",
    "\tnum_media = int(req.form.get('NumMedia', 0))           # Permite saber si enviaron un mensaje con contenido multimedia\n",
    " \n",
    "\tresponse_text = \"\"\n",
    "\tnext_state = state\n",
    " \n",
    "\tif state == \"saludo\":\n",
    "\t\tif (message == \"a\") or (message == \"🅰️\"):\n",
    "\t\t\tresponse_text = (\"Gracias por tu proactividad. Te guiaré para reportar el predio. Primero, envía la **ubicación** del lote afectado.\")\n",
    "\t\t\tnext_state  = \"ask-location\"\n",
    "\n",
    "\t\telif (message == \"b\") or (message == \"🅱️\"):\n",
    "\t\t\tresponse_text = (\"Claro, dime, ¿cuál es tu duda?\")\n",
    "\t\t\tnext_state = \"NLP-Chatbot\"\n",
    "   \n",
    "\telif state == \"ask-location\":\n",
    "\t\tif lat and lon:\n",
    "\t\t\tuser_session[sender][\"data\"][\"lat\"]  = lat\n",
    "\t\t\tuser_session[sender][\"data\"][\"lon\"] = lon\n",
    "\t\t\tuser_session[sender][\"data\"][\"address\"]   = address\n",
    "\t\t\tresponse_text = (\n",
    "\t\t\t\t\"📍 Ubicación recibida *correctamente*.\\n\"\n",
    "\t\t\t\t\"Ahora, por favor envía una **foto** del lote o planta afectada.\"\n",
    "\t\t\t)\n",
    "\t\t\tnext_state = \"ask_photo\"\n",
    "\t\telse:\n",
    "\t\t\tresponse_text = (\"Parece que la información que enviaste no es correcta. Por favor, usa la función de ubicación de WhatsApp.\")\n",
    "\n",
    "\telif state == \"ask_photo\":\n",
    "\t\tif num_media > 0:\n",
    "\t\t\tphoto_url = req.form.get('MediaUrl0')\n",
    "\t\t\tuser_session[sender][\"data\"][\"photo_url\"] = photo_url\n",
    "\t\t\tresponse_text = (\n",
    "\t\t\t\t\"📸 Foto recibida correctamente.\\n\"\n",
    "\t\t\t\t\"Por último, clasifica el **nivel de riesgo** que observas: Alto, Medio o Bajo.\"\n",
    "\t\t\t)\n",
    "\t\t\tnext_state = \"ask_risk\"\n",
    "\t\telse:\n",
    "\t\t\tresponse_text = (\n",
    "\t\t\t\t\"⚠️ No se detectó una foto. Por favor envía una imagen del lote afectado.\"\n",
    "\t\t\t)\n",
    "\n",
    "\telif state == \"ask_risk\":\n",
    "    \n",
    "\t\tif message in [\"alta\", \"media\", \"baja\", \"alto\", \"medio\", \"bajo\"]:\n",
    "\t\t\tuser_session[sender][\"data\"][\"user_risk_assesment\"] = message.capitalize()\n",
    "\t\t\tdata = user_session[sender][\"data\"]\n",
    "\t\t\tresponse_text = (\n",
    "\t\t\t\tf\"✅ Gracias por tu reporte.\\n\"\n",
    "\t\t\t\tf\"📍 Ubicación: ({data['lat']}, {data['lon']})\\n\"\n",
    "\t\t\t\tf\"📸 Foto: Confirmada\\n\"\n",
    "\t\t\t\tf\"🚨 Riesgo: {data['user_risk_assesment']}\\n\\n\"\n",
    "\t\t\t\t\"¿Esta información es correcta? Responde con *Sí* o *No*.\"\n",
    "\t\t\t)\n",
    "\t\t\tnext_state = \"confirmation\"\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tresponse_text = (\n",
    "\t\t\t\t\"Por favor indica el nivel de riesgo como: Alta, Media o Baja.\"\n",
    "\t\t\t)\n",
    "\n",
    "\telif state == \"confirmation\":\n",
    "\t\tmsg = message.strip().lower()\n",
    "\n",
    "\t\tif re.search(r'^\\s*s[ií]\\s*$', msg):\n",
    "\t\t\tresponse_text = (\n",
    "\t\t\t\t\"🌾 Tu reporte ya fue registrado. ¡Gracias por tu colaboración!\"\n",
    "\t\t\t)\n",
    "\t\t\tprint(user_session[sender])\n",
    "\t\telif re.search(r'^\\s*no\\s*$', msg):\n",
    "\t\t\tresponse_text = \"❌ Entendido. Tu reporte no se registrará.\"\n",
    "\t\telse:\n",
    "\t\t\tresponse_text = \"Por el momento, solo podemos aceptar respuestas como 'Sí' o 'No'.\"\n",
    "\telse:\n",
    "\t\tresponse_text = (\"Por el momento, solo podemos aceptar respuestas como 'Sí', 'No', etc. \")\n",
    "\n",
    "\treturn response_text, next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db12ece",
   "metadata": {},
   "source": [
    "Creamos la función principal para recibir mensajes desde la sandbox de Twilio. En este punto tenemos 2 formas de atender al usuario:\n",
    "\n",
    "- Seguir el flujo de un reporte de infestación.\n",
    "- Permitir al usuario hacer preguntas sobre el manual de operaciones y data histórica de focos de infestación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd6a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reiniciamos user_session para hacer pruebas\n",
    "\n",
    "user_session = {}\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/reply_whatsapp\", methods=['POST'])\n",
    "\n",
    "def reply_whatsapp():\n",
    "\n",
    "\tsender  = request.form.get(\"From\") # Guardamos quien nos escribio\n",
    "\tmessage = request.form.get(\"Body\", \"\").strip().lower() # Tomamos el texto del mensaje para mostrarlo en la consola\n",
    "\tresp = MessagingResponse()\n",
    " \n",
    "\tprint(f\"Mensaje recibido: --> {message}\")\n",
    "\n",
    "\t# si no tenemos un chat registrado con el usuario, procedemos a agregarlo al diccionario de sesiones\n",
    "\tif sender not in user_session:\n",
    "\t\tuser_session[sender] = {\"state\": \"saludo\",\n",
    "\t\t\t\t\t\t\t\t\"data\" : {\n",
    "\t\t\t\t\t\t\t\t\t\"lat\" : None, \n",
    "        \t\t\t\t\t\t\t\"lon\" : None,\n",
    "\t\t\t\t\t\t\t\t\t\"address\" : None,\n",
    "\t\t\t\t\t\t\t\t\t\"photo_url\" : None,\n",
    "\t\t\t\t\t\t\t\t\t\"user_risk_assesment\": None,\n",
    "\t\t\t\t\t\t\t\t\t\"price\" : None\n",
    "\t\t\t\t\t\t\t\t}\n",
    "              \t\t\t\t\t}\n",
    "\n",
    "\t\tsend_menu(FROM_NUMBER, sender)\n",
    "\t\tresp.message(\"Quedo a la espera de tu respuesta.\")\n",
    "\t\treturn Response(str(resp), mimetype=\"text/xml\")\n",
    "\t\n",
    "\t# Si el usuario decidió hacer uso del feature de preguntas, usamos el LLM para contestar con el sistema RAG\n",
    "\tif user_session[sender]['state'] == \"NLP-Chatbot\":\n",
    "\t\tprint(\"Generando respuesta con el LLM\")\n",
    "\t\tquery = message\n",
    "\t\tresponse = rag_chain.invoke({\"query\": query})\n",
    "\n",
    "\t\traw_output = response[\"result\"]\n",
    "\n",
    "\t\tif \"RESPUESTA DE ASISTENTE:\" in raw_output:\n",
    "\t\t\tclean_output = raw_output.split(\"RESPUESTA DE ASISTENTE:\")[-1].strip()\n",
    "\t\telse:\n",
    "\t\t\tclean_output = raw_output.strip()\n",
    "\t\t\n",
    "\t\tresponse_text = clean_output\n",
    "\t\tprint(f\"{response_text}\")\n",
    "\n",
    "\t# Si el usuario decidió seguir con el flujo de reporte, continuamos con el mismo\n",
    "\telse:\n",
    "\t\t\n",
    "\t\tresponse_text, next_state = report_message_flow(sender, user_session[sender]['state'], request)\n",
    "\t\tuser_session[sender][\"state\"] = next_state\n",
    "\t\t\n",
    "\t\n",
    "\t# Enviamos la respuesta segun lo que el usuario decidio al inicio en el menu\n",
    "\tresp.message(response_text)\n",
    "\t\n",
    "\treturn Response(str(resp), mimetype='text/xml')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tapp.run(port=3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch311 (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
