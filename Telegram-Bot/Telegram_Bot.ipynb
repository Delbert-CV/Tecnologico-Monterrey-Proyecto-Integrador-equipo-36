{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032c6c4b",
   "metadata": {},
   "source": [
    "# Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd1accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import Field\n",
    "from sqlalchemy import create_engine, text\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from typing import Optional, List, Any\n",
    "from langchain.schema import Document\n",
    "from transformers import pipeline\n",
    "from flask import Flask, request\n",
    "from datetime import datetime\n",
    "from peft import PeftModel\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import requests\n",
    "import uuid\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a7754b",
   "metadata": {},
   "source": [
    "# Microsoft Phi-3 como modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ee809",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405cbe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo local \n",
    "model_path = \"D:/LLM Models/microsoft-phi-3-mini-4k-instruct\"\n",
    "\n",
    "# LoRA adapters generados con el archivo LL_Fine_Tune_agave.ipynb\n",
    "lora_path = \"D:/LLM Models/agave_V002/phi-3-v2\" #\"D:/LLM Models/agave_V001/agave_baseline_phi3_V01\"   \n",
    "\n",
    "# 8 bits de cuantizacion\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "# Usamos el tokenizer de Phi-3\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a137390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementamos los pesos LoRA a Phi-3\n",
    "model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "\n",
    "# Colocamos el  modelo en evaluacion para que no cambie los pesos por cada prompt que\n",
    "# se le env√≠e\n",
    "model.eval()\n",
    "\n",
    "# Definimos el pipeline de nuevo\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.4,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6505b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(tokenizer, sentence):\n",
    "    converted_sample = [{\"role\": \"user\", \"content\": sentence}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        converted_sample, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbab8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_new_tokens=200, skip_special_tokens=True):\n",
    "    tokenized_input = tokenizer(\n",
    "        prompt, \n",
    "        add_special_tokens=False, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    bad_words = [\"Detective:\", \"Client:\", \"Good day\", \"Mr. Johnson\", \n",
    "                 \"Instrucci√≥n:\", \"Entrada:\", \"break-in\"]\n",
    "    bad_words_ids = [tokenizer.encode(word, add_special_tokens=False) \n",
    "                     for word in bad_words]\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        gen_output = model.generate(\n",
    "            **tokenized_input,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.1,  \n",
    "            top_p=0.85,       \n",
    "            top_k=40,        \n",
    "            repetition_penalty=1.15, \n",
    "            do_sample=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            bad_words_ids=bad_words_ids\n",
    "        )\n",
    "    \n",
    "    output = tokenizer.batch_decode(gen_output, skip_special_tokens=skip_special_tokens)\n",
    "    return output[0]\n",
    "\n",
    "# def generate(model, tokenizer, prompt, max_new_tokens=256, skip_special_tokens=False):\n",
    "  \n",
    "#   tokenized_input = tokenizer(\n",
    "#   prompt, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "#   model.eval()\n",
    "#   gen_output = model.generate(**tokenized_input,\n",
    "#   eos_token_id=tokenizer.eos_token_id,\n",
    "#   max_new_tokens=max_new_tokens)\n",
    "#   output = tokenizer.batch_decode(gen_output, skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "#   return output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49850fd8",
   "metadata": {},
   "source": [
    "# Sistemas RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8dbb04",
   "metadata": {},
   "source": [
    "#### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20a8e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    # model_name=\"intfloat/multilingual-e5-base\",\n",
    "    # encode_kwargs={\"normalize_embeddings\": True}\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={\n",
    "        'device': 'cuda'#,\n",
    "        #'trust_remote_code': True\n",
    "    },\n",
    "    encode_kwargs={\n",
    "        \"normalize_embeddings\": True,\n",
    "        \"batch_size\": 12\n",
    "    }\n",
    ")\n",
    "\n",
    "vs = FAISS.load_local(\n",
    "    #\"C:/Users/Delbert/Documents/Maestria/Proyecto Integrador/Avance 1/Tecnologico-Monterrey-Proyecto-Integrador-equipo-36/Baseline/rag_faiss_store\",\n",
    "    \"C:/Users/Delbert/Documents/Maestria/Proyecto Integrador/Avance 1/Tecnologico-Monterrey-Proyecto-Integrador-equipo-36/Telegram-Bot/rag_faiss_store\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "print(\"Total docs:\", len(vs.docstore._dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6254b7ba",
   "metadata": {},
   "source": [
    "### Modelo Phi-3 como un custom LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eebc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phi3ChatTemplateLLM(LLM):\n",
    "    model: Any = Field(exclude=True)\n",
    "    tokenizer: Any = Field(exclude=True)\n",
    "    max_new_tokens: int = 256\n",
    "    skip_special_tokens: bool = True\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"phi3-chat-template\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs: Any) -> str:\n",
    "        \n",
    "        chat_prompt = gen_prompt(self.tokenizer, prompt)\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            out = generate(\n",
    "                self.model,\n",
    "                self.tokenizer,\n",
    "                chat_prompt,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                skip_special_tokens=self.skip_special_tokens\n",
    "            )\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "custom_llm = Phi3ChatTemplateLLM(model=model, tokenizer=tokenizer, max_new_tokens=256, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_prompt = PromptTemplate(\n",
    "#     input_variables=[\"context\", \"question\"],\n",
    "#     template=(\n",
    "# \"\"\"Eres GorgoBot, asistente experto en picudo del agave para CNIT.\n",
    "\n",
    "# REGLAS ESTRICTAS:\n",
    "# 1. Responde SOLO con informaci√≥n del CONTEXTO\n",
    "# 2. Si no est√° en el CONTEXTO: \"No cuento con esa informaci√≥n espec√≠fica\"\n",
    "# 3. M√°ximo 3 oraciones concisas\n",
    "# 4. NO inventes n√∫meros, fechas ni estad√≠sticas\n",
    "# 5. NO generes di√°logos ni conversaciones ficticias\n",
    "# 6. Usa lenguaje simple para agricultores\n",
    "\n",
    "# CONTEXTO DISPONIBLE:\n",
    "# {context}\n",
    "\n",
    "# PREGUNTA: {question}\n",
    "\n",
    "# RESPUESTA DE ASISTENTE (m√°ximo 3 oraciones):\"\"\"\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "my_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "\"\"\"Eres GorgoBot, asistente experto en picudo del agave para CNIT.\n",
    "\n",
    "REGLAS ESTRICTAS:\n",
    "1. Responde SOLO con informaci√≥n del CONTEXTO\n",
    "2. Si no est√° en el CONTEXTO: \"No cuento con esa informaci√≥n espec√≠fica\"\n",
    "3. M√°ximo 3 oraciones concisas\n",
    "4. NO inventes n√∫meros, fechas ni estad√≠sticas\n",
    "5. NO generes di√°logos ni conversaciones ficticias\n",
    "6. Usa lenguaje simple para agricultores\n",
    "\n",
    "CONTEXTO DISPONIBLE:\n",
    "{context}\n",
    "\n",
    "PREGUNTA: {question}\n",
    "\n",
    "RESPUESTA DE ASISTENTE (m√°ximo 3 oraciones):\"\"\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=custom_llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": my_prompt},\n",
    "    return_source_documents=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575014f9",
   "metadata": {},
   "source": [
    "# Conexi√≥n a base de datos de reportes en GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d27969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos variables de entorno\n",
    "load_dotenv(dotenv_path=\"chatbot.env\")\n",
    "\n",
    "# Guardamos variables de entorno para conectarnos a la BDD\n",
    "USER = os.getenv('USER_DB')\n",
    "PASSWORD = os.getenv('PASSWORD_DB')\n",
    "HOST = os.getenv('HOST_DB')\n",
    "DBNAME = 'postgres'#os.getenv('DBNAME')\n",
    "PORT = 5432\n",
    "\n",
    "# Instanciamos el engine para conectarnos a nuestra BDD en GCP\n",
    "engine = create_engine(f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DBNAME}\")\n",
    "\n",
    "# Confirmamos que nos podamos conectar\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        print(\"‚úÖ Conexi√≥n exitosa con super_user\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error de conexi√≥n:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20660094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_cases_count():\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            total_cases = conn.execute(text(\"SELECT COUNT(*) FROM reportes_chatbot;\")).scalar()\n",
    "        return total_cases\n",
    "\n",
    "    except:\n",
    "        print(\"Error conect√°ndose a la BDD.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_report_into_bdd(data):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        query = text(\"\"\"\n",
    "        INSERT INTO reportes_chatbot (\n",
    "            report_id,\n",
    "            case_number,\n",
    "            lat,\n",
    "            lon,\n",
    "            address,\n",
    "            photo_url,\n",
    "            user_risk_assesment,\n",
    "            user_lot_description,\n",
    "            contact_phone,\n",
    "            price,\n",
    "            current_state\n",
    "        ) VALUES (\n",
    "            :report_id,\n",
    "            :case_number,\n",
    "            :lat,\n",
    "            :lon,\n",
    "            :address,\n",
    "            :photo_url,\n",
    "            :user_risk_assesment,\n",
    "            :user_lot_description,\n",
    "            :phone_number,\n",
    "            :price,\n",
    "            'Nuevo'\n",
    "        );\"\"\")\n",
    "        \n",
    "        try:\n",
    "            with engine.begin() as conn:\n",
    "                conn.execute(query, data)\n",
    "            print(f\"‚úÖ Reporte insertado: {data.get('case_number')} ({data.get('report_id')})\")\n",
    "            return True, data.get('case_number')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error al insertar el reporte {data.get('case_number')}: {e}\")\n",
    "            return False\n",
    "        \n",
    "    except:\n",
    "        print(\"‚ùå No se pudo insertar el reporte a la BDD, se procede a desechar.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dce06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_case_state(case_suffix: str):\n",
    "\n",
    "    # Aseguramos el formato correcto con el prefijo 'CASO-'\n",
    "    case_number = f\"CASO-{case_suffix.strip()}\"\n",
    "\n",
    "    query = text(\"\"\"\n",
    "        SELECT current_state\n",
    "        FROM reportes_chatbot\n",
    "        WHERE case_number = :case_number;\n",
    "    \"\"\")\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(query, {\"case_number\": case_number}).fetchone()\n",
    "\n",
    "            if result is None:\n",
    "                return False, None\n",
    "            else:\n",
    "                return True, result[0]\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è Error al consultar la base de datos: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac2fe1",
   "metadata": {},
   "source": [
    "# Conexi√≥n a Telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca30cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = os.getenv(\"TELEGRAM_BOT_TOKEN\")\n",
    "BASE_URL = f\"https://api.telegram.org/bot{TOKEN}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd254023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario que guardara la informacion temporalmente\n",
    "# antes de escribirla a la BDD\n",
    "user_session = {}\n",
    "case_counter = get_total_cases_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1227a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_menu(chat_id):\n",
    "\n",
    "    menu_message = (\n",
    "        \"üëã *Bienvenido, ser√° un gusto atenderte.*\\n\\n\"\n",
    "        \"¬øQu√© te interesa llevar a cabo?\\n\\n\"\n",
    "        \"A. *Hacer un reporte de un predio infectado*\\n\\n\"\n",
    "        \"B. *Preguntar por el seguimiento a un reporte anterior.*\\n\\n\"\n",
    "        \"C. *Preguntar generalidades sobre el muestreo de gorgojos.*\\n\\n\"\n",
    "        \"_Actualmente, solo estas 3 opciones est√°n disponibles._\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"chat_id\": chat_id,\n",
    "        \"text\": menu_message,\n",
    "        \"parse_mode\": \"Markdown\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/sendMessage\", json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úÖ Men√∫ enviado correctamente a {chat_id}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Error enviando men√∫: {response.text}\")\n",
    "            \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al enviar men√∫: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d54ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_message_flow(sender_id, state, message_data):\n",
    "\n",
    "    # Inicializamos variables\n",
    "    response_text = \"\"\n",
    "    next_state = state\n",
    "    \n",
    "    current_case_state_dict = {\n",
    "        \"Nuevo\": \" el caso fue recibido y ser√° investigado por un experto en control de plagas de picudo.\",\n",
    "        \"En proceso\" : \" el caso fue asignado a un experto en control de plagas de picudo.\",\n",
    "        \"En campo\"   : \" el caso se encuentra siendo investigado en el sitio reportado\",\n",
    "        \"Falsa alerta\" : \" el caso se cataloga como una falsa alerta con base en la investigaci√≥n realizada.\",\n",
    "        \"Verdadera alerta\" : \" se confirma una infestaci√≥n de picudo y se tomaron acciones adecuadas para erradicarlo.\",\n",
    "        \"Concluido\" : \" se llev√≥ a cabo la investigaci√≥n, erradicaci√≥n y control del picudo.\"\n",
    "    }\n",
    "    \n",
    "    # Detectamos si el usuario ha compartido su numero de telefono\n",
    "    contact = message_data.get(\"contact\", {})\n",
    "    phone_number = contact.get(\"phone_number\")\n",
    "\n",
    "    # Detectamos el contenido del mensaje\n",
    "    text = message_data.get(\"text\", \"\").strip().lower() if \"text\" in message_data else \"\"\n",
    "    location = message_data.get(\"location\", {})\n",
    "    photo = message_data.get(\"photo\", [])\n",
    "    lat = location.get(\"latitude\")\n",
    "    lon = location.get(\"longitude\")\n",
    "    \n",
    "    # Guardamos el contacto si este es publico\n",
    "    if phone_number:\n",
    "        user_session[sender_id][\"data\"][\"phone_number\"] = phone_number\n",
    "\n",
    "\n",
    "    # Extraemos la URL de la foto (si existe)\n",
    "    photo_url = None\n",
    "    if photo:\n",
    "        file_id = photo[-1][\"file_id\"]  # La √∫ltima suele ser la de mejor resoluci√≥n\n",
    "        photo_url = f\"https://api.telegram.org/file/bot{TOKEN}/{file_id}\"\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # Saludo inicial para reportar un predio con problemas\n",
    "    # ============================================================\n",
    "    \n",
    "    if state == \"saludo\":\n",
    "        if text in [\"a\", \"üÖ∞Ô∏è\"]:\n",
    "            response_text = (\n",
    "                \"Gracias por tu proactividad üåæ.\\n Primero, env√≠a la **ubicaci√≥n** del lote afectado usando el √≠cono üìç.\"\n",
    "            )\n",
    "            next_state = \"ask-location\"\n",
    "\n",
    "        elif text in [\"b\", \"üÖ±Ô∏è\"]:\n",
    "            response_text = (\n",
    "                \"Claro, por favor ind√≠came tu n√∫mero de caso.\\n\\nEl formato del caso debe contener el a√±o y n√∫mero de reporte.\\n\"\n",
    "                \"\\nEste es un ejemplo de un n√∫mero de caso v√°lido: 2025-0001\"\n",
    "            )\n",
    "            next_state = \"ask-case-file\"\n",
    "            \n",
    "        elif text in [\"c\"]:\n",
    "            response_text = (\n",
    "                \"Perfecto. Puedes escribir hasta 3 preguntas y te responder√© con ayuda del asistente inteligente.\"\n",
    "            )\n",
    "            next_state = \"NLP-Chatbot-1\"\n",
    "\n",
    "    # ============================================================\n",
    "    # Preguntamos por el n√∫mero de caso en caso de que se haya seleccionado esa opcion.\n",
    "    # ============================================================\n",
    "    elif state == \"ask-case-file\":\n",
    "        regex_pattern = r\"^(20\\d{2})-(\\d{4})$\"\n",
    "        \n",
    "        if text:\n",
    "            text = text.strip()\n",
    "            valid_case_format = re.match(regex_pattern, text)\n",
    "            \n",
    "            if valid_case_format:\n",
    "                \n",
    "                case_success, current_case_state = get_case_state(text)\n",
    "                \n",
    "                if not case_success:\n",
    "                    response_text = (f\"No existe un caso con el correlativo {text} en nuetra base de datos.\\n\\n\"\n",
    "                                      \"Por lo anterior, te instamos a realizar un reporte si conoces alguna situaci√≥n o predio que \\n\"\n",
    "                                      \"presente un riesgo de infestaci√≥n.\")\n",
    "                    \n",
    "                    next_state = 'restart'\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    description = current_case_state_dict.get(\n",
    "                            current_case_state, \n",
    "                            \"no se tiene una descripci√≥n disponible para este estado.\"\n",
    "                        )\n",
    "                    response_text = (f\"El caso {text} se encuentra en el estado: {current_case_state} \"\n",
    "                                     f\"\\n\\nEsto significa que {description}\\n\\nPara reiniciar el chat, escribe alg√∫n texto de nuevo, por favor.\")\n",
    "                    \n",
    "                    next_state = 'restart'\n",
    "                \n",
    "            else:\n",
    "                response_text = (\"No reconozco ese formato de caso.\\nPor favor, env√≠a tu caso usando el formato `a√±o`-`# de caso`.\\n\\nEjemplo: 2025-0001\")\n",
    "        \n",
    "        else:\n",
    "            response_text = \"‚ö†Ô∏è No detect√© un n√∫mero de caso. Por favor, env√≠a el n√∫mero siguiendo el formato 2025-<Numero de caso en 4 digitos>.\"\n",
    "        \n",
    "    # ============================================================\n",
    "    # Preguntamos por la ubicaci√≥n. Si esta se env√≠a, generamos un n√∫mero de caso unico y un\n",
    "    # numero de caso relativo para evitar duplicar reportes\n",
    "    # ============================================================\n",
    "    \n",
    "    elif state == \"ask-location\":\n",
    "        if lat and lon:\n",
    "            \n",
    "            # Usaremos el contador global de casos actuales en la BDD\n",
    "            case_counter = get_total_cases_count() + 1\n",
    "\n",
    "            # Generamos un id de reporte unico al tener ubicacion\n",
    "            report_id = f\"report_{uuid.uuid4()}\"\n",
    "            user_session[sender_id][\"data\"][\"report_id\"] = report_id\n",
    "\n",
    "            # Generamos el n√∫mero de caso legible\n",
    "            year = datetime.now().year\n",
    "            case_number = f\"CASO-{year}-{case_counter:04d}\"\n",
    "            user_session[sender_id][\"data\"][\"case_number\"] = case_number\n",
    "            \n",
    "            # Guardamos la ubicacion de latitud y longitud\n",
    "            user_session[sender_id][\"data\"][\"lat\"] = lat\n",
    "            user_session[sender_id][\"data\"][\"lon\"] = lon\n",
    "            \n",
    "            response_text = (\n",
    "                \"üìç *Ubicaci√≥n recibida correctamente.*\\n\\n Ahora, por favor env√≠a una **foto** del lote o planta afectada. üì∏\"\n",
    "            )\n",
    "            next_state = \"ask_photo\"\n",
    "        else:\n",
    "            response_text = (\n",
    "                \"‚ö†Ô∏è No detect√© una ubicaci√≥n v√°lida. Por favor, usa el bot√≥n de üìç para enviarla correctamente.\"\n",
    "            )\n",
    "            \n",
    "    # ============================================================\n",
    "    # Pedimos una fotograf√≠a sobre lo reportado por el usuario\n",
    "    # ============================================================\n",
    "    elif state == \"ask_photo\":\n",
    "        if photo_url:\n",
    "            user_session[sender_id][\"data\"][\"photo_url\"] = photo_url\n",
    "            response_text = (\n",
    "                \"üì∏ *Foto recibida correctamente.*\\n\" \n",
    "                \"Por favor, ahora describe las condiciones del predio. Es de ayuda conocer si el predio se encuentra\\n\\n\"\n",
    "                \"1. Abandonado\\n2. En mal estado.\\n3. Infestado por picudos.\\n\"\n",
    "                \"\\nCualquier informacion adicional en tu descripci√≥n es de ayuda \"\n",
    "            )\n",
    "            next_state = \"ask_lot_description\"\n",
    "        else:\n",
    "            response_text = \"‚ö†Ô∏è No se detect√≥ una foto. Por favor env√≠a una imagen del lote afectado.\"\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # Luego preguntamos por una descripci√≥n detallada del usuario\n",
    "    # ============================================================\n",
    "    elif state == \"ask_lot_description\":    \n",
    "        if text:\n",
    "            user_session[sender_id][\"data\"][\"user_lot_description\"] = text\n",
    "            \n",
    "            response_text = (\n",
    "                \"Gracias por tu descripci√≥n del predio. Ahora, clasifica el riesgo que representa para t√≠ este predio de alguna de las siguientes formas:\\n\"\n",
    "                \"1. Alto\\n2. Medio\\n3. Bajo\"\n",
    "            )\n",
    "            \n",
    "            next_state = 'ask_risk'\n",
    "        \n",
    "        else:\n",
    "            response_text = \"‚ö†Ô∏è No se detect√≥ una respuesta v√°lida. Por favor, describe el estado del predio usando oraciones completas.\"\n",
    "    \n",
    "    # ============================================================\n",
    "    # Ahora preguntamos el riesgo que el usuario percibe\n",
    "    # ============================================================\n",
    "    elif state == \"ask_risk\":\n",
    "        \n",
    "        if text in [\"alta\", \"media\", \"baja\", \"alto\", \"medio\", \"bajo\", \"1\", \"2\", \"3\"]:\n",
    "            \n",
    "            if text in [\"1\"]:\n",
    "                text = \"Alto\"\n",
    "            if text in [\"2\"]:\n",
    "                text = \"Medio\"\n",
    "            if text in [\"3\"]:\n",
    "                text = \"Bajo\"\n",
    "            \n",
    "            user_session[sender_id][\"data\"][\"user_risk_assesment\"] = text.capitalize()\n",
    "            data = user_session[sender_id][\"data\"]\n",
    "            \n",
    "            response_text = (\n",
    "                f\"‚úÖ Gracias por tu reporte.\\n\"\n",
    "                f\"üìç Ubicaci√≥n: ({data['lat']}, {data['lon']})\\n\"\n",
    "                f\"üì∏ Foto: Confirmada\\n\"\n",
    "                f\"üö® Riesgo: {data['user_risk_assesment']}\\n\"\n",
    "                f\"üí¨ Descripci√≥n: Recibida\\n\\n\"\n",
    "                \"¬øEsta informaci√≥n es correcta? Responde con *S√≠* o *No*.\"\n",
    "            )\n",
    "            next_state = \"confirmation\"\n",
    "        else:\n",
    "            response_text = \"Por favor indica el nivel de riesgo como: Alta, Media o Baja.\"\n",
    "\n",
    "    # ============================================================\n",
    "    # Ahora informamos sobre el reporte dando el correlativo de cada caso\n",
    "    # ============================================================\n",
    "    elif state == \"confirmation\":\n",
    "        \n",
    "        # Si el reporte es correcto, lo confirmamos\n",
    "        if re.search(r'^\\s*s[i√≠]\\s*$', text):\n",
    "        \n",
    "            response_text = \"üåæ Tu reporte ya fue registrado. ¬°Gracias por tu colaboraci√≥n!\"\n",
    "            next_state = \"restart\"\n",
    "            \n",
    "            inserted, case = insert_report_into_bdd(user_session[sender_id][\"data\"])\n",
    "            \n",
    "            if inserted:\n",
    "                response_text = response_text + f\"\\n\\nAdicionalmente, se cre√≥ el caso {case} por si quisieras consultar el estado del mismo.\"\n",
    "            \n",
    "        # Si nos dice que no es correcto, desechamos el reporte\n",
    "        elif re.search(r'^\\s*no\\s*$', text):\n",
    "            response_text = \"‚ùå Entendido. Tu reporte no se registrar√°.\\nPara repetir el reporte, o hacer otro, puedes volver a escribir sobre este mismo chat.\"\n",
    "            next_state = \"restart\"\n",
    "        \n",
    "        # Si nos responde algo que no sabemos que es\n",
    "        else:\n",
    "            response_text = \"Por favor responde √∫nicamente con *S√≠* o *No*.\"\n",
    "            \n",
    "\n",
    "    # Else que solo existe por si no capturamos la logica de la respuesta\n",
    "    else:\n",
    "        response_text = \"No entend√≠ tu respuesta. Por favor intenta nuevamente.\"\n",
    "\n",
    "    return response_text, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a830eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_message(chat_id, text):\n",
    "    \"\"\"\n",
    "    Env√≠a un mensaje de texto al usuario usando la API de Telegram.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"chat_id\": chat_id,\n",
    "        \"text\": text,\n",
    "        \"parse_mode\": \"Markdown\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/sendMessage\", json=payload)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ö†Ô∏è Error enviando mensaje: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al enviar mensaje: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta funcion la usamos porque telegram necesita que hagamos scape \n",
    "# de algunos caracteres como puntos, signos de exclamacion, etc.\n",
    "def escape_markdown_v2(text):\n",
    "    return re.sub(r'([_*\\[\\]()~`>#+\\-=|{}.!])', r'\\\\\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c546dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "def process_rag_async(chat_id, query):\n",
    "\n",
    "    try:\n",
    "        response = rag_chain.invoke({\"query\": query})\n",
    "        raw_output = response[\"result\"]\n",
    "        \n",
    "        if \"RESPUESTA DE ASISTENTE:\" in raw_output:\n",
    "            clean_output = raw_output.split(\"RESPUESTA DE ASISTENTE:\")[-1].strip()\n",
    "        else:\n",
    "            clean_output = raw_output.strip()\n",
    "        \n",
    "        send_message(chat_id, escape_markdown_v2(clean_output))\n",
    "        \n",
    "        # Actualizar estado\n",
    "        if user_session[chat_id][\"state\"] == \"NLP-Chatbot-3\":\n",
    "            send_message(chat_id, \"Has llegado al l√≠mite de consultars consecutivos. En estos momentos se reiniciar√° el chat. Gracias.\")\n",
    "            user_session[chat_id][\"state\"] = \"restart\"\n",
    "        elif user_session[chat_id][\"state\"] == \"NLP-Chatbot-2\":\n",
    "            user_session[chat_id][\"state\"] = \"NLP-Chatbot-3\"\n",
    "        elif user_session[chat_id][\"state\"] == \"NLP-Chatbot-1\":\n",
    "            user_session[chat_id][\"state\"] = \"NLP-Chatbot-2\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en RAG: {e}\")\n",
    "        send_message(chat_id, \"Lo siento, ocurri√≥ un error procesando tu consulta.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/webhook\", methods=[\"POST\"])\n",
    "def telegram_webhook():\n",
    "    \"\"\"\n",
    "    Webhook principal para manejar mensajes entrantes desde Telegram.\n",
    "    Procesa texto, fotos y ubicaciones, mantiene el flujo de conversaci√≥n\n",
    "    y responde usando la API de Telegram.\n",
    "    \"\"\"\n",
    "    data = request.get_json()\n",
    "\n",
    "    if \"message\" not in data:\n",
    "        return \"ok\", 200\n",
    "\n",
    "    message = data[\"message\"]\n",
    "    chat_id = message[\"chat\"][\"id\"]\n",
    "    username = message[\"chat\"].get(\"username\", \"Desconocido\")\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Mensaje recibido de @{username} | chat_id={chat_id}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Si es la primera vez que el usuario escribe\n",
    "    if (chat_id not in user_session) or (user_session[chat_id][\"state\"] == \"restart\"):\n",
    "        print(\"Enviando menu al usuario\")\n",
    "        user_session[chat_id] = {\n",
    "            \"state\": \"saludo\",\n",
    "            \"data\": {\n",
    "                \"lat\": None,\n",
    "                \"lon\": None,\n",
    "                \"address\": None,\n",
    "                \"photo_url\": None,\n",
    "                \"user_risk_assesment\": None,\n",
    "                \"user_lot_description\": None,\n",
    "                \"report_id\" : None,\n",
    "                \"case_number\" : None,\n",
    "                \"phone_number\": None,\n",
    "                \"price\": None\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Enviamos el men√∫ inicial\n",
    "        send_menu(chat_id)\n",
    "        return \"ok\", 200\n",
    "\n",
    "    # Si el usuario se encuentra en modo Chatbot (NLP)\n",
    "    # Si est√° en modo chatbot\n",
    "    if user_session[chat_id][\"state\"] in [\"NLP-Chatbot-1\", \"NLP-Chatbot-2\", \"NLP-Chatbot-3\"]:\n",
    "        print(\"ü§ñ Enviando consulta al RAG Chatbot...\")\n",
    "        query = message.get(\"text\", \"\")\n",
    "\n",
    "        thread = Thread(target=process_rag_async, args=(chat_id, query))\n",
    "        thread.daemon = True\n",
    "        thread.start()\n",
    "        \n",
    "        # Responder INMEDIATAMENTE a Telegram\n",
    "        return \"ok\", 200\n",
    "\n",
    "    # En caso contrario, seguimos el flujo de reporte\n",
    "    current_state = user_session[chat_id][\"state\"]\n",
    "    response_text, next_state = report_message_flow(chat_id, current_state, message)\n",
    "    user_session[chat_id][\"state\"] = next_state\n",
    "\n",
    "    send_message(chat_id, response_text)\n",
    "    return \"ok\", 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c90e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tapp.run(port=3000, debug=False, use_reloader=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch311 (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
