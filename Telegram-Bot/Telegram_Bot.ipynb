{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032c6c4b",
   "metadata": {},
   "source": [
    "# Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd1accd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import Field\n",
    "from sqlalchemy import create_engine, text\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from typing import Optional, List, Any\n",
    "from langchain.schema import Document\n",
    "from transformers import pipeline\n",
    "from flask import Flask, request\n",
    "from datetime import datetime\n",
    "from peft import PeftModel\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import requests\n",
    "import uuid\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a7754b",
   "metadata": {},
   "source": [
    "# Microsoft Phi-3 como modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169ee809",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "405cbe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# Modelo local \n",
    "model_path = \"D:/LLM Models/microsoft-phi-3-mini-4k-instruct\"\n",
    "\n",
    "# LoRA adapters generados con el archivo LL_Fine_Tune_agave.ipynb\n",
    "lora_path = \"D:/LLM Models/agave_V001/agave_baseline_phi3_V01\"   \n",
    "\n",
    "# 8 bits de cuantizacion\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "# Usamos el tokenizer de Phi-3\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a137390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Implementamos los pesos LoRA a Phi-3\n",
    "model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "\n",
    "# Colocamos el  modelo en evaluacion para que no cambie los pesos por cada prompt que\n",
    "# se le env√≠e\n",
    "model.eval()\n",
    "\n",
    "# Definimos el pipeline de nuevo\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=64,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6505b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(tokenizer, sentence):\n",
    "    converted_sample = [{\"role\": \"user\", \"content\": sentence}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        converted_sample, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acbab8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=256, skip_special_tokens=False):\n",
    "  \n",
    "  tokenized_input = tokenizer(\n",
    "  prompt, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "  model.eval()\n",
    "  gen_output = model.generate(**tokenized_input,\n",
    "  eos_token_id=tokenizer.eos_token_id,\n",
    "  max_new_tokens=max_new_tokens)\n",
    "  output = tokenizer.batch_decode(gen_output, skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "  return output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49850fd8",
   "metadata": {},
   "source": [
    "# Sistemas RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8dbb04",
   "metadata": {},
   "source": [
    "#### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e20a8e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Delbert\\AppData\\Local\\Temp\\ipykernel_7520\\2113877534.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 620196\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-base\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "vs = FAISS.load_local(\n",
    "    \"C:/Users/Delbert/Documents/Maestria/Proyecto Integrador/Avance 1/Tecnologico-Monterrey-Proyecto-Integrador-equipo-36/Baseline/rag_faiss_store\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "print(\"Total docs:\", len(vs.docstore._dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6254b7ba",
   "metadata": {},
   "source": [
    "### Modelo Phi-3 como un custom LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eebc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phi3ChatTemplateLLM(LLM):\n",
    "    model: Any = Field(exclude=True)\n",
    "    tokenizer: Any = Field(exclude=True)\n",
    "    max_new_tokens: int = 256\n",
    "    skip_special_tokens: bool = True\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"phi3-chat-template\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs: Any) -> str:\n",
    "        \n",
    "        chat_prompt = gen_prompt(self.tokenizer, prompt)\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            out = generate(\n",
    "                self.model,\n",
    "                self.tokenizer,\n",
    "                chat_prompt,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                skip_special_tokens=self.skip_special_tokens\n",
    "            )\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "custom_llm = Phi3ChatTemplateLLM(model=model, tokenizer=tokenizer, max_new_tokens=256, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea7b8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "       \"\"\"Eres un asistente experto en el picudo del agave (Scyphophorus acupunctatus) para CNIT. Tu objetivo es ayudar a productores y t√©cnicos con informaci√≥n pr√°ctica sobre esta plaga.\n",
    "\n",
    "INSTRUCCIONES CR√çTICAS:\n",
    "1. Responde √öNICAMENTE bas√°ndote en el CONTEXTO proporcionado\n",
    "2. Si el CONTEXTO no contiene la informaci√≥n: responde exactamente \"No cuento con esa informaci√≥n. ¬øPuedo ayudarte en algo m√°s?\"\n",
    "3. NUNCA inventes, supongas o uses conocimiento externo\n",
    "4. Mant√©n respuestas concisas con m√°ximo de 4 oraciones\n",
    "5. Usa lenguaje natural y accesible para agricultores\n",
    "6. NO menciones campos t√©cnicos como: trap_id, severity_encoded, fechas espec√≠ficas de base de datos\n",
    "7. Cuando hables de ubicaciones, usa nombres de municipios/estados, no coordenadas\n",
    "\n",
    "CONTEXTO DISPONIBLE:\n",
    "{context}\n",
    "\n",
    "PREGUNTA DEL USUARIO:\n",
    "{question}\n",
    "\n",
    "RESPUESTA DE ASISTENTE:\"\"\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=custom_llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": my_prompt},\n",
    "    return_source_documents=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575014f9",
   "metadata": {},
   "source": [
    "# Conexi√≥n a base de datos de reportes en GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d27969e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conexi√≥n exitosa con super_user\n"
     ]
    }
   ],
   "source": [
    "# Cargamos variables de entorno\n",
    "load_dotenv(dotenv_path=\"chatbot.env\")\n",
    "\n",
    "# Guardamos variables de entorno para conectarnos a la BDD\n",
    "USER = os.getenv('USER_DB')\n",
    "PASSWORD = os.getenv('PASSWORD_DB')\n",
    "HOST = os.getenv('HOST_DB')\n",
    "DBNAME = 'postgres'#os.getenv('DBNAME')\n",
    "PORT = 5432\n",
    "\n",
    "# Instanciamos el engine para conectarnos a nuestra BDD en GCP\n",
    "engine = create_engine(f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DBNAME}\")\n",
    "\n",
    "# Confirmamos que nos podamos conectar\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        print(\"‚úÖ Conexi√≥n exitosa con super_user\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error de conexi√≥n:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20660094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_cases_count():\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            total_cases = conn.execute(text(\"SELECT COUNT(*) FROM reportes_chatbot;\")).scalar()\n",
    "        return total_cases\n",
    "\n",
    "    except:\n",
    "        print(\"Error conect√°ndose a la BDD.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f7d12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_report_into_bdd(data):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        query = text(\"\"\"\n",
    "        INSERT INTO reportes_chatbot (\n",
    "            report_id,\n",
    "            case_number,\n",
    "            lat,\n",
    "            lon,\n",
    "            address,\n",
    "            photo_url,\n",
    "            user_risk_assesment,\n",
    "            user_lot_description,\n",
    "            contact_phone,\n",
    "            price,\n",
    "            current_state\n",
    "        ) VALUES (\n",
    "            :report_id,\n",
    "            :case_number,\n",
    "            :lat,\n",
    "            :lon,\n",
    "            :address,\n",
    "            :photo_url,\n",
    "            :user_risk_assesment,\n",
    "            :user_lot_description,\n",
    "            :phone_number,\n",
    "            :price,\n",
    "            'Nuevo'\n",
    "        );\"\"\")\n",
    "        \n",
    "        try:\n",
    "            with engine.begin() as conn:\n",
    "                conn.execute(query, data)\n",
    "            print(f\"‚úÖ Reporte insertado: {data.get('case_number')} ({data.get('report_id')})\")\n",
    "            return True, data.get('case_number')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error al insertar el reporte {data.get('case_number')}: {e}\")\n",
    "            return False\n",
    "        \n",
    "    except:\n",
    "        print(\"‚ùå No se pudo insertar el reporte a la BDD, se procede a desechar.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9dce06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_case_state(case_suffix: str):\n",
    "\n",
    "    # Aseguramos el formato correcto con el prefijo 'CASO-'\n",
    "    case_number = f\"CASO-{case_suffix.strip()}\"\n",
    "\n",
    "    query = text(\"\"\"\n",
    "        SELECT current_state\n",
    "        FROM reportes_chatbot\n",
    "        WHERE case_number = :case_number;\n",
    "    \"\"\")\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(query, {\"case_number\": case_number}).fetchone()\n",
    "\n",
    "            if result is None:\n",
    "                return False, None\n",
    "            else:\n",
    "                return True, result[0]\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è Error al consultar la base de datos: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac2fe1",
   "metadata": {},
   "source": [
    "# Conexi√≥n a Telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cca30cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = os.getenv(\"TELEGRAM_BOT_TOKEN\")\n",
    "BASE_URL = f\"https://api.telegram.org/bot{TOKEN}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd254023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario que guardara la informacion temporalmente\n",
    "# antes de escribirla a la BDD\n",
    "user_session = {}\n",
    "case_counter = get_total_cases_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1227a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_menu(chat_id):\n",
    "\n",
    "    menu_message = (\n",
    "        \"üëã *Bienvenido, ser√° un gusto atenderte.*\\n\\n\"\n",
    "        \"¬øQu√© te interesa llevar a cabo?\\n\\n\"\n",
    "        \"A. *Hacer un reporte de un predio infectado*\\n\\n\"\n",
    "        \"B. *Preguntar por el seguimiento a un reporte anterior.*\\n\\n\"\n",
    "        \"C. *Preguntar generalidades sobre el muestreo de gorgojos.*\\n\\n\"\n",
    "        \"_Actualmente, solo estas 3 opciones est√°n disponibles._\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"chat_id\": chat_id,\n",
    "        \"text\": menu_message,\n",
    "        \"parse_mode\": \"Markdown\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/sendMessage\", json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úÖ Men√∫ enviado correctamente a {chat_id}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Error enviando men√∫: {response.text}\")\n",
    "            \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al enviar men√∫: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b5d54ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_message_flow(sender_id, state, message_data):\n",
    "\n",
    "    # Inicializamos variables\n",
    "    response_text = \"\"\n",
    "    next_state = state\n",
    "    \n",
    "    current_case_state_dict = {\n",
    "        \"Nuevo\": \" el caso fue recibido y ser√° investigado por un experto en control de plagas de picudo.\",\n",
    "        \"En proceso\" : \" el caso fue asignado a un experto en control de plagas de picudo.\",\n",
    "        \"En campo\"   : \" el caso se encuentra siendo investigado en el sitio reportado\",\n",
    "        \"Falsa alerta\" : \" el caso se cataloga como una falsa alerta con base en la investigaci√≥n realizada.\",\n",
    "        \"Verdadera alerta\" : \" se confirma una infestaci√≥n de picudo y se tomaron acciones adecuadas para erradicarlo.\",\n",
    "        \"Concluido\" : \" se llev√≥ a cabo la investigaci√≥n, erradicaci√≥n y control del picudo.\"\n",
    "    }\n",
    "    \n",
    "    # Detectamos si el usuario ha compartido su numero de telefono\n",
    "    contact = message_data.get(\"contact\", {})\n",
    "    phone_number = contact.get(\"phone_number\")\n",
    "\n",
    "    # Detectamos el contenido del mensaje\n",
    "    text = message_data.get(\"text\", \"\").strip().lower() if \"text\" in message_data else \"\"\n",
    "    location = message_data.get(\"location\", {})\n",
    "    photo = message_data.get(\"photo\", [])\n",
    "    lat = location.get(\"latitude\")\n",
    "    lon = location.get(\"longitude\")\n",
    "    \n",
    "    # Guardamos el contacto si este es publico\n",
    "    if phone_number:\n",
    "        user_session[sender_id][\"data\"][\"phone_number\"] = phone_number\n",
    "\n",
    "\n",
    "    # Extraemos la URL de la foto (si existe)\n",
    "    photo_url = None\n",
    "    if photo:\n",
    "        file_id = photo[-1][\"file_id\"]  # La √∫ltima suele ser la de mejor resoluci√≥n\n",
    "        photo_url = f\"https://api.telegram.org/file/bot{TOKEN}/{file_id}\"\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # Saludo inicial para reportar un predio con problemas\n",
    "    # ============================================================\n",
    "    \n",
    "    if state == \"saludo\":\n",
    "        if text in [\"a\", \"üÖ∞Ô∏è\"]:\n",
    "            response_text = (\n",
    "                \"Gracias por tu proactividad üåæ.\\n Primero, env√≠a la **ubicaci√≥n** del lote afectado usando el √≠cono üìç.\"\n",
    "            )\n",
    "            next_state = \"ask-location\"\n",
    "\n",
    "        elif text in [\"b\", \"üÖ±Ô∏è\"]:\n",
    "            response_text = (\n",
    "                \"Claro, por favor ind√≠came tu n√∫mero de caso.\\n\\nEl formato del caso debe contener el a√±o y n√∫mero de reporte.\\n\"\n",
    "                \"\\nEste es un ejemplo de un n√∫mero de caso v√°lido: 2025-0001\"\n",
    "            )\n",
    "            next_state = \"ask-case-file\"\n",
    "            \n",
    "        elif text in [\"c\"]:\n",
    "            response_text = (\n",
    "                \"Perfecto. Puedes escribir hasta 3 preguntas y te responder√© con ayuda del asistente inteligente.\"\n",
    "            )\n",
    "            next_state = \"NLP-Chatbot-1\"\n",
    "\n",
    "    # ============================================================\n",
    "    # Preguntamos por el n√∫mero de caso en caso de que se haya seleccionado esa opcion.\n",
    "    # ============================================================\n",
    "    elif state == \"ask-case-file\":\n",
    "        regex_pattern = r\"^(20\\d{2})-(\\d{4})$\"\n",
    "        \n",
    "        if text:\n",
    "            text = text.strip()\n",
    "            valid_case_format = re.match(regex_pattern, text)\n",
    "            \n",
    "            if valid_case_format:\n",
    "                \n",
    "                case_success, current_case_state = get_case_state(text)\n",
    "                \n",
    "                if not case_success:\n",
    "                    response_text = (f\"No existe un caso con el correlativo {text} en nuetra base de datos.\\n\\n\"\n",
    "                                      \"Por lo anterior, te instamos a realizar un reporte si conoces alguna situaci√≥n o predio que \\n\"\n",
    "                                      \"presente un riesgo de infestaci√≥n.\")\n",
    "                    \n",
    "                    next_state = 'restart'\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    description = current_case_state_dict.get(\n",
    "                            current_case_state, \n",
    "                            \"no se tiene una descripci√≥n disponible para este estado.\"\n",
    "                        )\n",
    "                    response_text = (f\"El caso {text} se encuentra en el estado: {current_case_state} \"\n",
    "                                     f\"\\n\\nEsto significa que {description}\\n\\nPara reiniciar el chat, escribe alg√∫n texto de nuevo, por favor.\")\n",
    "                    \n",
    "                    next_state = 'restart'\n",
    "                \n",
    "            else:\n",
    "                response_text = (\"No reconozco ese formato de caso.\\nPor favor, env√≠a tu caso usando el formato `a√±o`-`# de caso`.\\n\\nEjemplo: 2025-0001\")\n",
    "        \n",
    "        else:\n",
    "            response_text = \"‚ö†Ô∏è No detect√© un n√∫mero de caso. Por favor, env√≠a el n√∫mero siguiendo el formato 2025-<Numero de caso en 4 digitos>.\"\n",
    "        \n",
    "    # ============================================================\n",
    "    # Preguntamos por la ubicaci√≥n. Si esta se env√≠a, generamos un n√∫mero de caso unico y un\n",
    "    # numero de caso relativo para evitar duplicar reportes\n",
    "    # ============================================================\n",
    "    \n",
    "    elif state == \"ask-location\":\n",
    "        if lat and lon:\n",
    "            \n",
    "            # Usaremos el contador global de casos actuales en la BDD\n",
    "            case_counter = get_total_cases_count() + 1\n",
    "\n",
    "            # Generamos un id de reporte unico al tener ubicacion\n",
    "            report_id = f\"report_{uuid.uuid4()}\"\n",
    "            user_session[sender_id][\"data\"][\"report_id\"] = report_id\n",
    "\n",
    "            # Generamos el n√∫mero de caso legible\n",
    "            year = datetime.now().year\n",
    "            case_number = f\"CASO-{year}-{case_counter:04d}\"\n",
    "            user_session[sender_id][\"data\"][\"case_number\"] = case_number\n",
    "            \n",
    "            # Guardamos la ubicacion de latitud y longitud\n",
    "            user_session[sender_id][\"data\"][\"lat\"] = lat\n",
    "            user_session[sender_id][\"data\"][\"lon\"] = lon\n",
    "            \n",
    "            response_text = (\n",
    "                \"üìç *Ubicaci√≥n recibida correctamente.*\\n\\n Ahora, por favor env√≠a una **foto** del lote o planta afectada. üì∏\"\n",
    "            )\n",
    "            next_state = \"ask_photo\"\n",
    "        else:\n",
    "            response_text = (\n",
    "                \"‚ö†Ô∏è No detect√© una ubicaci√≥n v√°lida. Por favor, usa el bot√≥n de üìç para enviarla correctamente.\"\n",
    "            )\n",
    "            \n",
    "    # ============================================================\n",
    "    # Pedimos una fotograf√≠a sobre lo reportado por el usuario\n",
    "    # ============================================================\n",
    "    elif state == \"ask_photo\":\n",
    "        if photo_url:\n",
    "            user_session[sender_id][\"data\"][\"photo_url\"] = photo_url\n",
    "            response_text = (\n",
    "                \"üì∏ *Foto recibida correctamente.*\\n\" \n",
    "                \"Por favor, ahora describe las condiciones del predio. Es de ayuda conocer si el predio se encuentra\\n\\n\"\n",
    "                \"1. Abandonado\\n2. En mal estado.\\n3. Infestado por picudos.\\n\"\n",
    "                \"\\nCualquier informacion adicional en tu descripci√≥n es de ayuda \"\n",
    "            )\n",
    "            next_state = \"ask_lot_description\"\n",
    "        else:\n",
    "            response_text = \"‚ö†Ô∏è No se detect√≥ una foto. Por favor env√≠a una imagen del lote afectado.\"\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # Luego preguntamos por una descripci√≥n detallada del usuario\n",
    "    # ============================================================\n",
    "    elif state == \"ask_lot_description\":    \n",
    "        if text:\n",
    "            user_session[sender_id][\"data\"][\"user_lot_description\"] = text\n",
    "            \n",
    "            response_text = (\n",
    "                \"Gracias por tu descripci√≥n del predio. Ahora, clasifica el riesgo que representa para t√≠ este predio de alguna de las siguientes formas:\\n\"\n",
    "                \"1. Alto\\n2. Medio\\n3. Bajo\"\n",
    "            )\n",
    "            \n",
    "            next_state = 'ask_risk'\n",
    "        \n",
    "        else:\n",
    "            response_text = \"‚ö†Ô∏è No se detect√≥ una respuesta v√°lida. Por favor, describe el estado del predio usando oraciones completas.\"\n",
    "    \n",
    "    # ============================================================\n",
    "    # Ahora preguntamos el riesgo que el usuario percibe\n",
    "    # ============================================================\n",
    "    elif state == \"ask_risk\":\n",
    "        \n",
    "        if text in [\"alta\", \"media\", \"baja\", \"alto\", \"medio\", \"bajo\", \"1\", \"2\", \"3\"]:\n",
    "            \n",
    "            if text in [\"1\"]:\n",
    "                text = \"Alto\"\n",
    "            if text in [\"2\"]:\n",
    "                text = \"Medio\"\n",
    "            if text in [\"3\"]:\n",
    "                text = \"Bajo\"\n",
    "            \n",
    "            user_session[sender_id][\"data\"][\"user_risk_assesment\"] = text.capitalize()\n",
    "            data = user_session[sender_id][\"data\"]\n",
    "            \n",
    "            response_text = (\n",
    "                f\"‚úÖ Gracias por tu reporte.\\n\"\n",
    "                f\"üìç Ubicaci√≥n: ({data['lat']}, {data['lon']})\\n\"\n",
    "                f\"üì∏ Foto: Confirmada\\n\"\n",
    "                f\"üö® Riesgo: {data['user_risk_assesment']}\\n\"\n",
    "                f\"üí¨ Descripci√≥n: Recibida\\n\\n\"\n",
    "                \"¬øEsta informaci√≥n es correcta? Responde con *S√≠* o *No*.\"\n",
    "            )\n",
    "            next_state = \"confirmation\"\n",
    "        else:\n",
    "            response_text = \"Por favor indica el nivel de riesgo como: Alta, Media o Baja.\"\n",
    "\n",
    "    # ============================================================\n",
    "    # Ahora informamos sobre el reporte dando el correlativo de cada caso\n",
    "    # ============================================================\n",
    "    elif state == \"confirmation\":\n",
    "        \n",
    "        # Si el reporte es correcto, lo confirmamos\n",
    "        if re.search(r'^\\s*s[i√≠]\\s*$', text):\n",
    "        \n",
    "            response_text = \"üåæ Tu reporte ya fue registrado. ¬°Gracias por tu colaboraci√≥n!\"\n",
    "            next_state = \"restart\"\n",
    "            \n",
    "            inserted, case = insert_report_into_bdd(user_session[sender_id][\"data\"])\n",
    "            \n",
    "            if inserted:\n",
    "                response_text = response_text + f\"\\n\\nAdicionalmente, se cre√≥ el caso {case} por si quisieras consultar el estado del mismo.\"\n",
    "            \n",
    "        # Si nos dice que no es correcto, desechamos el reporte\n",
    "        elif re.search(r'^\\s*no\\s*$', text):\n",
    "            response_text = \"‚ùå Entendido. Tu reporte no se registrar√°.\\nPara repetir el reporte, o hacer otro, puedes volver a escribir sobre este mismo chat.\"\n",
    "            next_state = \"restart\"\n",
    "        \n",
    "        # Si nos responde algo que no sabemos que es\n",
    "        else:\n",
    "            response_text = \"Por favor responde √∫nicamente con *S√≠* o *No*.\"\n",
    "            \n",
    "\n",
    "    # Else que solo existe por si no capturamos la logica de la respuesta\n",
    "    else:\n",
    "        response_text = \"No entend√≠ tu respuesta. Por favor intenta nuevamente.\"\n",
    "\n",
    "    return response_text, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a830eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_message(chat_id, text):\n",
    "    \"\"\"\n",
    "    Env√≠a un mensaje de texto al usuario usando la API de Telegram.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"chat_id\": chat_id,\n",
    "        \"text\": text,\n",
    "        \"parse_mode\": \"Markdown\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/sendMessage\", json=payload)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ö†Ô∏è Error enviando mensaje: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al enviar mensaje: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6872930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta funcion la usamos porque telegram necesita que hagamos scape \n",
    "# de algunos caracteres como puntos, signos de exclamacion, etc.\n",
    "def escape_markdown_v2(text):\n",
    "    return re.sub(r'([_*\\[\\]()~`>#+\\-=|{}.!])', r'\\\\\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8244964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/webhook\", methods=[\"POST\"])\n",
    "def telegram_webhook():\n",
    "    \"\"\"\n",
    "    Webhook principal para manejar mensajes entrantes desde Telegram.\n",
    "    Procesa texto, fotos y ubicaciones, mantiene el flujo de conversaci√≥n\n",
    "    y responde usando la API de Telegram.\n",
    "    \"\"\"\n",
    "    data = request.get_json()\n",
    "\n",
    "    if \"message\" not in data:\n",
    "        return \"ok\", 200\n",
    "\n",
    "    message = data[\"message\"]\n",
    "    chat_id = message[\"chat\"][\"id\"]\n",
    "    username = message[\"chat\"].get(\"username\", \"Desconocido\")\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Mensaje recibido de @{username} | chat_id={chat_id}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Si es la primera vez que el usuario escribe\n",
    "    if (chat_id not in user_session) or (user_session[chat_id][\"state\"] == \"restart\"):\n",
    "        print(\"Enviando menu al usuario\")\n",
    "        user_session[chat_id] = {\n",
    "            \"state\": \"saludo\",\n",
    "            \"data\": {\n",
    "                \"lat\": None,\n",
    "                \"lon\": None,\n",
    "                \"address\": None,\n",
    "                \"photo_url\": None,\n",
    "                \"user_risk_assesment\": None,\n",
    "                \"user_lot_description\": None,\n",
    "                \"report_id\" : None,\n",
    "                \"case_number\" : None,\n",
    "                \"phone_number\": None,\n",
    "                \"price\": None\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Enviamos el men√∫ inicial\n",
    "        send_menu(chat_id)\n",
    "        return \"ok\", 200\n",
    "\n",
    "    # Si el usuario se encuentra en modo Chatbot (NLP)\n",
    "    if (user_session[chat_id][\"state\"] in [\"NLP-Chatbot-1\", \"NLP-Chatbot-2\", \"NLP-Chatbot-3\"]):\n",
    "        \n",
    "        print(\"ü§ñ Enviando consulta al RAG Chatbot...\")\n",
    "        query = message.get(\"text\", \"\")\n",
    "        response = rag_chain.invoke({\"query\": query})\n",
    "\n",
    "        raw_output = response[\"result\"]\n",
    "        if \"RESPUESTA DE ASISTENTE:\" in raw_output:\n",
    "            clean_output = raw_output.split(\"RESPUESTA DE ASISTENTE:\")[-1].strip()\n",
    "        else:\n",
    "            clean_output = raw_output.strip()\n",
    "\n",
    "        send_message(chat_id, escape_markdown_v2(clean_output))\n",
    "        \n",
    "        if user_session[chat_id][\"state\"] == \"NLP-Chatbot-3\":\n",
    "            send_message(chat_id, \"Has llegado al l√≠mite de preguntas consecutivas. El Chat se reiniciar√° ahora. Espero haber sido de ayuda.\")\n",
    "            user_session[chat_id][\"state\"] = \"restart\"\n",
    "            \n",
    "        if user_session[chat_id][\"state\"] == \"NLP-Chatbot-2\":\n",
    "            user_session[chat_id][\"state\"] = \"NLP-Chatbot-3\"\n",
    "            \n",
    "        if user_session[chat_id][\"state\"] == \"NLP-Chatbot-1\":\n",
    "            user_session[chat_id][\"state\"] = \"NLP-Chatbot-2\"\n",
    "            \n",
    "        \n",
    "        return \"ok\", 200\n",
    "\n",
    "    # En caso contrario, seguimos el flujo de reporte\n",
    "    current_state = user_session[chat_id][\"state\"]\n",
    "    response_text, next_state = report_message_flow(chat_id, current_state, message)\n",
    "    user_session[chat_id][\"state\"] = next_state\n",
    "\n",
    "    send_message(chat_id, response_text)\n",
    "    return \"ok\", 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c90e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:3000\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n",
      "Enviando menu al usuario\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:47:10] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Men√∫ enviado correctamente a 6940133416\n",
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:47:13] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n",
      "ü§ñ Enviando consulta al RAG Chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:47:47] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n",
      "ü§ñ Enviando consulta al RAG Chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:49:00] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n",
      "ü§ñ Enviando consulta al RAG Chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:49:45] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=7206382205\n",
      "==================================================\n",
      "Enviando menu al usuario\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:52:12] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Men√∫ enviado correctamente a 7206382205\n",
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=7206382205\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:52:14] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=7206382205\n",
      "==================================================\n",
      "ü§ñ Enviando consulta al RAG Chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:52:53] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=7206382205\n",
      "==================================================\n",
      "ü§ñ Enviando consulta al RAG Chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:53:57] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=7206382205\n",
      "==================================================\n",
      "ü§ñ Enviando consulta al RAG Chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:54:42] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=7206382205\n",
      "==================================================\n",
      "Enviando menu al usuario\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:55:01] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Men√∫ enviado correctamente a 7206382205\n",
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=7206382205\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:55:04] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=7206382205\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:55:32] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=7206382205\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:55:38] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=7206382205\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:55:59] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=7206382205\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:56:06] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=7206382205\n",
      "==================================================\n",
      "‚úÖ Reporte insertado: CASO-2025-0009 (report_f8127bfe-925a-474b-8ac2-5a4e538fa5c4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:56:12] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=7206382205\n",
      "==================================================\n",
      "Enviando menu al usuario\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:56:19] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Men√∫ enviado correctamente a 7206382205\n",
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=7206382205\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:56:22] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=7206382205\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Nov/2025 21:56:30] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tapp.run(port=3000, debug=False, use_reloader=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch311 (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
