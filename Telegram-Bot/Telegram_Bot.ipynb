{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032c6c4b",
   "metadata": {},
   "source": [
    "# Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd1accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import Field\n",
    "from sqlalchemy import create_engine, text\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from typing import Optional, List, Any\n",
    "from langchain.schema import Document\n",
    "from transformers import pipeline\n",
    "from flask import Flask, request\n",
    "from datetime import datetime\n",
    "from threading import Thread\n",
    "from peft import PeftModel\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import requests\n",
    "import uuid\n",
    "import json\n",
    "import torch\n",
    "import pdb\n",
    "import gc\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a7754b",
   "metadata": {},
   "source": [
    "# Microsoft Phi-3 como modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ee809",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405cbe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo local \n",
    "model_path = \"D:/LLM Models/microsoft-phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Configuraci√≥n de cuantizaci√≥n a 8 bits\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "# Cargamos el tokenizer de Phi-3\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Cargamos el modelo base directamente sin LoRA\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Colocamos el modelo en modo evaluaci√≥n\n",
    "model.eval()\n",
    "\n",
    "# Definimos el pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.4,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575014f9",
   "metadata": {},
   "source": [
    "# Conexi√≥n a base de datos de reportes en GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d27969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos variables de entorno\n",
    "load_dotenv(dotenv_path=\"chatbot.env\")\n",
    "\n",
    "# Guardamos variables de entorno para conectarnos a la BDD\n",
    "USER = os.getenv('USER_DB')\n",
    "PASSWORD = os.getenv('PASSWORD_DB')\n",
    "HOST = os.getenv('HOST_DB')\n",
    "DBNAME = 'postgres'#os.getenv('DBNAME')\n",
    "PORT = 5432\n",
    "\n",
    "# Instanciamos el engine para conectarnos a nuestra BDD en GCP\n",
    "engine = create_engine(f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DBNAME}\")\n",
    "\n",
    "# Confirmamos que nos podamos conectar\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        print(\"‚úÖ Conexi√≥n exitosa con super_user\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error de conexi√≥n:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea0708",
   "metadata": {},
   "source": [
    "# Sistema RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c250955",
   "metadata": {},
   "source": [
    "### Embeddings y VectorStore con FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1027f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=50)\n",
    "\n",
    "# Definimos como funcionaran nuestros embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    # model_name=\"intfloat/multilingual-e5-base\",\n",
    "    # encode_kwargs={\"normalize_embeddings\": True}\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={\n",
    "        'device': 'cuda'#,\n",
    "        #'trust_remote_code': True\n",
    "    },\n",
    "    encode_kwargs={\n",
    "        \"normalize_embeddings\": True,\n",
    "        \"batch_size\": 8\n",
    "    }\n",
    ")\n",
    "\n",
    "vs = FAISS.load_local(\n",
    "    #\"C:/Users/Delbert/Documents/Maestria/Proyecto Integrador/Avance 1/Tecnologico-Monterrey-Proyecto-Integrador-equipo-36/Baseline/rag_faiss_store\",\n",
    "    \"C:/Users/Delbert/Documents/Maestria/Proyecto Integrador/Avance 1/Tecnologico-Monterrey-Proyecto-Integrador-equipo-36/Telegram-Bot/rag_faiss_store\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc0f1c",
   "metadata": {},
   "source": [
    "### Clase custom para el ChatBot y generacion de respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bb1562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(tokenizer, sentence):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": sentence}], tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=512, skip_special_tokens=True, stop=None):\n",
    "    inputs = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(output, skip_special_tokens=skip_special_tokens)[0]\n",
    "\n",
    "    if stop:\n",
    "        for token in stop:\n",
    "            if token in decoded:\n",
    "                decoded = decoded.split(token)[0]\n",
    "                break\n",
    "    \n",
    "    if \"</answer>\" not in decoded:\n",
    "        decoded += \"</answer>\"\n",
    "        \n",
    "    return decoded[len(prompt):].strip()\n",
    "\n",
    "\n",
    "class Phi3ChatTemplateLLM(LLM):\n",
    "    model: Any = Field(exclude=True)\n",
    "    tokenizer: Any = Field(exclude=True)\n",
    "    max_new_tokens: int = 512\n",
    "    skip_special_tokens: bool = True\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"phi3-chat-template\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs: Any) -> str:\n",
    "        try:\n",
    "            # user_prompt = prompt\n",
    "            chat_prompt = prompt #gen_prompt(self.tokenizer, user_prompt)\n",
    "            \n",
    "            return generate(\n",
    "                self.model,\n",
    "                self.tokenizer,\n",
    "                chat_prompt,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                skip_special_tokens=self.skip_special_tokens\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error al generar respuesta con LLM: {str(e)}\"\n",
    "\n",
    "custom_llm = Phi3ChatTemplateLLM(model=model, tokenizer=tokenizer, max_new_tokens=512, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e742ba",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_prompt = PromptTemplate(\n",
    "#     input_variables=[\"context\", \"question\"],\n",
    "#     template=( \"\"\"You are an assistant called GorgoBot. Use only the following pieces of retrieved context (wrapped between <context> and </context>) to answer the latest question.\n",
    "#               If the exact information is unavailable, indicate what is missing. Always answer in Spanish.\n",
    "              \n",
    "#               Maintain a polite, professional and pleasant tone and focus on resolving issues efficiently. Respond to user queries by providing empathetic and detailed solutions.\n",
    "#               If you cannot get a crystal clear answer from the context, say that you don't know, do not make anything up.             \n",
    "#               <context> {context} </context>\n",
    "              \n",
    "#               Then answer the question wrapped between <question> and </question>.\n",
    "              \n",
    "#               <question>{question}</question>\n",
    "              \n",
    "#               Finally generate the answer wrapped between <answer> and </answer>.\n",
    "#               \"\"\"\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "my_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"\"\"\"You are an assistant called GorgoBot. Use only the following pieces of retrieved context (wrapped between <context> and </context>) to answer the latest question.\n",
    "If the exact information is unavailable, indicate what is missing. Always answer in Spanish.\n",
    "\n",
    "\n",
    "Do NOT repeat instructions or examples found inside <context>.\n",
    "\n",
    "Maintain a polite, professional and pleasant tone and focus on resolving issues efficiently. Respond to user queries by providing empathetic and detailed solutions.\n",
    "If you cannot get a crystal clear answer from the context, say that you don't know, do not make anything up.             \n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Then answer the question wrapped between <question> and </question>.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Write your final response **only inside** the <answer> and </answer> tags. Do not include anything else.\n",
    "\n",
    "<answer>\n",
    "\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "retriever = vs.as_retriever( search_kwargs={\"k\": 3})\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=custom_llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": my_prompt},\n",
    "    return_source_documents=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac2fe1",
   "metadata": {},
   "source": [
    "# Conexi√≥n a Telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a860de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_cases_count():\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            total_cases = conn.execute(text(\"SELECT COUNT(*) FROM reportes_chatbot;\")).scalar()\n",
    "        return total_cases\n",
    "\n",
    "    except:\n",
    "        print(\"Error conect√°ndose a la BDD.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab14b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_report_into_bdd(data):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        query = text(\"\"\"\n",
    "        INSERT INTO reportes_chatbot (\n",
    "            report_id,\n",
    "            case_number,\n",
    "            lat,\n",
    "            lon,\n",
    "            address,\n",
    "            photo_url,\n",
    "            user_risk_assesment,\n",
    "            user_lot_description,\n",
    "            contact_phone,\n",
    "            price,\n",
    "            current_state\n",
    "        ) VALUES (\n",
    "            :report_id,\n",
    "            :case_number,\n",
    "            :lat,\n",
    "            :lon,\n",
    "            :address,\n",
    "            :photo_url,\n",
    "            :user_risk_assesment,\n",
    "            :user_lot_description,\n",
    "            :phone_number,\n",
    "            :price,\n",
    "            'Nuevo'\n",
    "        );\"\"\")\n",
    "        \n",
    "        try:\n",
    "            with engine.begin() as conn:\n",
    "                conn.execute(query, data)\n",
    "            print(f\"‚úÖ Reporte insertado: {data.get('case_number')} ({data.get('report_id')})\")\n",
    "            return True, data.get('case_number')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error al insertar el reporte {data.get('case_number')}: {e}\")\n",
    "            return False\n",
    "        \n",
    "    except:\n",
    "        print(\"‚ùå No se pudo insertar el reporte a la BDD, se procede a desechar.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf3a409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_case_state(case_suffix: str):\n",
    "\n",
    "    # Aseguramos el formato correcto con el prefijo 'CASO-'\n",
    "    case_number = f\"CASO-{case_suffix.strip()}\"\n",
    "\n",
    "    query = text(\"\"\"\n",
    "        SELECT current_state\n",
    "        FROM reportes_chatbot\n",
    "        WHERE case_number = :case_number;\n",
    "    \"\"\")\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(query, {\"case_number\": case_number}).fetchone()\n",
    "\n",
    "            if result is None:\n",
    "                return False, None\n",
    "            else:\n",
    "                return True, result[0]\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è Error al consultar la base de datos: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf7309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_menu(chat_id):\n",
    "\n",
    "    menu_message = (\n",
    "        \"üëã *Bienvenido, ser√° un gusto atenderte.*\\n\\n\"\n",
    "        \"¬øQu√© te interesa llevar a cabo?\\n\\n\"\n",
    "        \"A. *Hacer un reporte de un predio infectado*\\n\\n\"\n",
    "        \"B. *Preguntar por el seguimiento a un reporte anterior.*\\n\\n\"\n",
    "        \"C. *Preguntar generalidades sobre el muestreo de gorgojos.*\\n\\n\"\n",
    "        \"_Actualmente, solo estas 3 opciones est√°n disponibles._\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"chat_id\": chat_id,\n",
    "        \"text\": menu_message,\n",
    "        \"parse_mode\": \"Markdown\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/sendMessage\", json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úÖ Men√∫ enviado correctamente a {chat_id}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Error enviando men√∫: {response.text}\")\n",
    "            \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al enviar men√∫: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33569564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_message_flow(sender_id, state, message_data):\n",
    "\n",
    "    # Inicializamos variables\n",
    "    response_text = \"\"\n",
    "    next_state = state\n",
    "    \n",
    "    current_case_state_dict = {\n",
    "        \"Nuevo\": \" el caso fue recibido y ser√° investigado por un experto en control de plagas de picudo.\",\n",
    "        \"En proceso\" : \" el caso fue asignado a un experto en control de plagas de picudo.\",\n",
    "        \"En campo\"   : \" el caso se encuentra siendo investigado en el sitio reportado\",\n",
    "        \"Falsa alerta\" : \" el caso se cataloga como una falsa alerta con base en la investigaci√≥n realizada.\",\n",
    "        \"Verdadera alerta\" : \" se confirma una infestaci√≥n de picudo y se tomaron acciones adecuadas para erradicarlo.\",\n",
    "        \"Concluido\" : \" se llev√≥ a cabo la investigaci√≥n, erradicaci√≥n y control del picudo.\"\n",
    "    }\n",
    "    \n",
    "    # Detectamos si el usuario ha compartido su numero de telefono\n",
    "    contact = message_data.get(\"contact\", {})\n",
    "    phone_number = contact.get(\"phone_number\")\n",
    "\n",
    "    # Detectamos el contenido del mensaje\n",
    "    text = message_data.get(\"text\", \"\").strip().lower() if \"text\" in message_data else \"\"\n",
    "    location = message_data.get(\"location\", {})\n",
    "    photo = message_data.get(\"photo\", [])\n",
    "    lat = location.get(\"latitude\")\n",
    "    lon = location.get(\"longitude\")\n",
    "    \n",
    "    # Guardamos el contacto si este es publico\n",
    "    if phone_number:\n",
    "        user_session[sender_id][\"data\"][\"phone_number\"] = phone_number\n",
    "\n",
    "\n",
    "    # Extraemos la URL de la foto (si existe)\n",
    "    photo_url = None\n",
    "    if photo:\n",
    "        file_id = photo[-1][\"file_id\"]  # La √∫ltima suele ser la de mejor resoluci√≥n\n",
    "        photo_url = f\"https://api.telegram.org/file/bot{TOKEN}/{file_id}\"\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # Saludo inicial para reportar un predio con problemas\n",
    "    # ============================================================\n",
    "    \n",
    "    if state == \"saludo\":\n",
    "        if text in [\"a\", \"üÖ∞Ô∏è\"]:\n",
    "            response_text = (\n",
    "                \"Gracias por tu proactividad üåæ.\\n Primero, env√≠a la **ubicaci√≥n** del lote afectado usando el √≠cono üìç.\"\n",
    "            )\n",
    "            next_state = \"ask-location\"\n",
    "\n",
    "        elif text in [\"b\", \"üÖ±Ô∏è\"]:\n",
    "            response_text = (\n",
    "                \"Claro, por favor ind√≠came tu n√∫mero de caso.\\n\\nEl formato del caso debe contener el a√±o y n√∫mero de reporte.\\n\"\n",
    "                \"\\nEste es un ejemplo de un n√∫mero de caso v√°lido: 2025-0001\"\n",
    "            )\n",
    "            next_state = \"ask-case-file\"\n",
    "            \n",
    "        elif text in [\"c\"]:\n",
    "            response_text = (\n",
    "                \"Perfecto. Puedes escribir hasta 3 preguntas y te responder√© con ayuda del asistente inteligente.\"\n",
    "            )\n",
    "            next_state = \"NLP-Chatbot-1\"\n",
    "\n",
    "    # ============================================================\n",
    "    # Preguntamos por el n√∫mero de caso en caso de que se haya seleccionado esa opcion.\n",
    "    # ============================================================\n",
    "    elif state == \"ask-case-file\":\n",
    "        regex_pattern = r\"^(20\\d{2})-(\\d{4})$\"\n",
    "        \n",
    "        if text:\n",
    "            text = text.strip()\n",
    "            valid_case_format = re.match(regex_pattern, text)\n",
    "            \n",
    "            if valid_case_format:\n",
    "                \n",
    "                case_success, current_case_state = get_case_state(text)\n",
    "                \n",
    "                if not case_success:\n",
    "                    response_text = (f\"No existe un caso con el correlativo {text} en nuetra base de datos.\\n\\n\"\n",
    "                                      \"Por lo anterior, te instamos a realizar un reporte si conoces alguna situaci√≥n o predio que \\n\"\n",
    "                                      \"presente un riesgo de infestaci√≥n.\")\n",
    "                    \n",
    "                    next_state = 'restart'\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    description = current_case_state_dict.get(\n",
    "                            current_case_state, \n",
    "                            \"no se tiene una descripci√≥n disponible para este estado.\"\n",
    "                        )\n",
    "                    response_text = (f\"El caso {text} se encuentra en el estado: {current_case_state} \"\n",
    "                                     f\"\\n\\nEsto significa que {description}\\n\\nPara reiniciar el chat, escribe alg√∫n texto de nuevo, por favor.\")\n",
    "                    \n",
    "                    next_state = 'restart'\n",
    "                \n",
    "            else:\n",
    "                response_text = (\"No reconozco ese formato de caso.\\nPor favor, env√≠a tu caso usando el formato `a√±o`-`# de caso`.\\n\\nEjemplo: 2025-0001\")\n",
    "        \n",
    "        else:\n",
    "            response_text = \"‚ö†Ô∏è No detect√© un n√∫mero de caso. Por favor, env√≠a el n√∫mero siguiendo el formato 2025-<Numero de caso en 4 digitos>.\"\n",
    "        \n",
    "    # ============================================================\n",
    "    # Preguntamos por la ubicaci√≥n. Si esta se env√≠a, generamos un n√∫mero de caso unico y un\n",
    "    # numero de caso relativo para evitar duplicar reportes\n",
    "    # ============================================================\n",
    "    \n",
    "    elif state == \"ask-location\":\n",
    "        if lat and lon:\n",
    "            \n",
    "            # Usaremos el contador global de casos actuales en la BDD\n",
    "            case_counter = get_total_cases_count() + 1\n",
    "\n",
    "            # Generamos un id de reporte unico al tener ubicacion\n",
    "            report_id = f\"report_{uuid.uuid4()}\"\n",
    "            user_session[sender_id][\"data\"][\"report_id\"] = report_id\n",
    "\n",
    "            # Generamos el n√∫mero de caso legible\n",
    "            year = datetime.now().year\n",
    "            case_number = f\"CASO-{year}-{case_counter:04d}\"\n",
    "            user_session[sender_id][\"data\"][\"case_number\"] = case_number\n",
    "            \n",
    "            # Guardamos la ubicacion de latitud y longitud\n",
    "            user_session[sender_id][\"data\"][\"lat\"] = lat\n",
    "            user_session[sender_id][\"data\"][\"lon\"] = lon\n",
    "            \n",
    "            response_text = (\n",
    "                \"üìç *Ubicaci√≥n recibida correctamente.*\\n\\n Ahora, por favor env√≠a una **foto** del lote o planta afectada. üì∏\"\n",
    "            )\n",
    "            next_state = \"ask_photo\"\n",
    "        else:\n",
    "            response_text = (\n",
    "                \"‚ö†Ô∏è No detect√© una ubicaci√≥n v√°lida. Por favor, usa el bot√≥n de üìç para enviarla correctamente.\"\n",
    "            )\n",
    "            \n",
    "    # ============================================================\n",
    "    # Pedimos una fotograf√≠a sobre lo reportado por el usuario\n",
    "    # ============================================================\n",
    "    elif state == \"ask_photo\":\n",
    "        if photo_url:\n",
    "            user_session[sender_id][\"data\"][\"photo_url\"] = photo_url\n",
    "            response_text = (\n",
    "                \"üì∏ *Foto recibida correctamente.*\\n\" \n",
    "                \"Por favor, ahora describe las condiciones del predio. Es de ayuda conocer si el predio se encuentra\\n\\n\"\n",
    "                \"1. Abandonado\\n2. En mal estado.\\n3. Infestado por picudos.\\n\"\n",
    "                \"\\nCualquier informacion adicional en tu descripci√≥n es de ayuda \"\n",
    "            )\n",
    "            next_state = \"ask_lot_description\"\n",
    "        else:\n",
    "            response_text = \"‚ö†Ô∏è No se detect√≥ una foto. Por favor env√≠a una imagen del lote afectado.\"\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # Luego preguntamos por una descripci√≥n detallada del usuario\n",
    "    # ============================================================\n",
    "    elif state == \"ask_lot_description\":    \n",
    "        if text:\n",
    "            user_session[sender_id][\"data\"][\"user_lot_description\"] = text\n",
    "            \n",
    "            response_text = (\n",
    "                \"Gracias por tu descripci√≥n del predio. Ahora, clasifica el riesgo que representa para t√≠ este predio de alguna de las siguientes formas:\\n\"\n",
    "                \"1. Alto\\n2. Medio\\n3. Bajo\"\n",
    "            )\n",
    "            \n",
    "            next_state = 'ask_risk'\n",
    "        \n",
    "        else:\n",
    "            response_text = \"‚ö†Ô∏è No se detect√≥ una respuesta v√°lida. Por favor, describe el estado del predio usando oraciones completas.\"\n",
    "    \n",
    "    # ============================================================\n",
    "    # Ahora preguntamos el riesgo que el usuario percibe\n",
    "    # ============================================================\n",
    "    elif state == \"ask_risk\":\n",
    "        \n",
    "        if text in [\"alta\", \"media\", \"baja\", \"alto\", \"medio\", \"bajo\", \"1\", \"2\", \"3\"]:\n",
    "            \n",
    "            if text in [\"1\"]:\n",
    "                text = \"Alto\"\n",
    "            if text in [\"2\"]:\n",
    "                text = \"Medio\"\n",
    "            if text in [\"3\"]:\n",
    "                text = \"Bajo\"\n",
    "            \n",
    "            user_session[sender_id][\"data\"][\"user_risk_assesment\"] = text.capitalize()\n",
    "            data = user_session[sender_id][\"data\"]\n",
    "            \n",
    "            response_text = (\n",
    "                f\"‚úÖ Gracias por tu reporte.\\n\"\n",
    "                f\"üìç Ubicaci√≥n: ({data['lat']}, {data['lon']})\\n\"\n",
    "                f\"üì∏ Foto: Confirmada\\n\"\n",
    "                f\"üö® Riesgo: {data['user_risk_assesment']}\\n\"\n",
    "                f\"üí¨ Descripci√≥n: Recibida\\n\\n\"\n",
    "                \"¬øEsta informaci√≥n es correcta? Responde con *S√≠* o *No*.\"\n",
    "            )\n",
    "            next_state = \"confirmation\"\n",
    "        else:\n",
    "            response_text = \"Por favor indica el nivel de riesgo como: Alta, Media o Baja.\"\n",
    "\n",
    "    # ============================================================\n",
    "    # Ahora informamos sobre el reporte dando el correlativo de cada caso\n",
    "    # ============================================================\n",
    "    elif state == \"confirmation\":\n",
    "        \n",
    "        # Si el reporte es correcto, lo confirmamos\n",
    "        if re.search(r'^\\s*s[i√≠]\\s*$', text):\n",
    "        \n",
    "            response_text = \"üåæ Tu reporte ya fue registrado. ¬°Gracias por tu colaboraci√≥n!\"\n",
    "            next_state = \"restart\"\n",
    "            \n",
    "            inserted, case = insert_report_into_bdd(user_session[sender_id][\"data\"])\n",
    "            \n",
    "            if inserted:\n",
    "                response_text = response_text + f\"\\n\\nAdicionalmente, se cre√≥ el caso {case} por si quisieras consultar el estado del mismo.\"\n",
    "            \n",
    "        # Si nos dice que no es correcto, desechamos el reporte\n",
    "        elif re.search(r'^\\s*no\\s*$', text):\n",
    "            response_text = \"‚ùå Entendido. Tu reporte no se registrar√°.\\nPara repetir el reporte, o hacer otro, puedes volver a escribir sobre este mismo chat.\"\n",
    "            next_state = \"restart\"\n",
    "        \n",
    "        # Si nos responde algo que no sabemos que es\n",
    "        else:\n",
    "            response_text = \"Por favor responde √∫nicamente con *S√≠* o *No*.\"\n",
    "            \n",
    "\n",
    "    # Else que solo existe por si no capturamos la logica de la respuesta\n",
    "    else:\n",
    "        response_text = \"No entend√≠ tu respuesta. Por favor intenta nuevamente.\"\n",
    "\n",
    "    return response_text, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b7eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_message(chat_id, text):\n",
    "    \"\"\"\n",
    "    Env√≠a un mensaje de texto al usuario usando la API de Telegram.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"chat_id\": chat_id,\n",
    "        \"text\": text,\n",
    "        \"parse_mode\": \"Markdown\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/sendMessage\", json=payload)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ö†Ô∏è Error enviando mensaje: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al enviar mensaje: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d241c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta funcion la usamos porque telegram necesita que hagamos scape \n",
    "# de algunos caracteres como puntos, signos de exclamacion, etc.\n",
    "def escape_markdown_v2(text):\n",
    "    return re.sub(r'([_*\\[\\]()~`>#+\\-=|{}.!])', r'\\\\\\1', text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a998979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rag_async(chat_id, query):\n",
    "\n",
    "    try:\n",
    "        response = rag_chain.invoke({\"query\": query})\n",
    "        raw_output = response[\"result\"]\n",
    "        pattern = r'<answer>(.*?)</answer>'\n",
    "        match = re.search(pattern, raw_output, re.DOTALL)\n",
    "        \n",
    "        clean_output = raw_output.split(\"</answer>\")[0].strip() #match.group(1).strip() # raw_output.split(\"RESPUESTA DE ASISTENTE:\")[-1].strip()\n",
    "        \n",
    "        send_message(chat_id, escape_markdown_v2(clean_output))\n",
    "        \n",
    "        # Actualizar estado\n",
    "        if user_session[chat_id][\"state\"] == \"NLP-Chatbot-3\":\n",
    "            send_message(chat_id, \"Has llegado al l√≠mite de consultars consecutivos. En estos momentos se reiniciar√° el chat. Gracias.\")\n",
    "            user_session[chat_id][\"state\"] = \"restart\"\n",
    "            \n",
    "        elif user_session[chat_id][\"state\"] == \"NLP-Chatbot-2\":\n",
    "            user_session[chat_id][\"state\"] = \"NLP-Chatbot-3\"\n",
    "            \n",
    "        elif user_session[chat_id][\"state\"] == \"NLP-Chatbot-1\":\n",
    "            user_session[chat_id][\"state\"] = \"NLP-Chatbot-2\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en RAG: {e}\")\n",
    "        send_message(chat_id, \"Lo siento, ocurri√≥ un error procesando tu consulta.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = os.getenv(\"TELEGRAM_BOT_TOKEN\")\n",
    "BASE_URL = f\"https://api.telegram.org/bot{TOKEN}\"\n",
    "\n",
    "# Diccionario que guardara la informacion temporalmente\n",
    "# antes de escribirla a la BDD\n",
    "user_session = {}\n",
    "case_counter = get_total_cases_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c08469",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/webhook\", methods=[\"POST\"])\n",
    "def telegram_webhook():\n",
    "    \"\"\"\n",
    "    Webhook principal para manejar mensajes entrantes desde Telegram.\n",
    "    Procesa texto, fotos y ubicaciones, mantiene el flujo de conversaci√≥n\n",
    "    y responde usando la API de Telegram.\n",
    "    \"\"\"\n",
    "    data = request.get_json()\n",
    "\n",
    "    if \"message\" not in data:\n",
    "        return \"ok\", 200\n",
    "\n",
    "    message = data[\"message\"]\n",
    "    chat_id = message[\"chat\"][\"id\"]\n",
    "    username = message[\"chat\"].get(\"username\", \"Desconocido\")\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Mensaje recibido de @{username} | chat_id={chat_id}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Si es la primera vez que el usuario escribe\n",
    "    if (chat_id not in user_session) or (user_session[chat_id][\"state\"] == \"restart\"):\n",
    "        print(\"Enviando menu al usuario\")\n",
    "        user_session[chat_id] = {\n",
    "            \"state\": \"saludo\",\n",
    "            \"data\": {\n",
    "                \"lat\": None,\n",
    "                \"lon\": None,\n",
    "                \"address\": None,\n",
    "                \"photo_url\": None,\n",
    "                \"user_risk_assesment\": None,\n",
    "                \"user_lot_description\": None,\n",
    "                \"report_id\" : None,\n",
    "                \"case_number\" : None,\n",
    "                \"phone_number\": None,\n",
    "                \"price\": None\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Enviamos el men√∫ inicial\n",
    "        send_menu(chat_id)\n",
    "        return \"ok\", 200\n",
    "\n",
    "    # Si el usuario se encuentra en modo Chatbot (NLP)\n",
    "    # Si est√° en modo chatbot\n",
    "    if user_session[chat_id][\"state\"] in [\"NLP-Chatbot-1\", \"NLP-Chatbot-2\", \"NLP-Chatbot-3\"]:\n",
    "        print(\"ü§ñ Enviando consulta al RAG Chatbot...\")\n",
    "        query = message.get(\"text\", \"\")\n",
    "\n",
    "        thread = Thread(target=process_rag_async, args=(chat_id, query))\n",
    "        thread.daemon = True\n",
    "        thread.start()\n",
    "        \n",
    "        # Responder INMEDIATAMENTE a Telegram\n",
    "        return \"ok\", 200\n",
    "\n",
    "    # En caso contrario, seguimos el flujo de reporte\n",
    "    current_state = user_session[chat_id][\"state\"]\n",
    "    response_text, next_state = report_message_flow(chat_id, current_state, message)\n",
    "    user_session[chat_id][\"state\"] = next_state\n",
    "\n",
    "    send_message(chat_id, response_text)\n",
    "    return \"ok\", 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47860511",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tapp.run(port=3000, debug=False, use_reloader=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch311 (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
