{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032c6c4b",
   "metadata": {},
   "source": [
    "# Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd1accd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Python_Envs\\torch311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import Field\n",
    "from sqlalchemy import create_engine, text\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from typing import Optional, List, Any\n",
    "from langchain.schema import Document\n",
    "from transformers import pipeline\n",
    "from flask import Flask, request\n",
    "from datetime import datetime\n",
    "from peft import PeftModel\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import requests\n",
    "import uuid\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a7754b",
   "metadata": {},
   "source": [
    "# Microsoft Phi-3 como modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169ee809",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "405cbe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:16<00:00,  8.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# Modelo local \n",
    "model_path = \"D:/LLM Models/microsoft-phi-3-mini-4k-instruct\"\n",
    "\n",
    "# LoRA adapters generados con el archivo LL_Fine_Tune_agave.ipynb\n",
    "lora_path = \"D:/LLM Models/agave_V001/agave_baseline_phi3_V01\"   \n",
    "\n",
    "# 8 bits de cuantizacion\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "# Usamos el tokenizer de Phi-3\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a137390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Implementamos los pesos LoRA a Phi-3\n",
    "model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "\n",
    "# Colocamos el  modelo en evaluacion para que no cambie los pesos por cada prompt que\n",
    "# se le env√≠e\n",
    "model.eval()\n",
    "\n",
    "# Definimos el pipeline de nuevo\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=64,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d11b37",
   "metadata": {},
   "source": [
    "Ahora, durante el fine-tuning de Phi-3 se us√≥ el siguiente template de conversaci√≥n:\n",
    "\n",
    "```\n",
    "<|user|>\n",
    "`Pregunta o  prompt del usuario`.<|end|>\n",
    "<|assistant|>\n",
    "`Respusta de Phi-3 al usar el Chat template`\n",
    "```\n",
    "\n",
    "Esto se hizo con base en la documentaci√≥n mostrada por (Voigt, 2025), y por lo tanto para que nuestro modelo fine-tuned funcione correctamente, debemos convertir nuestros prompts a esta forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6505b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(tokenizer, sentence):\n",
    "    converted_sample = [{\"role\": \"user\", \"content\": sentence}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        converted_sample, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acbab8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=256, skip_special_tokens=False):\n",
    "  \n",
    "  tokenized_input = tokenizer(\n",
    "  prompt, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "  model.eval()\n",
    "  gen_output = model.generate(**tokenized_input,\n",
    "  eos_token_id=tokenizer.eos_token_id,\n",
    "  max_new_tokens=max_new_tokens)\n",
    "  output = tokenizer.batch_decode(gen_output, skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "  return output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49850fd8",
   "metadata": {},
   "source": [
    "# Sistema RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b36479",
   "metadata": {},
   "source": [
    "Ahora que ya hicimos un fine-tuning sobre `Phi-3`, aunque este fue bastante superficial porque solo usamos 12,500 registros `instruction`-`input`-`output`, podemos indicar que nuestro LLM ahora \"escribir√°\" de manera m√°s propia la informaci√≥n con base en las respuestras de los prompts anteriores.\n",
    "\n",
    "Por lo anterior, lo que haremos ahora ser√° imlementar un sistema RAG el cual le dar√° las siguientes caracter√≠sticas a nuetro  LLM:\n",
    "\n",
    "- Lo volver√° m√°s estable al responder. Por medio del sistema RAG, contextos y dem√°s, ser√° posible reforzar la forma en que el modelo responder√° y demostrar√° la informaci√≥n.\n",
    "- Permite darle instrucciones al LLM para no halucinar. Por ejemplo, es posible indicar que si la informaci√≥n no se encuentra disponible en el contexto, que no responda la pregunta.\n",
    "\n",
    "Con esto, montar el sistema RAG sobre nuestro Phi-3 con fine-tuning permite que, aparte de que nuestro Bot hable m√°s propiamente, pueda usar informaci√≥n hist√≥rica para responder dudas comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e571d181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargaron 44 paginas desde el Manual Operativo.\n",
      "Y los primeros 100 caracteres son los siguientes:\n",
      "\n",
      "DIRECCI√ìN DE PROTECCI√ìN FITOSANITARIA \n",
      "  Clave: MOP-DPF-PRAV \n",
      "  Versi√≥n: 4 \n",
      " \n",
      "I \n",
      "  Emisi√≥n: 04/2017 \n"
     ]
    }
   ],
   "source": [
    "# Cargamos el Manual Operativo como un documento para consulta del bot.\n",
    "loader = PyPDFLoader(r'C:/Users/Delbert/Documents/Maestria/Proyecto Integrador/Data/Manual_Operativo_Agave.pdf')\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Se cargaron {len(docs)} paginas desde el Manual Operativo.\")\n",
    "print(f\"Y los primeros 100 caracteres son los siguientes:\\n\")\n",
    "print(docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8dbb04",
   "metadata": {},
   "source": [
    "#### Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730d173e",
   "metadata": {},
   "source": [
    "Una parte ***importante*** de un sistema RAG son los embeddings que permitir√°n que nuestro LLM relacione de mejor manera las palabras. Estos vectores no son m√°s que \"pesos\" que relacionan palabras entre s√≠ y permitir√°n que el LLM pueda predecir de mejor manera las respuestas.\n",
    "\n",
    "Estos embeddings terminan siendo como el \"alma\" que permite que nuestro Bot no alucine y tampoco conteste algo que no sea real. En este paso, debido la cantidad de documentos, es normal esperar entre 60 a 120 minutos de generaci√≥n de embeddings. Este proceso se puede acelerar por medio de Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e20a8e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Delbert\\AppData\\Local\\Temp\\ipykernel_2400\\854352266.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "chunked_docs = splitter.split_documents(docs)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-base\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce1172c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 619918\n"
     ]
    }
   ],
   "source": [
    "vs = FAISS.load_local(\n",
    "    \"C:/Users/Delbert/Documents/Maestria/Proyecto Integrador/Avance 1/Tecnologico-Monterrey-Proyecto-Integrador-equipo-36/Baseline/rag_faiss_store\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "print(\"Total docs:\", len(vs.docstore._dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555cd0ed",
   "metadata": {},
   "source": [
    "Ahora bien, como nuestro LLM tuvo un fine-tuning con base en 2 templates (`instruction, input & output` y el `chat-template` de Phi-3), es importante que nuestro sistema RAG pueda encapsular las consultas que se le hagan de forma que el LLM lo entienda.\n",
    "\n",
    "En el caso en que cambiemos el LLM utilizado, deber√≠amos de generar una nueva clase para lograr lo mismo. Por lo tanto, en este punto, ya estamos practicamente comprometidos con continuar con Phi-3 ya que:\n",
    "\n",
    "1. Hicimos un proceso de fine-tuning sobre `Phi-3` por varias horas\n",
    "2. Creamos una clase para hacer wraping del sistema RAG para que `Phi-3` comprenda los queries haciendo uso  del template `ChatBot`.\n",
    "3. Realizar de nuevo estos entrenamientos sobre otro modelo puede ser computacionalmente m√°s caro. Por ejemplo, `Mistral-7B` aun cuantizado en 4bits consume bastante m√°s VRAM que `Phi-3` cuantizado en 8bits\n",
    "\n",
    "\n",
    "Por √∫ltimo, configuramos nuestro LLM para producir un m√°ximo de 32 tokens de salida. Si se quiere tener respustas m√°s largas, debemos definir un `max_new_tokens` distinto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eebc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phi3ChatTemplateLLM(LLM):\n",
    "    model: Any = Field(exclude=True)\n",
    "    tokenizer: Any = Field(exclude=True)\n",
    "    max_new_tokens: int = 32\n",
    "    skip_special_tokens: bool = True\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"phi3-chat-template\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs: Any) -> str:\n",
    "        # chat_prompt = gen_prompt(self.tokenizer, prompt)\n",
    "        # out = generate(\n",
    "        #     self.model,\n",
    "        #     self.tokenizer,\n",
    "        #     chat_prompt,\n",
    "        #     max_new_tokens=self.max_new_tokens,\n",
    "        #     skip_special_tokens=self.skip_special_tokens\n",
    "        # )\n",
    "        \n",
    "        chat_prompt = gen_prompt(self.tokenizer, prompt)\n",
    "        self.model.eval()\n",
    "        with torch.inference_mode():\n",
    "            out = generate(\n",
    "                self.model,\n",
    "                self.tokenizer,\n",
    "                chat_prompt,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                skip_special_tokens=self.skip_special_tokens\n",
    "            )\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d128d1c",
   "metadata": {},
   "source": [
    "Cargamos el modelo custom (contenido en la clase) para hacer pruebas.\n",
    "\n",
    "Usaremos el tokenizer incluido con la versi√≥n original de `Phi-3` para ahorrarnos el proceso de definici√≥n de tokens (por palabra, oraci√≥n, p√°rrafo, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28801e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_llm = Phi3ChatTemplateLLM(model=model, tokenizer=tokenizer, max_new_tokens=48, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f012d9",
   "metadata": {},
   "source": [
    "Configuramos el sistema `retriever` para implementar el RAG.\n",
    "\n",
    "Y generamos un prompt el cual, por medio de distintas pruebas, fue posible definir para obtener respuestas concretas de nuestro bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea7b8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"Eres un asistente operativo especializado en el picudo del agave que es conciso y resume las respuestas. \"\n",
    "        \"Usa **exclusivamente** la informaci√≥n del CONTEXTO para responder. \"\n",
    "        \"Si la respuesta no aparece en el contexto, di exactamente: \"\n",
    "        \"\\\"No aparece en el contexto.\\\" No inventes datos.\\n\\n\"\n",
    "        \"CONTEXTO:\\n{context}\\n\\n\"\n",
    "        \"PREGUNTA:\\n{question}\\n\\n\"\n",
    "        \"RESPUESTA DE ASISTENTE:\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=custom_llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": my_prompt},\n",
    "    return_source_documents=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea46a7",
   "metadata": {},
   "source": [
    "Y ahora generamos un par de prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38538c2c",
   "metadata": {},
   "source": [
    "Por √∫ltimo, dejamos las versiones de Torch y CUDA que utilizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1de0add8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Version de Pytorch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# \n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575014f9",
   "metadata": {},
   "source": [
    "# Conexi√≥n a base de datos de reportes en GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d27969e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conexi√≥n exitosa con super_user\n"
     ]
    }
   ],
   "source": [
    "# Cargamos variables de entorno\n",
    "load_dotenv(dotenv_path=\"chatbot.env\")\n",
    "\n",
    "# Guardamos variables de entorno para conectarnos a la BDD\n",
    "USER = os.getenv('USER_DB')\n",
    "PASSWORD = os.getenv('PASSWORD_DB')\n",
    "HOST = os.getenv('HOST_DB')\n",
    "DBNAME = 'postgres'#os.getenv('DBNAME')\n",
    "PORT = 5432\n",
    "\n",
    "# Instanciamos el engine para conectarnos a nuestra BDD en GCP\n",
    "engine = create_engine(f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DBNAME}\")\n",
    "\n",
    "# Confirmamos que nos podamos conectar\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        print(\"‚úÖ Conexi√≥n exitosa con super_user\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error de conexi√≥n:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20660094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_cases_count():\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            total_cases = conn.execute(text(\"SELECT COUNT(*) FROM reportes_chatbot;\")).scalar()\n",
    "        return total_cases\n",
    "\n",
    "    except:\n",
    "        print(\"Error conect√°ndose a la BDD.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f7d12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_report_into_bdd(data):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        query = text(\"\"\"\n",
    "        INSERT INTO reportes_chatbot (\n",
    "            report_id,\n",
    "            case_number,\n",
    "            lat,\n",
    "            lon,\n",
    "            address,\n",
    "            photo_url,\n",
    "            user_risk_assesment,\n",
    "            user_lot_description,\n",
    "            contact_phone,\n",
    "            price,\n",
    "            current_state\n",
    "        ) VALUES (\n",
    "            :report_id,\n",
    "            :case_number,\n",
    "            :lat,\n",
    "            :lon,\n",
    "            :address,\n",
    "            :photo_url,\n",
    "            :user_risk_assesment,\n",
    "            :user_lot_description,\n",
    "            :phone_number,\n",
    "            :price,\n",
    "            'Nuevo'\n",
    "        );\"\"\")\n",
    "        \n",
    "        try:\n",
    "            with engine.begin() as conn:\n",
    "                conn.execute(query, data)\n",
    "            print(f\"‚úÖ Reporte insertado: {data.get('case_number')} ({data.get('report_id')})\")\n",
    "            return True, data.get('case_number')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error al insertar el reporte {data.get('case_number')}: {e}\")\n",
    "            return False\n",
    "        \n",
    "    except:\n",
    "        print(\"‚ùå No se pudo insertar el reporte a la BDD, se procede a desechar.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9dce06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_case_state(case_suffix: str):\n",
    "\n",
    "    # Aseguramos el formato correcto con el prefijo 'CASO-'\n",
    "    case_number = f\"CASO-{case_suffix.strip()}\"\n",
    "\n",
    "    query = text(\"\"\"\n",
    "        SELECT current_state\n",
    "        FROM reportes_chatbot\n",
    "        WHERE case_number = :case_number;\n",
    "    \"\"\")\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(query, {\"case_number\": case_number}).fetchone()\n",
    "\n",
    "            if result is None:\n",
    "                return False, None\n",
    "            else:\n",
    "                return True, result[0]\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è Error al consultar la base de datos: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac2fe1",
   "metadata": {},
   "source": [
    "# Conexi√≥n a Telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cca30cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = os.getenv(\"TELEGRAM_BOT_TOKEN\")\n",
    "BASE_URL = f\"https://api.telegram.org/bot{TOKEN}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd254023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario que guardara la informacion temporalmente\n",
    "# antes de escribirla a la BDD\n",
    "user_session = {}\n",
    "case_counter = get_total_cases_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1227a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_menu(chat_id):\n",
    "\n",
    "    menu_message = (\n",
    "        \"üëã *Bienvenido, ser√° un gusto atenderte.*\\n\\n\"\n",
    "        \"¬øQu√© te interesa llevar a cabo?\\n\\n\"\n",
    "        \"A. *Hacer un reporte de un predio infectado*\\n\\n\"\n",
    "        \"B. *Preguntar por el seguimiento a un reporte anterior.*\\n\\n\"\n",
    "        \"C. *Preguntar generalidades sobre el muestreo de gorgojos y manual operativo.*\\n\\n\"\n",
    "        \"_Actualmente, solo estas 3 opciones est√°n disponibles._\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"chat_id\": chat_id,\n",
    "        \"text\": menu_message,\n",
    "        \"parse_mode\": \"Markdown\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/sendMessage\", json=payload)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úÖ Men√∫ enviado correctamente a {chat_id}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Error enviando men√∫: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al enviar men√∫: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b5d54ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_message_flow(sender_id, state, message_data):\n",
    "\n",
    "    # Inicializamos variables\n",
    "    response_text = \"\"\n",
    "    next_state = state\n",
    "    \n",
    "    current_case_state_dict = {\n",
    "        \"Nuevo\": \" el caso fue recibido y ser√° investigado por un experto en control de plagas de picudo.\",\n",
    "        \"En proceso\" : \" el caso fue asignado a un experto en control de plagas de picudo.\",\n",
    "        \"En campo\"   : \" el caso se encuentra siendo investigado en el sitio reportado\",\n",
    "        \"Falsa alerta\" : \" el caso se cataloga como una falsa alerta con base en la investigaci√≥n realizada.\",\n",
    "        \"Verdadera alerta\" : \" se confirma una infestaci√≥n de picudo y se tomaron acciones adecuadas para erradicarlo.\",\n",
    "        \"Concluido\" : \" se llev√≥ a cabo la investigaci√≥n, erradicaci√≥n y control del picudo.\"\n",
    "    }\n",
    "    \n",
    "    # Detectamos si el usuario ha compartido su numero de telefono\n",
    "    contact = message_data.get(\"contact\", {})\n",
    "    phone_number = contact.get(\"phone_number\")\n",
    "\n",
    "    # Detectamos el contenido del mensaje\n",
    "    text = message_data.get(\"text\", \"\").strip().lower() if \"text\" in message_data else \"\"\n",
    "    location = message_data.get(\"location\", {})\n",
    "    photo = message_data.get(\"photo\", [])\n",
    "    lat = location.get(\"latitude\")\n",
    "    lon = location.get(\"longitude\")\n",
    "    \n",
    "    # Guardamos el contacto si este es publico\n",
    "    if phone_number:\n",
    "        user_session[sender_id][\"data\"][\"phone_number\"] = phone_number\n",
    "\n",
    "\n",
    "    # Extraemos la URL de la foto (si existe)\n",
    "    photo_url = None\n",
    "    if photo:\n",
    "        file_id = photo[-1][\"file_id\"]  # La √∫ltima suele ser la de mejor resoluci√≥n\n",
    "        photo_url = f\"https://api.telegram.org/file/bot{TOKEN}/{file_id}\"\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # Saludo inicial para reportar un predio con problemas\n",
    "    # ============================================================\n",
    "    \n",
    "    if state == \"saludo\":\n",
    "        if text in [\"a\", \"üÖ∞Ô∏è\"]:\n",
    "            response_text = (\n",
    "                \"Gracias por tu proactividad üåæ.\\n Primero, env√≠a la **ubicaci√≥n** del lote afectado usando el √≠cono üìç.\"\n",
    "            )\n",
    "            next_state = \"ask-location\"\n",
    "\n",
    "        elif text in [\"b\", \"üÖ±Ô∏è\"]:\n",
    "            response_text = (\n",
    "                \"Claro, por favor ind√≠came tu n√∫mero de caso.\\n\\nEl formato del caso debe contener el a√±o y n√∫mero de reporte.\\n\"\n",
    "                \"\\nEste es un ejemplo de un n√∫mero de caso v√°lido: 2025-0001\"\n",
    "            )\n",
    "            next_state = \"ask-case-file\"\n",
    "            \n",
    "        elif text in [\"c\"]:\n",
    "            response_text = (\n",
    "                \"Perfecto. Puedes escribir hasta 3 preguntas y te responder√© con ayuda del asistente inteligente.\"\n",
    "            )\n",
    "            next_state = \"NLP-Chatbot-1\"\n",
    "\n",
    "    # ============================================================\n",
    "    # Preguntamos por el n√∫mero de caso en caso de que se haya seleccionado esa opcion.\n",
    "    # ============================================================\n",
    "    elif state == \"ask-case-file\":\n",
    "        regex_pattern = r\"^(20\\d{2})-(\\d{4})$\"\n",
    "        \n",
    "        if text:\n",
    "            text = text.strip()\n",
    "            valid_case_format = re.match(regex_pattern, text)\n",
    "            \n",
    "            if valid_case_format:\n",
    "                \n",
    "                case_success, current_case_state = get_case_state(text)\n",
    "                \n",
    "                if not case_success:\n",
    "                    response_text = (f\"No existe un caso con el correlativo {text} en nuetra base de datos.\\n\\n\"\n",
    "                                      \"Por lo anterior, te instamos a realizar un reporte si conoces alguna situaci√≥n o predio que \\n\"\n",
    "                                      \"presente un riesgo de infestaci√≥n.\")\n",
    "                    \n",
    "                    next_state = 'restart'\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    description = current_case_state_dict.get(\n",
    "                            current_case_state, \n",
    "                            \"no se tiene una descripci√≥n disponible para este estado.\"\n",
    "                        )\n",
    "                    response_text = (f\"El caso {text} se encuentra en el estado: {current_case_state} \"\n",
    "                                     f\"\\n\\nEsto significa que {description}\\n\\nPara reiniciar el chat, escribe alg√∫n texto de nuevo, por favor.\")\n",
    "                    \n",
    "                    next_state = 'restart'\n",
    "                \n",
    "            else:\n",
    "                response_text = (\"No reconozco ese formato de caso.\\nPor favor, env√≠a tu caso usando el formato `a√±o`-`# de caso`.\\n\\nEjemplo: 2025-0001\")\n",
    "        \n",
    "        else:\n",
    "            response_text = \"‚ö†Ô∏è No detect√© un n√∫mero de caso. Por favor, env√≠a el n√∫mero siguiendo el formato 2025-<Numero de caso en 4 digitos>.\"\n",
    "        \n",
    "    # ============================================================\n",
    "    # Preguntamos por la ubicaci√≥n. Si esta se env√≠a, generamos un n√∫mero de caso unico y un\n",
    "    # numero de caso relativo para evitar duplicar reportes\n",
    "    # ============================================================\n",
    "    \n",
    "    elif state == \"ask-location\":\n",
    "        if lat and lon:\n",
    "            \n",
    "            # Usaremos el contador global de casos actuales en la BDD\n",
    "            case_counter = get_total_cases_count() + 1\n",
    "\n",
    "            # Generamos un id de reporte unico al tener ubicacion\n",
    "            report_id = f\"report_{uuid.uuid4()}\"\n",
    "            user_session[sender_id][\"data\"][\"report_id\"] = report_id\n",
    "\n",
    "            # Generamos el n√∫mero de caso legible\n",
    "            year = datetime.now().year\n",
    "            case_number = f\"CASO-{year}-{case_counter:04d}\"\n",
    "            user_session[sender_id][\"data\"][\"case_number\"] = case_number\n",
    "            \n",
    "            # Guardamos la ubicacion de latitud y longitud\n",
    "            user_session[sender_id][\"data\"][\"lat\"] = lat\n",
    "            user_session[sender_id][\"data\"][\"lon\"] = lon\n",
    "            \n",
    "            response_text = (\n",
    "                \"üìç *Ubicaci√≥n recibida correctamente.*\\n\\n Ahora, por favor env√≠a una **foto** del lote o planta afectada. üì∏\"\n",
    "            )\n",
    "            next_state = \"ask_photo\"\n",
    "        else:\n",
    "            response_text = (\n",
    "                \"‚ö†Ô∏è No detect√© una ubicaci√≥n v√°lida. Por favor, usa el bot√≥n de üìç para enviarla correctamente.\"\n",
    "            )\n",
    "            \n",
    "    # ============================================================\n",
    "    # Pedimos una fotograf√≠a sobre lo reportado por el usuario\n",
    "    # ============================================================\n",
    "    elif state == \"ask_photo\":\n",
    "        if photo_url:\n",
    "            user_session[sender_id][\"data\"][\"photo_url\"] = photo_url\n",
    "            response_text = (\n",
    "                \"üì∏ *Foto recibida correctamente.*\\n\" \n",
    "                \"Por favor, ahora describe las condiciones del predio. Es de ayuda conocer si el predio se encuentra\\n\\n\"\n",
    "                \"1. Abandonado\\n2. En mal estado.\\n3. Infestado por picudos.\\n\"\n",
    "                \"\\nCualquier informacion adicional en tu descripci√≥n es de ayuda \"\n",
    "            )\n",
    "            next_state = \"ask_lot_description\"\n",
    "        else:\n",
    "            response_text = \"‚ö†Ô∏è No se detect√≥ una foto. Por favor env√≠a una imagen del lote afectado.\"\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # Luego preguntamos por una descripci√≥n detallada del usuario\n",
    "    # ============================================================\n",
    "    elif state == \"ask_lot_description\":    \n",
    "        if text:\n",
    "            user_session[sender_id][\"data\"][\"user_lot_description\"] = text\n",
    "            \n",
    "            response_text = (\n",
    "                \"Gracias por tu descripci√≥n del predio. Ahora, clasifica el riesgo que representa para t√≠ este predio de alguna de las siguientes formas:\\n\"\n",
    "                \"1. Alto\\n2. Medio\\n3. Bajo\"\n",
    "            )\n",
    "            \n",
    "            next_state = 'ask_risk'\n",
    "        \n",
    "        else:\n",
    "            response_text = \"‚ö†Ô∏è No se detect√≥ una respuesta v√°lida. Por favor, describe el estado del predio usando oraciones completas.\"\n",
    "    \n",
    "    # ============================================================\n",
    "    # Ahora preguntamos el riesgo que el usuario percibe\n",
    "    # ============================================================\n",
    "    elif state == \"ask_risk\":\n",
    "        \n",
    "        if text in [\"alta\", \"media\", \"baja\", \"alto\", \"medio\", \"bajo\", \"1\", \"2\", \"3\"]:\n",
    "            \n",
    "            if text in [\"1\"]:\n",
    "                text = \"Alto\"\n",
    "            if text in [\"2\"]:\n",
    "                text = \"Medio\"\n",
    "            if text in [\"3\"]:\n",
    "                text = \"Bajo\"\n",
    "            \n",
    "            user_session[sender_id][\"data\"][\"user_risk_assesment\"] = text.capitalize()\n",
    "            data = user_session[sender_id][\"data\"]\n",
    "            \n",
    "            response_text = (\n",
    "                f\"‚úÖ Gracias por tu reporte.\\n\"\n",
    "                f\"üìç Ubicaci√≥n: ({data['lat']}, {data['lon']})\\n\"\n",
    "                f\"üì∏ Foto: Confirmada\\n\"\n",
    "                f\"üö® Riesgo: {data['user_risk_assesment']}\\n\"\n",
    "                f\"üí¨ Descripci√≥n: Recibida\\n\\n\"\n",
    "                \"¬øEsta informaci√≥n es correcta? Responde con *S√≠* o *No*.\"\n",
    "            )\n",
    "            next_state = \"confirmation\"\n",
    "        else:\n",
    "            response_text = \"Por favor indica el nivel de riesgo como: Alta, Media o Baja.\"\n",
    "\n",
    "    # ============================================================\n",
    "    # Ahora informamos sobre el reporte dando el correlativo de cada caso\n",
    "    # ============================================================\n",
    "    elif state == \"confirmation\":\n",
    "        \n",
    "        # Si el reporte es correcto, lo confirmamos\n",
    "        if re.search(r'^\\s*s[i√≠]\\s*$', text):\n",
    "        \n",
    "            response_text = \"üåæ Tu reporte ya fue registrado. ¬°Gracias por tu colaboraci√≥n!\"\n",
    "            next_state = \"restart\"\n",
    "            \n",
    "            inserted, case = insert_report_into_bdd(user_session[sender_id][\"data\"])\n",
    "            \n",
    "            if inserted:\n",
    "                response_text = response_text + f\"\\n\\nAdicionalmente, se cre√≥ el caso {case} por si quisieras consultar el estado del mismo.\"\n",
    "            \n",
    "        # Si nos dice que no es correcto, desechamos el reporte\n",
    "        elif re.search(r'^\\s*no\\s*$', text):\n",
    "            response_text = \"‚ùå Entendido. Tu reporte no se registrar√°.\\nPara repetir el reporte, o hacer otro, puedes volver a escribir sobre este mismo chat.\"\n",
    "            next_state = \"restart\"\n",
    "        \n",
    "        # Si nos responde algo que no sabemos que es\n",
    "        else:\n",
    "            response_text = \"Por favor responde √∫nicamente con *S√≠* o *No*.\"\n",
    "            \n",
    "\n",
    "    # Else que solo existe por si no capturamos la logica de la respuesta\n",
    "    else:\n",
    "        response_text = \"No entend√≠ tu respuesta. Por favor intenta nuevamente.\"\n",
    "\n",
    "    return response_text, next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a830eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_message(chat_id, text):\n",
    "    \"\"\"\n",
    "    Env√≠a un mensaje de texto al usuario usando la API de Telegram.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"chat_id\": chat_id,\n",
    "        \"text\": text,\n",
    "        \"parse_mode\": \"Markdown\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/sendMessage\", json=payload)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ö†Ô∏è Error enviando mensaje: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al enviar mensaje: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6872930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_markdown_v2(text):\n",
    "    return re.sub(r'([_*\\[\\]()~`>#+\\-=|{}.!])', r'\\\\\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8244964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/webhook\", methods=[\"POST\"])\n",
    "def telegram_webhook():\n",
    "    \"\"\"\n",
    "    Webhook principal para manejar mensajes entrantes desde Telegram.\n",
    "    Procesa texto, fotos y ubicaciones, mantiene el flujo de conversaci√≥n\n",
    "    y responde usando la API de Telegram.\n",
    "    \"\"\"\n",
    "    data = request.get_json()\n",
    "\n",
    "    if \"message\" not in data:\n",
    "        return \"ok\", 200\n",
    "\n",
    "    message = data[\"message\"]\n",
    "    chat_id = message[\"chat\"][\"id\"]\n",
    "    username = message[\"chat\"].get(\"username\", \"Desconocido\")\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Mensaje recibido de @{username} | chat_id={chat_id}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Si es la primera vez que el usuario escribe\n",
    "    if (chat_id not in user_session) or (user_session[chat_id][\"state\"] == \"restart\"):\n",
    "        print(\"Enviando menu al usuario\")\n",
    "        user_session[chat_id] = {\n",
    "            \"state\": \"saludo\",\n",
    "            \"data\": {\n",
    "                \"lat\": None,\n",
    "                \"lon\": None,\n",
    "                \"address\": None,\n",
    "                \"photo_url\": None,\n",
    "                \"user_risk_assesment\": None,\n",
    "                \"user_lot_description\": None,\n",
    "                \"report_id\" : None,\n",
    "                \"case_number\" : None,\n",
    "                \"phone_number\": None,\n",
    "                \"price\": None\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Enviamos el men√∫ inicial\n",
    "        send_menu(chat_id)\n",
    "        return \"ok\", 200\n",
    "\n",
    "    # Si el usuario se encuentra en modo Chatbot (NLP)\n",
    "    if (user_session[chat_id][\"state\"] in [\"NLP-Chatbot-1\", \"NLP-Chatbot-2\", \"NLP-Chatbot-3\"]):\n",
    "        \n",
    "        print(\"ü§ñ Enviando consulta al RAG Chatbot...\")\n",
    "        query = message.get(\"text\", \"\")\n",
    "        response = rag_chain.invoke({\"query\": query})\n",
    "\n",
    "        raw_output = response[\"result\"]\n",
    "        if \"RESPUESTA DE ASISTENTE:\" in raw_output:\n",
    "            clean_output = raw_output.split(\"RESPUESTA DE ASISTENTE:\")[-1].strip()\n",
    "        else:\n",
    "            clean_output = raw_output.strip()\n",
    "\n",
    "        send_message(chat_id, escape_markdown_v2(clean_output))\n",
    "        \n",
    "        if user_session[chat_id][\"state\"] == \"NLP-Chatbot-3\":\n",
    "            send_message(chat_id, \"Has llegado al l√≠mite de preguntas consecutivas. El Chat se reiniciar√° ahora. Espero haber sido de ayuda.\")\n",
    "            user_session[chat_id][\"state\"] = \"restart\"\n",
    "            \n",
    "        if user_session[chat_id][\"state\"] == \"NLP-Chatbot-2\":\n",
    "            user_session[chat_id][\"state\"] = \"NLP-Chatbot-3\"\n",
    "            \n",
    "        if user_session[chat_id][\"state\"] == \"NLP-Chatbot-1\":\n",
    "            user_session[chat_id][\"state\"] = \"NLP-Chatbot-2\"\n",
    "            \n",
    "        \n",
    "        return \"ok\", 200\n",
    "\n",
    "    # En caso contrario, seguimos el flujo de reporte\n",
    "    current_state = user_session[chat_id][\"state\"]\n",
    "    response_text, next_state = report_message_flow(chat_id, current_state, message)\n",
    "    user_session[chat_id][\"state\"] = next_state\n",
    "\n",
    "    send_message(chat_id, response_text)\n",
    "    return \"ok\", 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32c90e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:3000\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n",
      "ü§ñ Enviando consulta al RAG Chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:27:58] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n",
      "ü§ñ Enviando consulta al RAG Chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:28:21] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=1244140352\n",
      "==================================================\n",
      "Enviando menu al usuario\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:28:30] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Men√∫ enviado correctamente a 1244140352\n",
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n",
      "ü§ñ Enviando consulta al RAG Chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:28:52] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=1244140352\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:28:53] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n",
      "Enviando menu al usuario\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:29:03] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Men√∫ enviado correctamente a 6940133416\n",
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:29:27] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=1244140352\n",
      "==================================================\n",
      "ü§ñ Enviando consulta al RAG Chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:29:38] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:29:53] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:30:01] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=1244140352\n",
      "==================================================\n",
      "ü§ñ Enviando consulta al RAG Chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:30:13] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:30:15] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:30:19] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n",
      "‚úÖ Reporte insertado: CASO-2025-0008 (report_7c5b1783-471d-42be-ad32-1ffffa1b8dda)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:30:23] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=1244140352\n",
      "==================================================\n",
      "ü§ñ Enviando consulta al RAG Chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:30:56] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n",
      "Enviando menu al usuario\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:30:57] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Men√∫ enviado correctamente a 6940133416\n",
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:31:04] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mensaje recibido de @Desconocido | chat_id=6940133416\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2025 21:31:09] \"POST /webhook HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tapp.run(port=3000, debug=False, use_reloader=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch311 (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
